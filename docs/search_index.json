[["index.html", "Компьютерный анализ текста в R Тема 1 О курсе 1.1 Благодарности", " Компьютерный анализ текста в R Ольга Алиева 2023-08-27 Тема 1 О курсе Этот сайт содержит материалы к курсу “Компьютерный анализ текста в R”. Курс постоянно совершенствуется, а его материалы обновляются. Если вы заметили ошибку или опечатку, можно писать на адрес oalieva@hse.ru. 1.1 Благодарности Будут добавлены позже "],["начало-работы-с-r.html", "Тема 2 Начало работы с R 2.1 Что такое R? 2.2 Пакеты и виньетки 2.3 Если не хватает пакетов 2.4 О воспроизводимости 2.5 Что мы (не) будем делать? 2.6 RStudio 2.7 Установка 2.8 Начало работы 2.9 R как калькулятор 2.10 Операторы присваивания 2.11 Векторы 2.12 Отсутствующие значения 2.13 Списки 2.14 Матрицы 2.15 Таблицы", " Тема 2 Начало работы с R 2.1 Что такое R? R — это язык программирования для статистической обработки данных и работы с графикой. Он создан в 90-х гг. на факультете статистики Оклендского университета. Иными словами, его делали статистики и для статистиков. Поэтому он прекрасно подходит для анализа данных, статистических вычислений и машинного обучения, а значит востребован в науке. Язык R — один из самых распространённых в научной среде. Им пользуются математики, биологи, генетики и другие учёные, которым нужно проводить статистические исследования и строить модели. Поэтому язык R нужно изучать тем, кто планирует заниматься научными исследованиями. — Яндекс Практикум Блог После установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Но за счет того, что R распространяется свободно, постоянно появляются новые алгоритмы, созданные внутри экспертного сообщества и тоже доступные для всех. Как и любой язык, R растет и развивается. 2.2 Пакеты и виньетки Если в базовой инсталляции R нет нужного решения – имеет смысл поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub, например пакет Dracor, к которому я буду обращаться в рамках этого курса. Некоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами. Несмотря на то, что R создавался изначально для работы со статистикой, система свободно распространяемых модулей значительно расширяет круг задач, которые можно решать на этом языке. Например, благодаря модулю Shiny можно создавать приложения и встраивать их в веб-страницы, а модуль Leaflet позволяет создавать интерактивные карты. Одну из них мы сделали в рамках проекта Antibarbari HSE. По технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы. Бывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково, например, семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить. 2.3 Если не хватает пакетов Это самое интересное. Если вы работаете в программе с графическим интерфейсом (SPSS, Minitab), то вы вынуждены формулировать свою задачу так, чтобы “вписаться” в набор кнопок, предусмотренных разработчиком. В R, столкнувшись с особой задачей, вы просто пишете под нее особую функцию. Новую функцию не обязательно публиковать в составе пакета – можно сохранить в рабочую директорию (с расширением .R) и наслаждаться самому. По мере того, как развиваются ваши навыки программирования, вы можете ставить и решать все более сложные и интересные задачи. 2.4 О воспроизводимости Когда вы решите опубликовать свое исследование, то и код к нему придется опубликовать (как правило, для этого используется GitHub) – поэтому надо сразу привыкать кодить так, чтобы ваш код был понятен другим. Например, добавлять пояснения при помощи знака # (как в Python) # случайный набор чисел из нормального распределения x &lt;- rnorm(1000) # случайная выборка из этого набора y &lt;- sample(x, 100) В идеале, впрочем, вы поясняете не то, что код делает (при грамотном кодинге это должно быть самоочевидо), а зачем. В этом примере код, правда, настолько простой, что не требует особых пояснений. Но в больших проектах от “читабельности” кода зависит не только то, поймет ли вас потенциальный рецензент, но и сможете ли вы сами вспомнить, какая строчка за что отвечает. Также это позволит вернуться к проекту через некоторое время и быстро вспомнить, что там происходит. Если вы получите интересные результаты и решите их опубликовать, то выложить в открытый доступ придется не только код, но и данные (если они не защищены копирайтом или другими ограничениями). Таким образом рецензент или другие ученые, которые будут читать вашу статью, сможет перепроверить ваши выводы. Ученые так делают! И это еще один довод в пользу того, чтобы научиться программировать, а не полагаться на ПО с графическим интерфейсом. 2.5 Что мы (не) будем делать? Хотя возможности R очень широки, мы будем заниматься в основном анализом текстовых данных. “Текст” в данном случае можно понимать как зафиксированную (в машиночитаемом виде) речь: от отзыва на товар до романа. Но в основном данные я подбираю таким образом, чтобы они были интересны гуманитариям. Курс включает в себя два основных блока и 16 уроков: общее введение в R (темы 1-8) text-mining (темы 9-16) Часть 1. Общее введение в R Знакомство с R и RStudio. Начало работы. Объекты, функции, операторы. Визуализация данных: базовый R, lattice, ggplot2 Трансформация данных. «Опрятные» данные с dplyr и tidyverse. Условия и циклы. Написание собственных функций. Итерации с purrr. Импорт данных. Импорт данных из XML. Воспроизводимые исследования. RMarkdown. Регулярные выражения: базовый R и stringr Веб-скрапинг. Консолидация: промежуточный проект Часть 2. Text Mining Токенизация. Морфологический и синтаксический анализ. Анализ эмоциональной тональности. Распределения слов и анализ частотности. Cкрытое распределение Дирихле (LDA). Латентно-семантический анализ. Кластеризация и метод главных компонент. Построение сетей в R. Описание и анализ сетей. Консолидация: итоговый проект Мы не будем: анализировать звучащую речь (хотя это тоже можно делать в R); распознавать рукописные символов, для этого есть гораздо другие мощные инструменты; изучать машинное обучение и нейронные сети; разрабатывать приложения. Домашние и самостоятельные задания в основном будут проходить в виде интерактивных уроков с использованием пакета swirl. Если вы плохо представляете, на что вообще способны количественные методы в гуманитаристике, посмотрите видео панельной дискуссии “Цифровые инструменты и методы: в чем их польза и как им обучить гуманитария?” (НИУ ВШЭ, 2023 г.). Это видео о том, зачем. О том, как – дальше. 2.6 RStudio Работать в R мы будем с использованием RStudio. Это свободная среда разработки (IDE) программного обеспечения с открытым исходным кодом для языка программирования R. Наша задача в этом уроке – установить R и R Studio и убедиться, что все работает; научиться самостоятельно находить помощь, совершать несложные вычисления. 2.7 Установка Установить R Скачать R для Windows: https://cran.r-project.org/bin/windows/ Скачать R для Mac: https://cran.r-project.org/bin/macosx/ Скачать R для Linux: https://cran.r-project.org/bin/linux/ Установить R Studio Скачать: https://www.rstudio.com/products/rstudio/download/ (достаточно бесплатной версии) На MacOS для работы библиотеки Stylo также понадобится установить XQuartz: https://www.xquartz.org/ Можно пользоваться R в облаке: https://posit.cloud/ (нужна регистрация). 2.8 Начало работы После установки и запуска RStudio вы увидите вот такие четыре панели (их названия подписаны на картинке): RStudio Panes. Источник. Для начала попробуйте получить информацию о сессии, введя в консоли такую команду: sessionInfo() sessionInfo() – это функция. За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. О функциях можно думать как о глаголах (“сделай то-то!”). Аргументы – это что-то вроде дополнений и обстоятельств. (Кстати, в “диалекте” tidyverse есть функции-наречия, так что аналогия законная.) Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). Также можно (и нужно) читать техническую документацию к пакетам. Сколько аргументов функции mean() имеют значения по умолчанию? Уточнить свою рабочую директорию (в которой R будет искать и сохранять файлы) можно при помощи функции getwd() без аргументов. Установить рабочую директорию можно при помощи функции setwd(), указав в качестве аргумента путь к рабочей директории на вашем компьютере (в кавычках, так как это символьный вектор). В моем случае это выглядит так: setwd(&quot;/Users/olga/R_Workflow/&quot;) Также для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory. Пакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет с интерактивными уроками программирования на языке R: install.packages(&quot;swirl&quot;) Для подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек: library(swirl) В R можно создавать проекты, и это очень удобно1. А теперь – первое задание на кодинг. После выполнения swirl предложит отправить e-mail преподавателю; ответьте Yes и сделайте скриншот сообщения об отправке. Если задание оценивается, укажите имя так, чтобы вас можно было узнать. Вот пример: Установите курс программирования на R: install_course(\"R Programming E\"). После этого привяжите пакет командой library(swirl) и выполните следующую команду: swirl(). Укажите ваше имя. Пройдите урок 2 Workspace and Files. После выполнения ответьте на несколько вопросов на закрепление материала. Какие действия в рабочей директории можно совершать из консоли? создать директориюудалить директориюсоздать файлпереименовать файлкопировать файлудалить файл Чтобы создать вложенную директорию при помощи функции dir.create(), аргументу recursive следует задать значение… TRUE FALSE Если все получилось, двигаемся дальше. 2.9 R как калькулятор Можно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt. sqrt(4) # квадратный корень ## [1] 2 2^3 # степень ## [1] 8 log10(100) #логарифм ## [1] 2 Если в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли. 2.10 Операторы присваивания Чтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться. x &lt;- 2 + 2 # создаем переменную y &lt;- 0.1 # создаем еще одну переменную x &lt;- y # переназначаем x + y ## [1] 0.2 Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания. Функция c() позволяет собрать несколько элементов в единый вектор: x &lt;- c(3, 5, 7) x_mean &lt;- mean(x) # также возможно x.mean или xMean x_mean ## [1] 5 В диалекте tidyverse предпочтение отдается подчеркиванию, а не точке; здесь сказывается влияние синтаксиса Python, где через точку получают доступ к методам объекта. Будьте внимательны: R чувствительна к регистру! Объекты, предназначенные для хранения данных, – это отдельные переменные, векторы, матрицы и массивы, списки, факторы, таблицы данных. Функции – это поименованные программы, предназначенные для создания новых объектов или выполнения определенных действий над ними (С. Мастицкий и В. Шитиков 2015, 24) Как вы уже знаете из урока в swirl, список всех объектов в окружении возвращает функция ls(). Удалять объекты можно при помощи rm(). Функции можно вкладывать друг в друга: rm(list = ls()) # удаляет все объекты в окружении ::::{.task .code} Снова запустите swirl(). Укажите ваше имя. Пройдите урок 1 Basic Building Blocks. :::: Если все получилось, можно двигаться дальше! Но сначала зафиксируем несколько новых функций из этих первого урока. Что вычисляет функция abs()? среднеемодульквадратный корень Сколько значений вернет функция, если разделить c(2, 4, 6) на 2? Буква “c” в названии функции c() означает… covercollapseconcatenate 2.11 Векторы В языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента. x &lt;- 2 class(x) # числовой вектор ## [1] &quot;numeric&quot; length(x) # длина вектора ## [1] 1 Основные типы данных, с которыми мы будем работать, следующие: целое число (integer) число с плавающей точкой (numeric, также называются double, то есть число двойной точности) строка (character) логическая переменная (logical) категориальная переменная, или фактор (factor) # проверить тип данных x &lt;- sqrt(2) class(x) ## [1] &quot;numeric&quot; is.integer(x) ## [1] FALSE is.numeric(x) ## [1] TRUE Для начала мы научимся генерировать векторы. Например, так. seq(1, 5, 0.5) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 rep(&quot;foo&quot;, 5) ## [1] &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; Запустите swirl() и пройдите урок 3 Sequences of Numbers. Проверьте свои знания, прежде чем двигаться дальше. Какие числа вернет команда pi:10? натуральныецелыерациональныевещественныекомплексные Какие функции могут использоваться для создания символьных векторов? seq()rep()c() Сколько значений вернет команда rep(c(0, 1, 2), times = 10)? Посчитайте в уме, не выполняя код. Факторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой. t &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), levels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) t ## [1] A B C ## Levels: A B C При попытке объединить в единый вектор данные разных типов, они будут принудительно приведены к одному типу: x &lt;- c(TRUE, 1, 3, FALSE) x # логические значения переработаны в числовые ## [1] 1 1 3 0 y &lt;- c(1, &quot;a&quot;, 2, &quot;лукоморье&quot;) # строки всегда в кавычках y # числа превратились в строки ## [1] &quot;1&quot; &quot;a&quot; &quot;2&quot; &quot;лукоморье&quot; Типы векторов в R. Источник Логические векторы можно получить в результате применения логических операторов (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов: x &lt;- c(1:10) # числа от 1 до 10 y &lt;- x &gt; 5 y # значения TRUE соответствуют единице, поэтому их можно складывать ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE sum(y) ## [1] 5 Здесь можно запустить swirl() и пройти урок 8 Logic. Это не обязательно, но очень полезно, если хотите разобраться в операторах! Попробуйте посчитать в уме: какое из выражений ниже вернет значение TRUE? 7 == 9!(57 != 8)= 10\">9 &gt;= 10 -7\">-6 &gt; -7 Функции all() и any() также возвращают логические значения: x &lt;- 10:20 any(x == 15) ## [1] TRUE all(x &gt; 9) ## [1] TRUE Запустите swirl() и пройдите урок 4 Vectors. Это позволит больше узнать про логические и символьные векторы. Несколько вопросов для самопроверки. Какие значение вернет команда (3 &gt; 5) &amp; (4 == 4)? TRUE FALSE NA Какие значения вернет команда (TRUE == TRUE) | (TRUE == FALSE)? TRUE FALSE NA Команда paste(LETTERS, 1:4, sep = \"-\") вернет… числовой вектор длиной 26символьный вектор длиной 26числовой вектор длиной 4символьный вектор длиной 4ошибку Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины. x &lt;- 2; y &lt;- c(10, 20, 30); z &lt;- c(5, 6, 7) y / x ## [1] 5 10 15 x + y ## [1] 12 22 32 y + z ## [1] 15 26 37 Векторы можно индексировать, то есть забирать из них какие-то элементы: x &lt;- seq(1, 5, 0.5) x[4:5] # индексы начинаются с 1 (в отличие от Python) ## [1] 2.5 3.0 Запустите swirl() и пройдите урок 6 Subsetting Vectors. Проверьте, все ли вы поняли из этого урока. Если вектор x содержит числовые значения и некоторое количество NA, то что вернет команда x[is.na(x)]? вектор длиной 0вектор всех NAлогический векторвектор без NAошибку Что надо изменить в этом коде, чтобы получить все, кроме NA? Дан именованный вектор: vect &lt;- c(foo = 11, bar = 2, norf = NA). Как можно выбрать второй элемент? vect[bar]vect[‘2’]vect[‘bar’] 2.12 Отсутствующие значения NULL означает, что значение не существует. Например, если мы создадим пустой вектор, то при попытке распечатать его получим NULL. А вот длина пустого вектора равна нулю! y &lt;- c() y ## NULL length(y) ## [1] 0 NA (not available) указывает на то, что значение существует, но оно неизвестно. Любые операции с NA приводят к появлению новых NA! Сравните: x &lt;- c(1, NA, 2) mean(x) ## [1] NA y &lt;- c(1, NULL, 2) mean(y) ## [1] 1.5 Как проверить, есть ли в данных NA или NULL? Знак ==, который вы встречали в уроке swirl, здесь не подойдет. x &lt;- NA x == NA ## [1] NA y &lt;- NULL y == NULL ## logical(0) Для этого есть специальные функции. is.na(x) ## [1] TRUE is.null(y) ## [1] TRUE When some people first get to R, they spend a lot of time trying to get rid of NAs. People probably did the same sort of thing when zero was invented. NA is a wonderful thing to have available to you. It is seldom pleasant when your data have missing values, but life if much better with NA than without. Burns (2012) Как избавиться от NA? В некоторых случаях достаточно аргумента функции. mean(c(1, NA, 2), na.rm=T) ## [1] 1.5 Чуть более сложные способы вы узнаете из урока swirl ниже. Запустите swirl() и пройдите урок 5 Missing Values. Готово? Тогда попробуйте ответить на вопрос ниже, не выполняя вычислений в R. Дан вектор x &lt;- c(44, NA, 5, NA). Сколько NA вернет команда x == NA? Впереди еще списки, матрицы и таблицы. Не забывайте давать себе отдых! 2.13 Списки Списки, или рекурсивные векторы (в отличие от атомарных векторов), могут хранить данные разных типов. list = list(a = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), b = c(1, 2, 3), c = c(T, F, T)) list ## $a ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $b ## [1] 1 2 3 ## ## $c ## [1] TRUE FALSE TRUE Можно получить доступ как к элементам списка целиком, так и к их содержимому. list$a # обращение к поименованным элементам ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; list[2] # одинарные квадратные скобки извлекают элемент списка целиком ## $b ## [1] 1 2 3 class(list[2]) ## [1] &quot;list&quot; list[[2]] # элементы второго элемента ## [1] 1 2 3 class(list[[2]]) ## [1] &quot;numeric&quot; list$c[1]# первый элемент второго элемента ## [1] TRUE Обратите внимание, что list[2] и list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML. Индексирование списка в R. Источник 🧂 Установите библиотеку rcorpora и загрузите список с названиями хлеба и сладкой выпечки. library(rcorpora) my_list &lt;- corpora(&quot;foods/breads_and_pastries&quot;) Узнайте длину my_list и введите ее в поле ниже. Достаньте из my_list элемент pastries и узнайте его длину. А теперь извлеките пятый элемент из pastries и введите ниже его название. Со списками покончено. Теперь можно пойти выпить кофе с my_list$pastries[13]. 2.14 Матрицы Матрица – это вектор, который имеет два дополнительных атрибута: количество строк и количество столбцов. Из этого следует, что матрица, как и вектор, может хранить данные одного типа. Проверим. M = matrix(c(1, 2, 3, 4), nrow = 2) M # все ок ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 M = matrix(c(1, 2, 3, &quot;a&quot;), nrow = 2) M # все превратилось в строку! ## [,1] [,2] ## [1,] &quot;1&quot; &quot;3&quot; ## [2,] &quot;2&quot; &quot;a&quot; В матрице есть ряды и столбцы. Их количество определяет размер (порядок) матрицы. Выше мы создали матрицу 2 x 2. Элементы матрицы, как и элементы вектора, можно извлекать по индексу. Сначала указывается номер ряда (строки), потом номер столбца. M = matrix(c(1, 2, 3, 4), nrow = 2) M[1, ] # первая строка полностью ## [1] 1 3 M[,2] # второй столбец полностью ## [1] 3 4 M[1,1] # одно значение ## [1] 1 Обратите внимание, как меняется размерность при индексировании. M = matrix(c(1, 2, 3, 4), nrow = 2) class(M) ## [1] &quot;matrix&quot; &quot;array&quot; dim(M) # функция для извлечения измерений ## [1] 2 2 class(M[1, ]) # первая строка полностью ## [1] &quot;numeric&quot; dim(M[1, ]) ## NULL Попытка узнать измерения вектора возвращает NULL, потому что с точки зрения R векторы не являются матрицами из одного столбца или одной строки, и потому не имеют измерений. С другой стороны, можно создать матрицу, в которой будет одна строка или один столбцец. При выводе они выглядят не так, как обычные векторы. Хотя казалось бы. # вектор-строка C = matrix(c(1, 2, 3), nrow = 1) C ## [,1] [,2] [,3] ## [1,] 1 2 3 # вектор-столбец D = matrix(c(1, 2, 3), nrow = 3) D ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 Над числовыми матрицами в R можно совершать разные операции из линейной алгебры (А. Буховец и П. Москалев 2015); многие из них нам понадобятся, когда мы будем говорить о латентно-семантическом анализе. Ниже несколько полезных функций (но пока их можно пропустить). # в квадратной матрице есть главная и побочная диагонали M = matrix(c(1, 2, 3, 4), nrow = 2) # ее мы распечатывали выше diag(M) ## [1] 1 4 # если поставить матрицу на бок, то получится транспонированная матрица t(M) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 # матрицу можно умножить на скаляр, то есть на обычное число. M * 3 ## [,1] [,2] ## [1,] 3 9 ## [2,] 6 12 # матрицы одного размера можно складывать M + M ## [,1] [,2] ## [1,] 2 6 ## [2,] 4 8 Если хотите, можете посмотреть видео. Упражнений на матрицы пока не будет! (Они настигнут вас позже.) 2.15 Таблицы Таблицы (кадры данных, data frames) – это двумерные объекты (как и матрицы). Датафреймы отличаются от матриц тем, что их столбцы могут хранить данные разного типа. Если списки являются разнородными аналогами векторов в одном измерении, кадры данных являются разнородными аналогами матриц для двумерных данных (Мэтлофф 2019, 134). # создание датафрейма df &lt;- data.frame(names = c(&quot;A&quot;, &quot;B&quot;), age = c(10, 11)) df ## names age ## 1 A 10 ## 2 B 11 # извлечение элементов df$names # забирает весь столбец ## [1] &quot;A&quot; &quot;B&quot; df[,&quot;names&quot;] # то же самое, другой способ ## [1] &quot;A&quot; &quot;B&quot; df[1, ] # забирает ряд ## names age ## 1 A 10 Потренируемся на датасете с данными о гапаксах2 в диалогах Платона. Датасет можно скачать по ссылке. Файл имеет расширение .Rdata; щелкнув на него правой кнопкой мыши, можете открыть его сразу в RStudio и потренироваться. Этот датасет позволяет перепроверить выводы Льюиса Кэмпбелла, профессора Сент-Эндрюсского университета в Шотландии. Еще 1867 г., впервые применив количественный метод для датировки диалогов Платона, он пришел к выводу, что для “позднего” стиля Платона, среди прочего, характерно обилие редкой лексики (Campbell 1867, xxxi). В корпус подлинных диалогов Кэмпбелл включал 26 текстов, которые делил на три хронологические группы. Свои вычисления он делал вручную, а мы можем попробовать все пересчитать в R. ## dialogue words hapax ratio group ## 1 Apology 8745 36 0.004 1 ## 2 Charmides 8311 31 0.004 1 ## 3 Cratylus 17944 122 0.007 1 ## 4 Critias 4950 104 0.021 3 ## 5 Crito 4169 19 0.005 1 ## 6 Euthydemus 12453 87 0.007 1 Вот так выглядят наши данные. Функция class() позволяет убедиться, что это датафрейм. ## [1] &quot;data.frame&quot; Потренируемся работать с данными в таблицах. # узнать имена столбцов colnames(hapax_plato) ## [1] &quot;dialogue&quot; &quot;words&quot; &quot;hapax&quot; &quot;ratio&quot; &quot;group&quot; # извлечь ряд(ы) по значению hapax_plato[hapax_plato$dialogue == &quot;Parmenides&quot;, ] ## dialogue words hapax ratio group ## 16 Parmenides 15155 20 0.001 2 # узнать тип данных в столбцах str(hapax_plato) ## &#39;data.frame&#39;: 26 obs. of 5 variables: ## $ dialogue: chr &quot;Apology&quot; &quot;Charmides&quot; &quot;Cratylus&quot; &quot;Critias&quot; ... ## $ words : chr &quot;8745&quot; &quot;8311&quot; &quot;17944&quot; &quot;4950&quot; ... ## $ hapax : chr &quot;36&quot; &quot;31&quot; &quot;122&quot; &quot;104&quot; ... ## $ ratio : chr &quot;0.004&quot; &quot;0.004&quot; &quot;0.007&quot; &quot;0.021&quot; ... ## $ group : num 1 1 1 3 1 1 1 1 1 1 ... # отобрать ряды по количеству слов hapax_plato[hapax_plato$words &gt; 10000, ] ## dialogue words hapax ratio group ## 1 Apology 8745 36 0.004 1 ## 2 Charmides 8311 31 0.004 1 ## 3 Cratylus 17944 122 0.007 1 ## 4 Critias 4950 104 0.021 3 ## 5 Crito 4169 19 0.005 1 ## 6 Euthydemus 12453 87 0.007 1 ## 7 Euthyphro 5181 15 0.003 1 ## 8 Gorgias 26337 125 0.005 1 ## 9 HippiasMinor 4360 12 0.003 1 ## 10 Ion 4024 32 0.008 1 ## 11 Laches 7674 27 0.004 1 ## 12 Laws 103193 914 0.009 3 ## 13 Lysis 6980 49 0.007 1 ## 14 Menexenus 4808 43 0.009 1 ## 15 Meno 9791 30 0.003 1 ## 16 Parmenides 15155 20 0.001 2 ## 17 Phaedo 21825 140 0.006 1 ## 18 Phaedrus 16645 228 0.014 2 ## 19 Philebus 17668 64 0.004 3 ## 20 Protagoras 17795 102 0.006 1 ## 21 Republic 88878 668 0.008 2 ## 22 Sophist 16024 107 0.007 3 ## 23 Statesman 16953 180 0.011 3 ## 24 Symposium 17461 127 0.007 1 ## 25 Theaetetus 22489 162 0.007 2 ## 26 Timaeus 23662 370 0.016 3 # преобразовать тип данных в столбцах hapax_plato$group &lt;- as.factor(hapax_plato$group) hapax_plato[,2:4] &lt;- sapply(hapax_plato[,2:4],as.numeric) # подробнее о функции `sapply()` в уроке про итерации И еще с датафреймами полезна функция summary(): summary(hapax_plato) ## dialogue words hapax ratio group ## Length:26 Min. : 4024 Min. : 12.00 Min. :0.001000 1:16 ## Class :character 1st Qu.: 7154 1st Qu.: 31.25 1st Qu.:0.004000 2: 4 ## Mode :character Median : 15590 Median : 94.50 Median :0.007000 3: 6 ## Mean : 19364 Mean :146.69 Mean :0.007154 ## 3rd Qu.: 17907 3rd Qu.:136.75 3rd Qu.:0.008000 ## Max. :103193 Max. :914.00 Max. :0.021000 Последнее упражнение на кодинг в этой главе! Запустите swirl() и пройдите урок 7 Matrices and Data Frames. Напоследок небольшое практическое задание. Код для его выполнения сохраните в виде файла с расширением .R. Его надо будет отправить преподавателю. # устанавливаем и загружаем нужный пакет install.packages(&quot;languageR&quot;) library(languageR) # загружаем датасет meta &lt;- data(&quot;spanishMeta&quot;) # допишите ваш код ниже # посчитайте средний год публикации романов Камило Хосе Селы # вычислите суммарное число слов в романах Эдуардо Мендосы # извлеките ряды с текстами, опубликованными до 1980 г. Поздравляем! С этой главой вы справились. Дальше будет сложнее, но интереснее. Литература "],["визуализации.html", "Тема 3 Визуализации 3.1 Базовый R 3.2 Lattice 3.3 Ggplot2 3.4 Экспорт графиков из среды R", " Тема 3 Визуализации 3.1 Базовый R В R существуют три основные системы построения графиков, которые могут быть полезны для достижения разных целей. Базовый R – это самая старая система, и в ее основе лежит идея палитры художника3. Идея заключается в том, что у вас есть чистый холст, на который вы добавляете что-то одно за другим: например, сначала вы создаете диаграмму рассеяния с несколькими точками, затем вы добавляете метки, линию регрессии, заголовки и т.п. Каждая деталь графика занимает еще одну строчку кода. Это интуитивно понятная модель, потому что часто в самом начале, исследуя данные, мы часто не знаем, какой график мы хотим построить. Обычно мы начинаем это построение с функции plot(), а затем добавляем функции, которые аннотируют график. Вот простой пример на данных о гапаксах у Платона, которые мы видели раньше. Чтобы построить диаграмму рассеяния (scatter plot), нужно передать функции plot() в качестве аргументов названия тех столбцов, которые мы хотим изобразить по осям x и y. Это можно записать так: plot(x, y). Или так: plot(y ~ x). Знак ~ (тильда) указывает на функцию. attach(hapax_plato) plot(hapax ~ words) Это можно записать и иначе: plot(hapax_plato$hapax ~ hapax_plato$words). Результат будет одинаковый. Теперь беремся за палитру. Данные скучились в левом нижнем углу и потому плохо читаются. Мы можем пожертвовать двумя очень длинными диалогами (это “Государство” и “Законы”) и сделать zoom in, указав вручную границы осей. attach(hapax_plato) plot(hapax ~ words, xlim = c(0, 30000), ylim = c(0, 500)) Но так мы все-таки теряем какую-то информацию – а вдруг она важная? Еще один способ справиться со слипшимися данными – преобразовать их. Применим логарифмическое преобразование. Обратите внимание, как меняются значения на осях. attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;) # добавим текст text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7) Уже гораздо интереснее! Попробуем обозначить цветом и формой пересказанные и прямые диалоги. Форма задается внутри функции plot() при помощи атрибута pch. Числовые значения этого атрибута соответствуют следующим значкам. Мы используем 2, 3 и 5. Значения атрибута pch Перестраиваем наш график. attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], pch = c(2, 3, 5)[group]) text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group]) Некоторые названия перекрываютcя (с этим мы научимся бороться позже), но все равно намного понятнее. Теперь можем поменять шрифт и, например, добавить линию регрессии (не хватает легенды, но что-то уже нет сил). attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], pch = c(2, 3, 5)[group], family = &quot;serif&quot;) text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], family = &quot;serif&quot;) # добавим линию регрессии my_lm &lt;- lm(hapax_plato$hapax ~ hapax_plato$words) abline(my_lm, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, untf = T) # и заголовок title(main = &quot;Число гапаксов в зависимости от длины диалога&quot;) При помощи графических параметров4 можно контролировать множество настроек. Но в этом и недостаток базовой графики. Не всем хватает терпения и вкуса этим заниматься, поэтому эта система сейчас не очень употребительна. Попробуйте интерпретировать график, который у нас получился. Прав ли был профессор Кэмпбелл, утверждая, что высокая доля гапаксов характерна для “поздних” текстов? Исходите из того, что единственный текст, о котором точно известно, что он поздний – это “Законы”. Судя по графику, количество гапаксов зависит от количества слов в тексте. Чем длиннее текст, тем больше вероятность встретить там редкое слово. 3.2 Lattice Система Lattice (букв. “Решетка”) была разработана специально для анализа многомерных данных (Sarkar 2008). Тут должны быть графики цветочки Например, мы сравниваем точность классификации текстов в зависимости от длины отрывка и количества слов-предикторов. Это уже три переменные (длина – количество слов – точность). Система решеток, или панелей, позволяет представить такие многомерные данные. Многомерно нет слов! В базовом R это тоже можно сделать, изменив графические параметры: par(mfrow = c(1,2)) # вот тут указываем число рядов и столбцов plot(hapax_plato$hapax ~ hapax_plato$words) plot(hapax_plato$ratio ~ hapax_plato$group) Но видно, что пространство при этом расходуется неэффективно. Кроме того, к таким графикам сложно создавать заголовки и подзаголовки, подбирать подписи и т.п. Все эти задачи решает Lattice. Идея этой системы в том, что каждый график строится с помощью одного вызова функции. При этом необходимо сразу указать большое количество информации, чтобы у фунцкии было достаточно данных для построения графика. library(lattice) attach(hapax_plato) # после вертикальной черты указана переменная, которая используется для группировки данных; в нашем случае номер группы (по Кэмпбеллу) xyplot(hapax ~ words | group, data = hapax_plato, scales=list(x=list(log=10))) # трансформация по одной оси Недостаток Lattice, однако, в том, что бывает сложно аннотировать отдельные панели, а также приходится сразу задавать весь график в одном вызове функции. Это не всегда удобно. После создания графика уже ничего нельзя добавить или убавить. 3.3 Ggplot2 Но настоящая графическая сила R – это пакет ggplot2. В его основе лежит идея “грамматики графических элементов” Лиланда Уилкинсона (Мастицкий 2017), и он позволяет объединить достоинства базовой графики R и Lattice. С одной стороны, вы можете постепенно достраивать график, добавляя элемент за элементом; с другой стороны, множество параметров подбираются автоматически, как в Lattice. 3.3.1 Быстрое решение: qplot() Настройки по умолчанию хорошо видно на графике ниже; их легко перенастроить. library(ggplot2) # загружается сразу с tidyverse options(scipen = 999) qplot(words, hapax, data = hapax_plato, log = &quot;xy&quot;) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. Функция qplot() – это быстрое решение для задач визуализации. В современных версиях ggplot использование функции qplot() не рекомендуется (deprecated), чтобы побудить пользователей изучать ggplot() как более совершенный инструмент. В данном случае мы построили диаграмму рассеяния, используя логарифмическую трансформацию по двум осям. Можно также выделить цветом различные типы диалогов, изменить размер точек, их прозначность и т.п. qplot(words, hapax, data = hapax_plato, log = &quot;xy&quot;, col = group, size = 1.5) + theme(legend.position = &quot;none&quot;) Диаграмма размаха (о ней подробнее можно посмотреть здесь) удобна в тех случаях, когда необходимо представить обобщенную статистическую информацию о распределении значений количественной переменной в разных группах. attach(hapax_plato) qplot(group, ratio, data = hapax_plato, geom = &quot;boxplot&quot;, color = group) Диаграмму размаха можно совместить с одномерной диаграммой рассеяния. qplot(group, ratio, data = hapax_plato, geom = c(&quot;boxplot&quot;, &quot;jitter&quot;), color = group) # вместо color можно использовать shape, который отвечает за форму элементов 3.3.2 Слой за слоем: ggplot() Для более детальной настройки графика рекомендууется использовать функцию ggplot(), которая имеет два основных аргумента: data и aes (англ. aesthetics); последняя присваивает эстетические атрибуты геометрическим объектам, которые используются на графике. Эти объекты могут слоями накладываться друг на друга (Wickham and Grolemund 2017). Посмотрим, как это работает, на примере, столбиковой диаграммы. Такая диаграмма позволяет представить распределение как количественных, так и качественных переменных. Для примера возьмем датасет diorisis_meta, который хранит данные о древнегреческих текстах, доступных в репозитории Diorisis5. ## # A tibble: 6 × 5 ## name title date genre subgenre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Achilles Tatius Leucippe and Clitophon 120 Narrative Novel ## 2 Aelian De Natura Animalium 230 Technical Natural History ## 3 Aelian Epistulae Rusticae 230 Letters Letters ## 4 Aelian Varia Historia 200 Essays Miscellanea ## 5 Aeneas Tacticus Poliorcetica -350 Technical Military ## 6 Aeschines Against Ctesiphon -330 Oratory Oratory Столбиковая диаграмма позволяет увидеть, тексты каких жанров чаще всего встречаются в этом корпусе. library(tidyverse) diorisis_meta %&gt;% group_by(genre) %&gt;% count() %&gt;% ggplot(aes(reorder(genre, n), n, fill = genre)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() Точечная диаграмма, или dotplot, подходит для тех случаев, когда мы исследуем распределение наблюдений для разных групп данных, причем наблюдений не очень много. Например, мы можем отразить распределение текстов в корпусе по годам. Категориальную переменную (например, жанр) можно дополнительно закодировать цветом (зд. подробнее о том, что можно увидеть на этом графике). diorisis_meta %&gt;% ggplot(aes(date, fill = factor(genre))) + geom_dotplot(binwidth = 10, stackdir = &quot;centerwhole&quot;, binpositions = &quot;all&quot;) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) Различные группы данных можно выделять не только цветом и формой, но и помещать каждую в свое окошко (facet). Попробуем выяснить: сколько поджанров в каждом жанре? diorisis_meta %&gt;% group_by(genre, subgenre) %&gt;% count %&gt;% filter(genre %in% c(&quot;Poetry&quot;, &quot;Technical&quot;)) %&gt;% ggplot(aes(reorder(subgenre, n), n, fill = subgenre)) + geom_col(show.legend = F) + facet_wrap(~genre, scales = &quot;free&quot;) + # вот здесь задаем группы coord_flip() Подробнее с разными видами графиков мы познакомимся дальше, но напоследок о том, как сделать так, чтобы текстовые подписи не наезжали друг на друга. library(ggrepel) hapax_plato %&gt;% ggplot(aes(words, hapax, col = group)) + geom_point(size = 1.2, alpha = 0.7, show.legend = F) + geom_label_repel(label = dialogue) + scale_x_log10() + scale_y_log10() + theme_bw() Ай, красота. 3.4 Экспорт графиков из среды R Способы: реализованные в R драйверы стандартных графических устройств; функция ggsave() меню программы RStudio. # код сохранит pdf в рабочую директорию pdf(file = &quot;Diorisis.pdf&quot;) diorisis_meta %&gt;% group_by(genre, subgenre) %&gt;% count %&gt;% filter(genre %in% c(&quot;Poetry&quot;, &quot;Technical&quot;)) %&gt;% ggplot(aes(reorder(subgenre, n), n, fill = subgenre)) + geom_col(show.legend = F) + facet_wrap(~genre, scales = &quot;free&quot;) + coord_flip() dev.off() # еще один способ сохранить последний график ggsave( filename = &quot;Diorisis.png&quot;, plot = last_plot(), device = &quot;png&quot;, scale = 1, width = NA, height = 500, units = &quot;px&quot;, dpi = 300 ) Литература "],["опрятные-данные.html", "Тема 4 Опрятные данные 4.1 Синтаксис tidyverse 4.2 Опрятные данные 4.3 Пример: буккроссинг", " Тема 4 Опрятные данные Tidy datasets are all alike, but every messy dataset is messy in its own way. — Hadley Wickham 4.1 Синтаксис tidyverse Существуют два основных “диалекта” R, один из которых опирается главным образом на функции и структуры данных базового R, а другой пользуется синтаксисом tidyverse (Winter 2020). Tidyverse – это семейство пакетов (метапакет), разработанных Хадли Уикхемом и др., которое включает в себя в том числе пакеты dplyr, ggplot2 и многие другие. # загрузить все семейство library(tidyverse) 4.1.1 Tibble Основная структура данных в tidyverse – это tibble, современный вариант датафрейма6. Тиббл, как говорят его разработчики, это ленивые и недовольные датафреймы: они делают меньше и жалуются больше7. Это позволяет решать проблемы на более ранних этапах, что, как правило, приводит к созданию более чистого и выразительного кода. Основные отличия от обычного датафрейма: текст по умолчанию конвертируется в строки, а не в факторы;8 усовершенствованный метод print(), не нужно постоянно вызывать head(); нет имен рядов; допускает синтаксически “неправильные” имена столбцов; при индексировании не меняет тип данных на вектор и др. load(&quot;./data/DiorisisMeta.Rdata&quot;) # распечатывает только первые 10 рядов, для каждого столбца указан тип данных, строки пронумерованы as_tibble(diorisis_meta) ## # A tibble: 784 × 5 ## name title date genre subgenre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Achilles Tatius Leucippe and Clitophon 120 Narrative Novel ## 2 Aelian De Natura Animalium 230 Technical Natural History ## 3 Aelian Epistulae Rusticae 230 Letters Letters ## 4 Aelian Varia Historia 200 Essays Miscellanea ## 5 Aeneas Tacticus Poliorcetica -350 Technical Military ## 6 Aeschines Against Ctesiphon -330 Oratory Oratory ## 7 Aeschines Against Timarchus -347 Oratory Oratory ## 8 Aeschines The Speech on the Embassy -336 Oratory Oratory ## 9 Aeschylus Agamemnon -458 Tragedy Tragedy ## 10 Aeschylus Eumenides -458 Tragedy Tragedy ## # ℹ 774 more rows # индексирование head(as.data.frame(diorisis_meta)[, 1]) # возвращает вектор ## [1] &quot;Achilles Tatius&quot; &quot;Aelian&quot; &quot;Aelian&quot; &quot;Aelian&quot; ## [5] &quot;Aeneas Tacticus&quot; &quot;Aeschines&quot; as_tibble(diorisis_meta)[,1] # возвращает тиббл ## # A tibble: 784 × 1 ## name ## &lt;chr&gt; ## 1 Achilles Tatius ## 2 Aelian ## 3 Aelian ## 4 Aelian ## 5 Aeneas Tacticus ## 6 Aeschines ## 7 Aeschines ## 8 Aeschines ## 9 Aeschylus ## 10 Aeschylus ## # ℹ 774 more rows # имена столбцов df &lt;- data.frame(&#39;var 1&#39; = 1:2, two = 3:4) df ## var.1 two ## 1 1 3 ## 2 2 4 tbl &lt;- tibble(&#39;var 1&#39; = 1:2, two = 3:4) tbl ## # A tibble: 2 × 2 ## `var 1` two ## &lt;int&gt; &lt;int&gt; ## 1 1 3 ## 2 2 4 4.1.2 Dplyr Но самое главное, tibble подходит для “грамматики манипуляции данных”, лежащей в основе dplyr9. Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными: mutate() добавляет новые переменные, которые являются функциями существующих переменных; select() выбирает переменные на основе их имен; filter() выбирает наблюдения на основе их значений; summarise() обобщает значения; arrange() изменяет порядок следования строк. Все эти глаголы естественным образом сочетаются с функцией group_by(), которая позволяет выполнять любые операции “по группам”, и с оператором pipe %&gt;% из пакета magrittr. В итоге получается более лаконичный и читаемый код, что можно показать на примере. diorisis_meta %&gt;% select(-subgenre) %&gt;% filter(genre == &quot;Narrative&quot;) %&gt;% # не нужны кавычки! group_by(name) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 20 × 2 ## # Groups: name [20] ## name n ## &lt;chr&gt; &lt;int&gt; ## 1 Plutarch 71 ## 2 Appian 14 ## 3 Flavius Josephus 4 ## 4 Xenophon 4 ## 5 Arrian 3 ## 6 Diodorus Siculus 3 ## 7 Philostratus the Athenian 2 ## 8 Achilles Tatius 1 ## 9 Cassius Dio 1 ## 10 Chariton 1 ## 11 Diogenes Laertius 1 ## 12 Dionysius of Halicarnassus 1 ## 13 Eusebius of Caesarea 1 ## 14 Herodotus 1 ## 15 Longus 1 ## 16 Lucian 1 ## 17 Polybius 1 ## 18 Pseudo Apollodorus 1 ## 19 Thucydides 1 ## 20 Xenophon of Ephesus 1 В базовом R мы бы делали то же самое вот так: diorisis_df &lt;- as.data.frame(diorisis_meta) diorisis_select &lt;- diorisis_df[,-5] # remove column diorisis_filter &lt;- diorisis_select[diorisis_select$genre == &quot;Narrative&quot;, ] diorisis_names &lt;- diorisis_filter$name diorisis_count &lt;- as.data.frame(table(diorisis_names)) diorisis_sort &lt;- diorisis_count[order(diorisis_count$Freq, decreasing =T),] head(diorisis_sort) ## diorisis_names Freq ## 15 Plutarch 71 ## 2 Appian 14 ## 10 Flavius Josephus 4 ## 19 Xenophon 4 ## 3 Arrian 3 ## 6 Diodorus Siculus 3 Тут должен быть какой-то поучительный вывод. 4.2 Опрятные данные Но tidyverse – это не только особый синтаксис, но и отдельная идеология “опрятных данных”. “Сырые” данные, с которыми мы работаем, редко бывают опрятны, и перед анализом их следует “почистить” и преобразовать10. Основные правила опрятных данных: отдельный столбец для каждой переменной; отдельный ряд для каждого наблюдения; у каждого значения отдельная ячейка; один датасет – одна таблица. Принципы опрятных данных Посмотрите на учебные тибблы из пакета tidyr и подумайте, какое из этих правил нарушено в каждом случае. data(&quot;table2&quot;) table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 data(&quot;table3&quot;) table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 data(&quot;table4a&quot;) table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 data(&quot;table4b&quot;) table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 Важные функции для преобразования данных из пакета tidyr:11 separate() делит один столбец на новые; unite() объединяет столбцы; pivot_longer() удлиняет таблицу; pivot_wider() расширяет таблицу; drop_na() и replace_na() указывают, что делать с NA и др. Также упомянем функцию distinct() из dplyr, которая оставляет только уникальные наблюдения и предсталяет собой аналог базовой unique() для таблиц. Кроме того, в dplyr есть полезное семейство функций _join, позволяющих объединять данные в различных таблицах.12 Дальше мы потренируемся с ними работать. 4.3 Пример: буккроссинг 4.3.1 Смотрим на данные Загрузим пример неопрятных данных и попробуем их преобразовать для анализа. Book-Crossing – датасет с рейтингами миллионов книг и обезличенными демографическими данными о более 250 тысячах их читателей. Этот датасет хранится в трех разных таблицах. head(ratings) ## # A tibble: 6 × 3 ## `User-ID` ISBN `Book-Rating` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 276725 034545104X 0 ## 2 276726 0155061224 5 ## 3 276727 0446520802 0 ## 4 276729 052165615X 3 ## 5 276729 0521795028 6 ## 6 276733 2080674722 0 head(users) ## # A tibble: 6 × 3 ## `User-ID` Location Age ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 nyc, new york, usa NULL ## 2 2 stockton, california, usa 18 ## 3 3 moscow, yukon territory, russia NULL ## 4 4 porto, v.n.gaia, portugal 17 ## 5 5 farnborough, hants, united kingdom NULL ## 6 6 santa monica, california, usa 61 head(books) ## # A tibble: 6 × 8 ## ISBN `Book-Title` `Book-Author` `Year-Of-Publication` Publisher `Image-URL-S` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0195… Classical M… Mark P. O. M… 2002 Oxford U… http://image… ## 2 0002… Clara Callan Richard Bruc… 2001 HarperFl… http://image… ## 3 0060… Decision in… Carlo D&#39;Este 1991 HarperPe… http://image… ## 4 0374… Flu: The St… Gina Bari Ko… 1999 Farrar S… http://image… ## 5 0393… The Mummies… E. J. W. Bar… 1999 W. W. No… http://image… ## 6 0399… The Kitchen… Amy Tan 1991 Putnam P… http://image… ## # ℹ 2 more variables: `Image-URL-M` &lt;chr&gt;, `Image-URL-L` &lt;chr&gt; Что не так с этими данными? users содержит больше одного значения в столбце Location много отсутствующих значений данные вводятся самими пользователями через сайт https://www.bookcrossing.com/ ; они могут содержать недостоверную информацию, см. напр. moscow, yukon territory, russia (Юкон – это территория Канады). Age представляет собой строку и др. Прежде чем начинать преобразование, надо сформулировать примерный вопрос и понять, что для нас важно, а что нет. Например: - Сколько читателей старше 30 лет пользуются сервисом в Австралии? - В какие года опубликованы самые популярные книги? - Кто популярнее у читателей, Роулинг или Толкин? - Какой процент пользователей никогда не оставляет отзывы? - Есть ли связь между возрастом и количеством оценок? и т.п. Чтобы объединить данные, надо понять, через какие переменные они связаны. Ответ: ratings и books связаны через переменную isbn, ratings и users связаны через переменную User-ID. 4.3.2 Трансформируем данные Начнем с пользователей. users_separated &lt;- users %&gt;% mutate(Age = as.numeric(Age)) %&gt;% filter(!is.na(Age)) %&gt;% # drop_na(Age) тоже решил бы нашу задачу separate(Location, into = c(NA, NA, &quot;country&quot;), sep = &quot;,&quot;) head(users_separated) # можно было бы не сохранять, но так нагляднее ## # A tibble: 6 × 3 ## `User-ID` country Age ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 &quot; usa&quot; 18 ## 2 4 &quot; portugal&quot; 17 ## 3 6 &quot; usa&quot; 61 ## 4 10 &quot; spain&quot; 26 ## 5 11 &quot; australia&quot; 14 ## 6 13 &quot; spain&quot; 26 Здесь можно сразу посмотреть, из каких стран и какого возраста пользователи. users_separated %&gt;% group_by(country) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 543 × 2 ## # Groups: country [543] ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot; usa&quot; 67138 ## 2 &quot; united kingdom&quot; 10935 ## 3 &quot; canada&quot; 9877 ## 4 &quot; spain&quot; 9505 ## 5 &quot; germany&quot; 8016 ## 6 &quot; australia&quot; 7824 ## 7 &lt;NA&gt; 5914 ## 8 &quot; italy&quot; 4754 ## 9 &quot; france&quot; 2395 ## 10 &quot; portugal&quot; 2175 ## # ℹ 533 more rows Последние ряды этого тибла выглядят достаточно причудливо: users_separated %&gt;% group_by(country) %&gt;% count() %&gt;% arrange(n) ## # A tibble: 543 × 2 ## # Groups: country [543] ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot; pasig city.&quot; 1 ## 2 &quot; &amp;#20013;&amp;#22269;&quot; 1 ## 3 &quot; &amp;#32654;&amp;#22269;&quot; 1 ## 4 &quot; 5057chadwick ct.&quot; 1 ## 5 &quot; 600 083&quot; 1 ## 6 &quot; \\\\n/a\\\\\\&quot;&quot; 1 ## 7 &quot; a new year is ahead&quot; 1 ## 8 &quot; aberdeenshire&quot; 1 ## 9 &quot; agusan del sur&quot; 1 ## 10 &quot; alabama&quot; 1 ## # ℹ 533 more rows Здесь возможно несколько стратегий. Можно выбрать все ряды с названиями реальных стран либо (если это соответствует исследовательской задаче) какую-то одну страну. Можно и проигнорировать, если происхождение пользователей не так важно. Допустим, мы решаем сосредоточиться на Испании. Обратите внимание, что в название страны после разделения функцией separate() попали пробелы, и от них надо избавиться. Это делается при помощи регулярных выражений (о них в другой раз) и функции mutate(). spain_data &lt;- users_separated %&gt;% mutate(country = str_replace_all(country, pattern = &quot;\\\\s+&quot;, &quot;&quot;)) %&gt;% # это означает, что пробел мы меняем на &quot;ничто&quot;, т.е. убираем filter(country == &quot;spain&quot;) %&gt;% group_by(Age) %&gt;% count() %&gt;% arrange(-n) head(spain_data) ## # A tibble: 6 × 2 ## # Groups: Age [6] ## Age n ## &lt;dbl&gt; &lt;int&gt; ## 1 25 514 ## 2 26 510 ## 3 23 480 ## 4 24 467 ## 5 28 459 ## 6 27 450 Столбиковая диаграмма подходит для визуализации подобных данных: spain_data %&gt;% ggplot(aes(Age, n)) + geom_bar(stat = &quot;identity&quot;, col = &quot;blue&quot;, fill = &quot;white&quot;) + theme_bw() Какие целеустремленные испанцы! Читают от 0 до 183 лет 😵 После того, как мы убрали лишние пробелы из названий стран, можно фильтровать: spain_id &lt;- users_separated %&gt;% mutate(country = str_replace_all(country, pattern = &quot;\\\\s+&quot;, &quot;&quot;)) %&gt;% filter(country == &quot;spain&quot;) # на этот раз мы не считаем число наблюдений в группе, а забираем все ряды, которые отвечают условию 4.3.3 Объединяем данные Мы уже выяснили, что ratings и users связаны через переменную User-ID, и в ratings хотели бы оставить только те id, которые отвечают заданному условию (страна, возраст и т.п.). Для такого рода объединений как раз подходят функции _join13. Функции семейства _join spain_ratings &lt;- spain_id %&gt;% left_join(ratings) %&gt;% filter(!is.na(ISBN)) %&gt;% filter(`Book-Rating` &gt; 7) %&gt;% # имена синтаксически неправильные, поэтому требуется знак &quot;`&quot; group_by(ISBN) %&gt;% count() %&gt;% arrange(-n) ## Joining with `by = join_by(`User-ID`)` spain_ratings ## # A tibble: 1,281 × 2 ## # Groups: ISBN [1,281] ## ISBN n ## &lt;chr&gt; &lt;int&gt; ## 1 8432206407 4 ## 2 8433969978 4 ## 3 846630679X 4 ## 4 8472236552 4 ## 5 8495501198 4 ## 6 840149186X 3 ## 7 8401499585 3 ## 8 8423310353 3 ## 9 8423662152 3 ## 10 8432215007 3 ## # ℹ 1,271 more rows Осталось выяснить, что это за книги. Для этого объединяем spain_ratings и books. spain_books &lt;- spain_ratings %&gt;% filter(n &gt; 2) %&gt;% left_join(books) %&gt;% filter(!is.na(`Book-Title`), !is.na(`Book-Author`)) %&gt;% ungroup() spain_books ## # A tibble: 15 × 9 ## ISBN n `Book-Title` `Book-Author` `Year-Of-Publication` Publisher ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8432206407 4 Sin Noticias … Eduardo Mend… 1995 Planeta … ## 2 8433969978 4 El Libro de L… Paul Auster 2003 Anagrama ## 3 846630679X 4 La caverna = … Jose Saramago 2002 Punto de… ## 4 8472236552 4 UN Viejo Que … Luis Sepulve… 1993 Tusquets… ## 5 8495501198 4 Memorias de u… Arthur Golden 2001 Suma de … ## 6 840149186X 3 El Club de Lo… N. H. Kleinb… 1995 Plaza &amp;a… ## 7 8401499585 3 Los Pilares d… Ken Follett 1995 Plaza &amp;a… ## 8 8423310353 3 El Camino (Co… Miguel Delib… 1991 Continen… ## 9 8432215007 3 El perfume Patrick Susk… 1997 Editoria… ## 10 8445071408 3 El Senor De L… J. R. R. Tol… 2001 Minotauro ## 11 8445071416 3 El Hobbit J. R. R. Tol… 1991 Minotauro ## 12 8477204055 3 El caballero … Robert Fisher 2000 Obelisco ## 13 8478884459 3 Harry Potter … J. K. Rowling 1999 Lectorum… ## 14 8484602508 3 Diario de Un … Antonio Salas 2003 Temas de… ## 15 8495501112 3 Son De Mar Manuel Vicent 2002 Suma de … ## # ℹ 3 more variables: `Image-URL-S` &lt;chr&gt;, `Image-URL-M` &lt;chr&gt;, ## # `Image-URL-L` &lt;chr&gt; Как минимум мы выяснили, что испанцы предпочитают читать по-испански! (Здесь снова можно подумать. Возможно, у одной книги разные ISBN, и стоило группировать не по ISBN, а по названию или автору?) Осталось избавиться от неинформативных столбцов (это ссылки, часто битые, на изображения обложки). Если мы знаем номера этих столбцов, то это можно сделать по индексу: spain_books %&gt;% select(3:5) %&gt;% rename(title = `Book-Title`, author = `Book-Author`) ## # A tibble: 15 × 3 ## title author `Year-Of-Publication` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Sin Noticias De Gurb (Biblioteca breve) Eduar… 1995 ## 2 El Libro de Las Ilusiones Paul … 2003 ## 3 La caverna = A caverna Jose … 2002 ## 4 UN Viejo Que Leia Novelas De Amor/the Old Men W… Luis … 1993 ## 5 Memorias de una geisha Arthu… 2001 ## 6 El Club de Los Poetas Muertos N. H.… 1995 ## 7 Los Pilares de La Tierra Ken F… 1995 ## 8 El Camino (Coleccion Destinolibro) Migue… 1991 ## 9 El perfume Patri… 1997 ## 10 El Senor De Los Anillos: LA Comunidad Del Anill… J. R.… 2001 ## 11 El Hobbit J. R.… 1991 ## 12 El caballero de la armadura oxidada Rober… 2000 ## 13 Harry Potter y la piedra filosofal J. K.… 1999 ## 14 Diario de Un Skin: Un Topo En El Movimiento Neo… Anton… 2003 ## 15 Son De Mar Manue… 2002 Однако у select() есть функции-помощники14, которые подходят для таких случаев: starts_with() ends_with() contains() matches() num_range() spain_books %&gt;% select(-contains(&quot;URL&quot;), -matches(&quot;Publisher&quot;)) %&gt;% # удалим заодно и издателя rename(title = `Book-Title`, author = `Book-Author`, published = `Year-Of-Publication`) # чиним имена ## # A tibble: 15 × 5 ## ISBN n title author published ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 8432206407 4 Sin Noticias De Gurb (Biblioteca breve) Eduar… 1995 ## 2 8433969978 4 El Libro de Las Ilusiones Paul … 2003 ## 3 846630679X 4 La caverna = A caverna Jose … 2002 ## 4 8472236552 4 UN Viejo Que Leia Novelas De Amor/the Old … Luis … 1993 ## 5 8495501198 4 Memorias de una geisha Arthu… 2001 ## 6 840149186X 3 El Club de Los Poetas Muertos N. H.… 1995 ## 7 8401499585 3 Los Pilares de La Tierra Ken F… 1995 ## 8 8423310353 3 El Camino (Coleccion Destinolibro) Migue… 1991 ## 9 8432215007 3 El perfume Patri… 1997 ## 10 8445071408 3 El Senor De Los Anillos: LA Comunidad Del … J. R.… 2001 ## 11 8445071416 3 El Hobbit J. R.… 1991 ## 12 8477204055 3 El caballero de la armadura oxidada Rober… 2000 ## 13 8478884459 3 Harry Potter y la piedra filosofal J. K.… 1999 ## 14 8484602508 3 Diario de Un Skin: Un Topo En El Movimient… Anton… 2003 ## 15 8495501112 3 Son De Mar Manue… 2002 Возможно, сюда стоит добавить что-то про работу с факторами. Но не сейчас. Литература "],["функциональное-программирование.html", "Тема 5 Функциональное программирование 5.1 Зачем писать функции? 5.2 Область видимости переменных 5.3 Аргументы функции 5.4 Векторизируй это 5.5 Векторизованные конструкции 5.6 Вместо циклов: базовый R и tidyverse 5.7 Purrr 5.8 Пример итерации 5.9 Furrr", " Тема 5 Функциональное программирование 5.1 Зачем писать функции? Программировать на R – прежде всего значит писать функции. Несмотря на десятки тысяч функций, обитающих в тысячах пакетов, рано или поздно вам понадобится своя функция, которая будет подходить для решения именно ваших задач. Функция и код – не одно и то же. Чтобы стать функцией, кусок кода должен, как минимум, получить имя. Зачем давать имя коду, который и так работает? Вот три причины, которые приводит Хадли Уикхем: у функции есть выразительное имя, которое облегчает понимание кода; при изменении требований необходимо обновлять код только в одном месте, а не во многих; меньше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом) Writing good functions is a lifetime journey. — Hadley Wickham Чтобы определить функцию, необходимо дать ей имя. Машине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена! Далее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть). Написание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию. Например, нам нужна функция, которая ищет совпадения в двух векторах и возвращает совпавшие элементы. Сначала решим задачу для двух векторов. x &lt;- c(&quot;гнев&quot;, &quot;богиня&quot;, &quot;воспой&quot;) y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) idx &lt;- which(x %in% y) # 2 x[idx] ## [1] &quot;богиня&quot; Теперь заменяем фактические переменные на формальные. common_words &lt;- function(x, y){ idx &lt;- which(x %in% y) x[idx] } И применяем к новым данным. x &lt;- c(&quot;лишь&quot;, &quot;явилась&quot;, &quot;заря&quot;, &quot;розоперстая&quot;, &quot;вестница&quot;, &quot;утра&quot;) y &lt;- c(&quot;вестница&quot;, &quot;утра&quot;, &quot;заря&quot;, &quot;на&quot;, &quot;великий&quot;, &quot;олимп&quot;, &quot;восходила&quot;) common_words(x, y) ## [1] &quot;заря&quot; &quot;вестница&quot; &quot;утра&quot; Ура, все работает! Запомните простое правило: если вы трижды скопировали код, пора писать функцию! 5.2 Область видимости переменных Напишем функцию, которая будет центрировать данные, то есть вычитать среднее из каждого значения (забудем на время, что это уже делает базовая scale()): center &lt;- function(x){ n = x - mean(x) return(n) } x &lt;- c(5, 10, 15) center(x) ## [1] -5 0 5 Внутри нашей функции есть переменная n, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции. 5.3 Аргументы функции Функция может принимать произвольное число аргументов. Доработаем наш код: center &lt;- function(x, na.rm = F){ if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие x - mean(x) # на этот раз без return() } x &lt;- c(5, 10, NA) center(x) ## [1] NA NA NA Что произошло? Почему следующий код выдает другой результат? center(x, na.rm = T) ## [1] -2.5 2.5 Вычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет. center &lt;- function(x, na.rm = F, what_is_your_name){ if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие x - mean(x) # на этот раз без return() } center(x, na.rm = T) ## [1] -2.5 2.5 center(x, na.rm = T, what_is_your_name = &quot;Locusclassicus&quot;) ## [1] -2.5 2.5 Часто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении. center &lt;- function(x){ if (length(x) == 1) {stop(&quot;И без меня посчитает&quot;)} x - mean(x) # на этот раз без return() } x &lt;- 10 center(x) # вернет ошибку 5.4 Векторизируй это Теперь самое главное: если мы хотим применить функцию к каждому элементу вектора, то в большинстве случаев достаточно просто вызвать функцию. Это называется векторизация. Это относится не только ко многим встроенным функциям R, но и к даже к операторам. x + 4 в действительности представляет собой +(x, 4): x &lt;- c(1.2, 2.51, 3.8) `+`(x, 4) ## [1] 5.20 6.51 7.80 Ключевую роль здесь играет переработка данных, о которой мы уже говорили: короткий вектор повторяется до тех пор, пока его длина не сравняется с длиной более длинного вектора. Как-то так: \\[\\left( \\begin{array}{c} 1.2 \\\\ 2.51 \\\\ 3.8 \\end{array} \\right) + \\left( \\begin{array}{c} 4 \\\\ 4 \\\\ 4 \\end{array} \\right)\\] Понимание того, как действуют векторизованные вычисления, очень важно для написания корректного кода. Посмотрите на пример ниже: почему функция is_article() возвращает два значения, хотя на входе только одно? is_article &lt;- function(x){ x == c(&quot;a&quot;, &quot;the&quot;) } x &lt;- &quot;the&quot; is_article(x) ## [1] FALSE TRUE Поскольку векторы сравниваются поэлементно, то функция ниже вернет разный результат в зависимости от того, в каком порядке заданы элементы: x &lt;- c(&quot;just&quot;, &quot;the&quot;) is_article(x) ## [1] FALSE TRUE x &lt;- c(&quot;the&quot;, &quot;just&quot;) is_article(x) # взрыв мозга ## [1] FALSE FALSE Подумайте, вектор какого типа и какой длины вернет код ниже. is_article &lt;- function(x) { articles &lt;- c(&quot;a&quot;, &quot;the&quot;) x %in% articles } x &lt;- c(rep(&quot;the&quot;, 5), rep(&quot;if&quot;, 5)) # is_article(x) 5.5 Векторизованные конструкции 5.5.1 Циклы Еще один способ повторить действия в R, при этом не копируя один и тот же код много раз, – это циклы. Один из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми. — Нормат Мэтлофф Существует два основных цикла: цикл for и цикл while. На практике чаще используется цикл for, потому что цикл while легко отправить в бесконечность. 5.5.1.1 Цикл for Цикл ниже считает количество букв для каждого слова в векторе. y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) result &lt;- c() for(i in y) { n &lt;- nchar(i) result &lt;- c(result, n) } result ## [1] 1 5 3 2 7 6 9 4 В данном случае мы указали, что надо совершить какую-то операцию над каждым элементом вектора; но по сути это избыточно, потому что nchar() тоже векторизована. nchar(y) ## [1] 1 5 3 2 7 6 9 4 Поэтому чаще цикл for применяют к другим структурам данных. Например, к спискам и датафреймам. Загрузим и немного изменим датасет о гапаксах у Платона. Изменения нужны, так как цикл работает для данных только одного типа, в то время как в нашей таблице столбец dialogue содержит символьные строки, а group – фактор. Обратите внимание, что оператор pipe и функции из dplyr работают и с обычными датафреймами: rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- hapax_plato %&gt;% select(-group, -dialogue) # str(hapax_plato) ## &#39;data.frame&#39;: 26 obs. of 3 variables: ## $ words: chr &quot;8745&quot; &quot;8311&quot; &quot;17944&quot; &quot;4950&quot; ... ## $ hapax: chr &quot;36&quot; &quot;31&quot; &quot;122&quot; &quot;104&quot; ... ## $ ratio: chr &quot;0.004&quot; &quot;0.004&quot; &quot;0.007&quot; &quot;0.021&quot; ... Сейчас все данные в нашей таблице имеют тип chr, то есть строка, и при помощи цикла мы можем их трансформировать. for (i in seq_along(hapax_plato)) { # seq_along ≈ 1:length(x) hapax_plato[,i] &lt;- as.numeric(hapax_plato[,i]) } str(hapax_plato) # убеждаемся, что все получилось ## &#39;data.frame&#39;: 26 obs. of 3 variables: ## $ words: num 8745 8311 17944 4950 4169 ... ## $ hapax: num 36 31 122 104 19 87 15 125 12 32 ... ## $ ratio: num 0.004 0.004 0.007 0.021 0.005 0.007 0.003 0.005 0.003 0.008 ... При помощи циклов можно не только трансформировать данные, но и создавать новые. Чтобы посчитать среднее для столбца, цикл писать не надо: для этого есть функция colSums() (или, для других задач, rowSums()). А вот посчитать медиану таким образом не получится, тут может пригодиться цикл. library(tictoc) tic() medians &lt;- c() for (i in seq_along(hapax_plato)) { m &lt;- median(hapax_plato[,i]) medians &lt;- c(medians, m) } toc() ## 0.003 sec elapsed medians ## [1] 15589.500 94.500 0.007 Мы сохранили результат, инициировав пустой вектор, к которому затем привязали данные по каждому столбцу. Это не всегда хорошая идея, поскольку для больших данных может сильно замедлить цикл15. Еще один способ – сразу инициировать вектор нужной длины. Сравнить скорость можно при помощи пакета tictoc. tic() medians &lt;- vector(&quot;double&quot;, ncol(hapax_plato)) for (i in seq_along(hapax_plato)) { medians[i] &lt;- median(hapax_plato[,i]) } toc() ## 0.003 sec elapsed Второй способ чуть быстрее, и для больших данных это может быть существенно (знала бы я это раньше). Вы уже заметили, что в циклах часто используется буква i. Но никакой особой магии в ней нет! 5.5.1.2 Цикл while Как уже говорилось, с циклами while стоит быть осторожнее. Посмотрите, например, на этот цикл, который перебирает слова, пока не найдет слово длиной 6 букв. Что могло пойти не так? tic() k &lt;- 0 n &lt;- 0 while (n != 6) { k &lt;- k + 1 n &lt;- nchar(y[k]) } y[k] ## [1] &quot;богиня&quot; toc() ## 0.004 sec elapsed То же самое можно сделать без цикла, причем быстрее! tic() y[nchar(y) == 6][1] ## [1] &quot;богиня&quot; toc() ## 0.001 sec elapsed В целом, ничего незаконного в циклах нет, но множество вложенных друг в друга циклов сложно воспринимать; порой они могут замедлить выполнение кода. И в базовом R, и в диалекте tidyverse для этого есть несколько решений, о которых скажем чуть ниже. Сначала рассмотрим еще одну векторизованную конструкцию – условие. 5.5.2 Условия Иногда необходимо ограничить выполнение функции неким условием. Короткие условия можно писать в одну строку без фигурных скобок. if(any(nchar(y) &gt; 6)) print(&quot;многабукв&quot;) ## [1] &quot;многабукв&quot; Более сложные и множественные условия требуют фигурных скобок. Можно сравнить это с условным периодом: протасис (всегда либо TRUE, либо FALSE) в круглых скобках, аподосис в фигурных. if (sum(nchar(y)) &gt; 10) { print(&quot;много букв&quot;) } else if (sum(nchar(y)) &lt; 5) { print(&quot;мало букв&quot;) } else { print(&quot;норм букв&quot;) } ## [1] &quot;много букв&quot; Также в R можно использовать специальную функцию: ifelse((sum(nchar(y)) &gt; 10), &quot;много букв&quot;, &quot;мало букв&quot;) ## [1] &quot;много букв&quot; Прописывая условие, не забывайте, что применение бинарного оператора к вектору возвращает логический вектор: x &lt;- c(1:10) x &gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE Условия с таким логическим вектором используют первый его элемент (а вряд ли это то, что вам нужно): if (x &gt;= 5) print(&quot;все сработало&quot;) ## Warning in if (x &gt;= 5) print(&quot;все сработало&quot;): the condition has length &gt; 1 and ## only the first element will be used Можно скорректировать код так: if (any(x &gt;= 5)) print(&quot;все сработало&quot;) ## [1] &quot;все сработало&quot; По той же причине внутри условия не надо использовать логические операторы | (“или”) или &amp; (“и”), потому что они векторизованы: x &lt; 3 | x &gt; 7 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE Вместо этого можно применять || (“или”) или &amp;&amp; (“и”), которые остановятся, дойдя до первого истинного значения. x &lt; 3 || x &gt; 7 ## [1] TRUE 5.6 Вместо циклов: базовый R и tidyverse Функция tapply() из базового R принимает на входе вектор, фактор (или список факторов) и функцию. Каждый фактор должен быть той же длины, что и вектор. Код ниже считает средний процент гапаксов по группам диалогов: load(&quot;./data/HapaxPlato.Rdata&quot;) # подготавливаем векторы my_fct &lt;- as.factor(hapax_plato$group) my_vct &lt;- as.numeric(hapax_plato$ratio) # применяем к ним функцию mean() tapply(my_vct, my_fct, mean) ## 1 2 3 ## 0.00550000 0.00750000 0.01133333 На диалекте tidyverse эта задача решается так: hapax_plato %&gt;% mutate(ratio = as.numeric(ratio)) %&gt;% group_by(group) %&gt;% summarise(mean = mean(ratio)) ## # A tibble: 3 × 2 ## group mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0055 ## 2 2 0.0075 ## 3 3 0.0113 Функция apply() вызывает функцию для каждой строки или столбца матрицы или датафрейма. # избавляемся от факторов и строк rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- subset(hapax_plato, select = -c(dialogue, group)) # преобразуем столбцы в числовой формат при помощи apply tic() hapax_plato&lt;- apply(hapax_plato, 2, as.numeric) toc() ## 0 sec elapsed Сравните со скростью цикла, который мы написали выше: load(&quot;./data/HapaxPlato.Rdata&quot;) # избавляемся от факторов и строк rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- subset(hapax_plato, select = -c(dialogue, group)) tic() for (i in seq_along(hapax_plato)) { hapax_plato[,i] &lt;- as.numeric(hapax_plato[,i]) } toc() ## 0.002 sec elapsed Функция apply() позволяет применять к данным собственные функции, в том числе анонимные. hapax_centered &lt;- apply(hapax_plato, 2, function(x) x - mean(x)) head(hapax_centered) ## words hapax ratio ## Apology -10619.423 -110.69231 -0.0031538462 ## Charmides -11053.423 -115.69231 -0.0031538462 ## Cratylus -1420.423 -24.69231 -0.0001538462 ## Critias -14414.423 -42.69231 0.0138461538 ## Crito -15195.423 -127.69231 -0.0021538462 ## Euthydemus -6911.423 -59.69231 -0.0001538462 Опять-таки, все это решается (даже проще) в грамматике dplyr: as_tibble(hapax_plato) %&gt;% mutate(words = words - mean(words), hapax = hapax - mean(hapax), ratio = ratio - mean(ratio)) Видно, что по времени мы при этом сильно не выигрываем; к тому же, нам пришлось повторить один код три раза. Значит, надо что-то менять. Например, так16: tic() as_tibble(hapax_plato) %&gt;% mutate_all(function(x) x - mean(x)) toc() Или даже так17: fn &lt;- function(x) x - mean(x) as_tibble(hapax_plato) %&gt;% mutate(across(1:3, fn)) %&gt;% invisible() В любом случае, нам удалось обойтись без цикла, код понятен и хорошо читается. Функции lapply() и sapply() подходят для применения функций к спискам (и к датафреймам, которые по сути представляют собой прямоугольные списки). Чтобы понять, как они работают, сначала создадим список. При анализе текста со списками приходится иметь дело достаточно часто: объекты типа stylo.corpus, которые создает пакет stylo, по сути являются списками. Создадим игрушечный корпус из двух игрушечных текстов. x &lt;- c(&quot;гнев&quot;, &quot;богиня&quot;, &quot;воспой&quot;) y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) corpus &lt;- list(x = x, y = y) Наш условный корпус – это список из 2 элементов (текстов), а каждый текст хранится как символьный вектор. Допустим, мы хотим взять из каждого диалога выборку размером 5 слов, то есть применить функцию sample() к элементам списка. При помощи lapply() (l = list) это делается так: set.seed(0211) lapply(corpus, sample, 5, replace = T) ## $x ## [1] &quot;воспой&quot; &quot;воспой&quot; &quot;воспой&quot; &quot;богиня&quot; &quot;воспой&quot; ## ## $y ## [1] &quot;державная&quot; &quot;вложила&quot; &quot;то&quot; &quot;мысли&quot; &quot;вложила&quot; Функция sapply() ведет себя так же, но упрощает результат до вектора или матрицы (s = simplify). sapply(corpus, sample, 5, replace = T) ## x y ## [1,] &quot;богиня&quot; &quot;ему&quot; ## [2,] &quot;гнев&quot; &quot;державная&quot; ## [3,] &quot;гнев&quot; &quot;ему&quot; ## [4,] &quot;богиня&quot; &quot;то&quot; ## [5,] &quot;воспой&quot; &quot;мысли&quot; Функция vapply() позволяет задать тип данных на выходе. vapply(corpus, sample, size = 5, replace = T, character(5)) ## x y ## [1,] &quot;воспой&quot; &quot;то&quot; ## [2,] &quot;богиня&quot; &quot;гера&quot; ## [3,] &quot;гнев&quot; &quot;вложила&quot; ## [4,] &quot;гнев&quot; &quot;гера&quot; ## [5,] &quot;гнев&quot; &quot;мысли&quot; Поскольку наш “корпус” – это список, то применить грамматику dplyr не очень удобно, списко легко превращается в таблицу: stack(corpus) # передвинуть и переименовать: `relocate()` и `rename()` ## values ind ## 1 гнев x ## 2 богиня x ## 3 воспой x ## 4 в y ## 5 мысли y ## 6 ему y ## 7 то y ## 8 вложила y ## 9 богиня y ## 10 державная y ## 11 гера y Теперь повторные выборки можно делать так: set.seed(0211) stack(corpus) %&gt;% group_by(ind) %&gt;% sample_n(size = 5, replace = T) ## # A tibble: 10 × 2 ## # Groups: ind [2] ## values ind ## &lt;chr&gt; &lt;fct&gt; ## 1 воспой x ## 2 воспой x ## 3 воспой x ## 4 богиня x ## 5 воспой x ## 6 державная y ## 7 вложила y ## 8 то y ## 9 мысли y ## 10 вложила y 5.7 Purrr По-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse18. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham and Grolemund 2017). You should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. — Hadley Wickham &amp; Garrett Grolemund В семействе функций map_ из этого пакета всего 23 вариации19. Вот основные из них: map() map_lgl() map_int() map_dbl() map_chr() Все они принимают на входе данные и функцию, которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_df() – таблицу: hapax_plato %&gt;% as_tibble() %&gt;% map_df(center) %&gt;% head() ## # A tibble: 6 × 3 ## words hapax ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -10619. -111. -0.00315 ## 2 -11053. -116. -0.00315 ## 3 -1420. -24.7 -0.000154 ## 4 -14414. -42.7 0.0138 ## 5 -15195. -128. -0.00215 ## 6 -6911. -59.7 -0.000154 Если на выходе требуется числовой вектор, то используем суффикс dbl: round(map_dbl(hapax_plato, mean), 3) # это именованный вектор! ## words hapax ratio ## 19364.423 146.692 0.007 Если необходимо несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2() 20. Аргументы, которые меняются при каждом вызове, пишутся до функции; аргументы, которые остаются неизменны, – после. mean = list(5, 10, -3) sd = list(1, 5, 50) map2(mean, sd, rnorm, n = 5) ## [[1]] ## [1] 5.727572 4.812749 4.528140 6.565481 4.356073 ## ## [[2]] ## [1] 18.6183347 0.5775309 23.3592929 3.2133071 15.5702820 ## ## [[3]] ## [1] -10.13466 51.54818 -23.10523 -12.53555 -66.80735 Как работает map2() Это можно обобщить следующим образом (источник){ width=60% }: Можно было бы предположить, что должны быть и map3(), map4() и т.д., но во всех случаеях, когда у функции больше двух аргументов, используется pmap(). 5.8 Пример итерации Функция map2() в анализе текста: функция, которая принимает на входе список таблиц, созданных функцией slide, и назначает каждому окну id.21. Используется для создания скользящего окна при создании эмбеддингов. corpus_tbl &lt;- as_tibble(stack(corpus)) windows &lt;- slider::slide(corpus_tbl, ~.x, .after = 1) out &lt;- map2(.x = windows, .y = 1:length(windows), ~ mutate(.x, window_id = .y)) # out is a list out[2] ## [[1]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 богиня x 2 ## 2 воспой x 2 Поскольку второй аргумент – это, по сути, индекс, можно было бы использовать функцию imap(): out &lt;- imap(.x = windows, ~ mutate(.x, window_id = .y)) out[2:3] ## [[1]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 богиня x 2 ## 2 воспой x 2 ## ## [[2]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 воспой x 3 ## 2 в y 3 5.9 Furrr Про параллельные вычисления, если останутся силы. Литература "],["импорт-и-экспорт-данных.html", "Тема 6 Импорт и экспорт данных 6.1 Рабочая директория 6.2 Чтение файлов из интернета 6.3 Чтение локальных файлов 6.4 xml и html 6.5 json 6.6 GutenbergR 6.7 RPerseus 6.8 Чтение нескольких файлов 6.9 Экспорт", " Тема 6 Импорт и экспорт данных 6.1 Рабочая директория Любой анализ данных начинается с импорта данных. Прежде чем что-то делать, проверьте свою рабочую директорию при помощи getwd() (подробнее). Для смены можно использовать как абсолютный, так и относительный путь: setwd(&quot;/Users/olga/R_Workflow/Text_Analysis_2023&quot;) # искать в текущей директории setwd(&quot;./Text_Analysis_2023&quot;) # перейти на уровень вверх setwd(&quot;../&quot;) 6.2 Чтение файлов из интернета Основная функция для скачивания файлов из Сети – download.file(), которой необходимо задать в качестве аргументов url, название сохраняемого файла, иногда также метод. Попробуем скачать датасет из Репозитория открытых данных по русской литературе и фольклору под названием “Байрон в русских переводах 1810–1860-х годов”. url &lt;- &quot;https://dataverse.pushdom.ru/api/access/datafile/:persistentId?persistentId=doi:10.5072/openlit-2019.11-R002/VQRXXK&quot; # если url начинается с https, на Mac _может_ потребоваться указать method = &quot;curl&quot; download.file(url, destfile = &quot;files/Byron.tab&quot;) После этого в папке files появится новый файл. Получить список скачанных файлов можно при помощи list.files(). list.files(&quot;./files&quot;) ## [1] &quot;03132019_Excel_Socrates.xlsx&quot; &quot;1922_Июль_.gexf&quot; ## [3] &quot;AmazonBooks.xlsx&quot; &quot;antibarbari_archive&quot; ## [5] &quot;archive.zip&quot; &quot;augustinus&quot; ## [7] &quot;BX-CSV-Dump&quot; &quot;Byron.tab&quot; ## [9] &quot;CiceroOff.txt&quot; &quot;karamzin_liza.pdf&quot; ## [11] &quot;karamzin_liza.txt&quot; &quot;names.txt&quot; ## [13] &quot;Persons_EDGES.csv&quot; &quot;template.docx&quot; ## [15] &quot;War_and_Peace.xml&quot; 6.3 Чтение локальных файлов Основные функции для чтения локальных файлов в базовом R: read.table() `read.csv()`` 6.3.1 csv и tsv Файл, который мы скачали, имеет расширение .tab. Такие файлы по структуре аналогичны файлам .tsv (tab separated values). Чтобы его прочитать, используем read.table(), указав тип разделителя: Byron &lt;- read.table(&quot;files/Byron.tab&quot;, sep = &quot;\\t&quot;, header = TRUE) head(Byron) Функция read.csv() отличается лишь тем, что автоматически выставляет значения аргументов sep = \",\", header = TRUE. В диалекте tidyverse для импорта подобных файлов используется пакет readr22. Вернемся к нашему датасету про буккроссинг, файлы которого имеют расширение csv. К сожалению, это не всегда гарантия того, что перед вами действительно csv: library(readr) users &lt;- read_csv(&quot;files/BX-CSV-Dump/BX-Users.csv&quot;) ## Rows: 278858 Columns: 1 ## ── Column specification ─────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): User-ID;Location;Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(users) ## # A tibble: 6 × 1 ## `User-ID;Location;Age` ## &lt;chr&gt; ## 1 1;nyc, new york, usa;NULL ## 2 2;stockton, california, usa;18 ## 3 3;moscow, yukon territory, russia;NULL ## 4 4;porto, v.n.gaia, portugal;17 ## 5 5;farnborough, hants, united kingdom;NULL ## 6 6;santa monica, california, usa;61 Чтобы исправить дело, воспользуемся другой функцией из того же пакета: users &lt;- read_delim(&quot;files/BX-CSV-Dump/BX-Users.csv&quot;, delim = &quot;;&quot;) ## Rows: 246666 Columns: 3 ## ── Column specification ─────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (2): Location, Age ## dbl (1): User-ID ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. users ## # A tibble: 246,666 × 3 ## `User-ID` Location Age ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 nyc, new york, usa NULL ## 2 2 stockton, california, usa 18 ## 3 3 moscow, yukon territory, russia NULL ## 4 4 porto, v.n.gaia, portugal 17 ## 5 5 farnborough, hants, united kingdom NULL ## 6 6 santa monica, california, usa 61 ## 7 7 washington, dc, usa NULL ## 8 8 timmins, ontario, canada NULL ## 9 9 germantown, tennessee, usa NULL ## 10 10 albacete, wisconsin, spain 26 ## # ℹ 246,656 more rows Очевидно, это не решает всех проблем, но как справиться с оставшимися, мы рассказывали в уроке об опрятных данных. 6.3.2 xls и xlsx Не самый любимый аналитиками, но очень распространенный тип файлов. Чтобы с ним работать, нужно установить пакет readxl из семейства tidyverse23. Это не единственный пакет для работы с Excel, но, пожалуй, самый удобный. Файл с самыми популярными на Amazon книгами можно взять здесь. library(readxl) amazon &lt;- read_excel(&quot;files/AmazonBooks.xlsx&quot;) head(amazon) ## # A tibble: 6 × 7 ## Name Author `User Rating` Reviews Price Year Genre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10-Day Green Smoothie Cleanse JJ Sm… 4.7 17350 8 2016 Non … ## 2 11/22/63: A Novel Steph… 4.6 2052 22 2011 Fict… ## 3 12 Rules for Life: An Antidote… Jorda… 4.7 18979 15 2018 Non … ## 4 1984 (Signet Classics) Georg… 4.7 21424 6 2017 Fict… ## 5 5,000 Awesome Facts (About Eve… Natio… 4.8 7665 12 2019 Non … ## 6 A Dance with Dragons (A Song o… Georg… 4.4 12643 11 2011 Fict… 6.3.3 txt Для чтения текстовых файлов в базовом R есть функция readlines(). readLines(con = &quot;files/karamzin_liza.txt&quot;, n = 1) ## [1] &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели -- куда глаза глядят -- по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москву, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. &quot; 6.3.4 doc Если есть возможность конвертировать документ Word в простой текстовый формат, то лучше так и сделать. Если нет, то устанавливаем пакет officer. library(officer) files &lt;- list.files(path = &quot;files&quot;, pattern = &quot;docx&quot;) files[1] ## [1] &quot;template.docx&quot; # read file doc &lt;- read_docx(paste0(&quot;files/&quot;, files[1])) content &lt;- docx_summary(doc) head(content, 2) # весь текст доступен в столбце text ## doc_index content_type style_name text level num_id ## 1 1 paragraph NA Chapter Title NA NA ## 2 2 paragraph NA Author’s Name NA NA Таким образом, однако, мы теряем все сноски. Следующий код позволяет их достать: library(xml2) xml_text(xml_find_all(doc$footnotes$get(), &quot;*&quot;)) Тут уже применяются функции для работы с xml. Поэтому лишний раз подумайте, не проще ли конвертировать документ Word в .txt. 6.3.5 pdf С pdf тоже без нужды лучше не иметь дела. Но если все-таки пришлось читать pdf, для этого есть пакет pdftools24. library(pdftools) ## Using poppler version 22.02.0 # длинющий вектор, который придется очищать от \\n (новая строка) liza &lt;- pdf_text(pdf = &quot;files/karamzin_liza.pdf&quot;) # метаданные в виде списка meta &lt;- pdf_info(pdf = &quot;files/karamzin_liza.pdf&quot;) meta$created ## [1] &quot;2023-07-16 19:36:09 MSK&quot; Разработчики утверждают, что пакет справится и с распознаванием текста, но установка нужных для этого зависимостей может быть сопряжена с такими трудостями, что вы, вероятно, захотите решить эту задачу за пределами R. Например, тут. 6.3.6 zip Для работы с архивами есть функция unzip(). Полезно помнить, что большой архив не обязательно распечатывать полностью. Если выставить аргумент list = TRUE, то функция вернет список всех файлов в архиве, из которых можно прочитать в память лишь избранные: archive &lt;- unzip(&quot;files/archive.zip&quot;, files = NULL, list = TRUE) archive ## Name Length Date ## 1 AmazonBooks - Sheet1.csv 4294967295 2021-02-18 19:58:00 ## 2 AmazonBooks.xlsx 4294967295 2021-02-18 19:58:00 Код ниже позволяет извлечь из архива только нужный файл: unzip(&quot;files/archive.zip&quot;, files = &quot;AmazonBooks.xlsx&quot;) После этого файл можно прочитать в R, как указано выше. 6.4 xml и html XML и HTML – это языки разметки. Язык HTML применяется для создания стандартных веб-страниц, поэтому если вы хотите достать некий текст из Интернета, то скорее всего вместе с текстом утащите еще так называемые теги (в треугольных скобках), то есть элементы этой самой разметки: url &lt;- &quot;https://www.thelatinlibrary.com/cicero/off1.shtml&quot; doc_html &lt;- scan(url, what = &quot;character&quot;, sep = &quot;\\n&quot;) head(doc_html) ## [1] &quot;&lt;html&gt;&quot; ## [2] &quot;\\t&lt;head&gt;&quot; ## [3] &quot;\\t\\t&lt;title&gt;&quot; ## [4] &quot;\\t\\t\\tCicero: de Officiis I&quot; ## [5] &quot;\\t\\t&lt;/title&gt;&quot; ## [6] &quot;\\t\\t&lt;meta http-equiv=\\&quot;Content-Type\\&quot; content=\\&quot;text/html; charset=utf-8\\&quot;&gt;&quot; Для анализа текста эти, как правило, не нужны, так как отвечают за оформление. Они “сообщают” браузеру, как отображать тот или иной контент. Пакет stylo дает возможность легко от них избавиться. Текст возвращается без тегов и уже разделенным на слова. library(stylo) # download.file(url, destfile = &quot;files/CiceroOff.txt&quot;) doc_text &lt;- load.corpus.and.parse(files = &quot;CiceroOff.txt&quot;, corpus.dir = &quot;files&quot;, markup.type = &quot;html&quot;, encoding = &quot;UTF-8&quot;) doc_vec &lt;- unlist(doc_text, use.names = FALSE) doc_vec[1:10] ## [1] &quot;m&quot; &quot;tvlli&quot; &quot;ciceronis&quot; &quot;de&quot; &quot;officiis&quot; &quot;liber&quot; ## [7] &quot;primvs&quot; &quot;quamquam&quot; &quot;te&quot; &quot;marce&quot; XML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” значит, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки. Один из таких настраиваемых языков – это TEI (Text Encoding Initiative). Большая часть размеченных литературных корпусов хранится именно в таком виде. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п. Добавлять и удалять разметку может любой пользователь в редакторе XML кода. По сути, это просто текст, хотя и причудливо (на первый взгляд) оформленный. Подбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan and Lang 2014) и (Холзнер 2004). Здесь мы лишь кратко затронем вопрос о том, как парсить XML документ в R, то есть извлекать из него нужную нам информацию. Главное, что надо понимать: любой XML документ представляет собой иерархически организованное дерево, у которого есть один и только один корневой элемент, из которого расходятся ветви. С этим деревом в R можно работать двумя способами: либо как со списком, который содержит другие вложенные в него списки, либо при помощи синтаксиса XPath. В видео по ссылке вы можете ближе познакомиться и с тем, и с другим подходом. В качестве примера загрузим еще один датасет “Пушкинсого дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”. Функция xmlTreeParse() разбирает XML-файл и генерирует R-структуру, представляющую дерево XML. Парсинг xml начинается с поиска корневого элемента. library(XML) filename = &quot;files/War_and_Peace.xml&quot; doc &lt;- xmlTreeParse(filename, useInternalNodes = T) rootnode &lt;- xmlRoot(doc) После этого можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена: names(xmlChildren(rootnode)) ## [1] &quot;teiHeader&quot; &quot;text&quot; Очевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее (индексирование как для списков): names(xmlChildren(rootnode[[&quot;text&quot;]])) ## [1] &quot;div&quot; &quot;div&quot; &quot;div&quot; &quot;div&quot; &quot;div&quot; Итак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это том. # это список divs_1 &lt;- rootnode[[&quot;text&quot;]][[&quot;div&quot;]] xmlGetAttr(divs_1, &quot;type&quot;) ## [1] &quot;volume&quot; Добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – пр помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на уровень ниже: divs_2 &lt;- getNodeSet(doc, &quot;/tei:TEI//tei:text//tei:div//tei:div&quot;, namespaces = c(tei = &quot;http://www.tei-c.org/ns/1.0&quot;)) length(divs_2) ## [1] 375 Теперь мы получили довольно длинный список (375 элементов), к которому снова можем применить xmlGetAttr(). Выясняем, что это, в основном, главы: unique(sapply(divs_2, xmlGetAttr, &quot;type&quot;)) ## [1] &quot;part&quot; &quot;chapter&quot; Чтобы извлечь текст из конкретного узла, нужна функция xmlValue. # забираем 2, т.к. 1 -- это часть, а не глава chapter_1 &lt;- xmlValue(divs_2[[2]]) Распечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на строки и выведем первую и последнюю: library(stringr) chapter_lines &lt;- str_split(chapter_1, pattern = &quot;\\n&quot;) chapter_lines[[1]][[5]] ## [1] &quot; — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j&#39;y crois) — je ne vous connais plus, vous n&#39;êtes plus mon ami, vous n&#39;êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.&quot; chapter_lines[[1]][[838]] ## [1] &quot; Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.&quot; Первая и последняя реплика по-французски: все правильно! Подробнее о работе с XML (и HTML) стоит посмотреть вот это видео. P.S. Для работы с XML также подходит пакет xml225. 6.5 json To be supplied. 6.6 GutenbergR Для R существуют пакеты, позволяющие извлекать тексты из онлайн-библиотек. Пакет GutenbergR26 поможет достать тексты из библиотеки Gutenberg, но будьте осторожны: распознаны они не всегда хорошо и порой содержат много разного шума, например примечания редактора, номера страниц и т.п. В билингвах источник и перевод могут идти вперемешку. И если в XML подобные элементы будут окружены соответствующими тегами, которые позволят их легко отбросить при анализе, то Gutenberg дает вам сырой текст. Часто его надо хорошенько чистить при помощи регулярных выражений или даже вручную. Поиск нужного текста лучше начать с изучения метаданных: library(gutenbergr) ## метаданные содержат множество NA gutenberg_authors ## # A tibble: 21,323 × 7 ## gutenberg_author_id author alias birthdate deathdate wikipedia aliases ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 United States U.S.… NA NA https://… &lt;NA&gt; ## 2 3 Lincoln, Abr… &lt;NA&gt; 1809 1865 https://… United… ## 3 4 Henry, Patri… &lt;NA&gt; 1736 1799 https://… &lt;NA&gt; ## 4 5 Adam, Paul &lt;NA&gt; 1849 1931 https://… &lt;NA&gt; ## 5 7 Carroll, Lew… Dodg… 1832 1898 https://… &lt;NA&gt; ## 6 8 United State… &lt;NA&gt; NA NA https://… Agency… ## 7 9 Melville, He… Melv… 1819 1891 https://… &lt;NA&gt; ## 8 10 Barrie, J. M… &lt;NA&gt; 1860 1937 https://… Barrie… ## 9 12 Smith, Josep… Smit… 1805 1844 https://… &lt;NA&gt; ## 10 14 Madison, Jam… Unit… 1751 1836 https://… &lt;NA&gt; ## # ℹ 21,313 more rows gutenberg_metadata ## # A tibble: 69,199 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 &quot;The De… Jeffe… 1638 en Politics/American … ## 2 2 &quot;The Un… Unite… 1 en Politics/American … ## 3 3 &quot;John F… Kenne… 1666 en &lt;NA&gt; ## 4 4 &quot;Lincol… Linco… 3 en US Civil War ## 5 5 &quot;The Un… Unite… 1 en United States/Poli… ## 6 6 &quot;Give M… Henry… 4 en American Revolutio… ## 7 7 &quot;The Ma… &lt;NA&gt; NA en &lt;NA&gt; ## 8 8 &quot;Abraha… Linco… 3 en US Civil War ## 9 9 &quot;Abraha… Linco… 3 en US Civil War ## 10 10 &quot;The Ki… &lt;NA&gt; NA en Banned Books List … ## # ℹ 69,189 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Функция gutenberg_works() позволяет уточнить, какие столбцы необходимо вернуть: latin_works &lt;- gutenberg_works(languages = &quot;la&quot;) latin_works ## # A tibble: 92 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 C. Iuli… Caesa… 3621 la Classical Antiquity ## 2 226 Cicero&#39;… Cicer… 128 la Classical Antiquity ## 3 227 Aeneidos Virgil 129 la Classical Antiquit… ## 4 229 The Buc… Virgil 129 la Classical Antiquity ## 5 231 Georgic… Virgil 129 la Classical Antiquity ## 6 237 Sexti P… Prope… 133 la &lt;NA&gt; ## 7 825 Latin V… Anony… 216 la Christianity ## 8 826 Latin V… Anony… 216 la Christianity ## 9 828 Latin V… Anony… 216 la Christianity ## 10 4317 Prophet… Anony… 216 la &lt;NA&gt; ## # ℹ 82 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Беглый взгляд на эту таблицу говорит о том, что в галактике Гутенберга царит полный хаос: например, в поле “название” хранится имя автора. Но после того, как нужный автор или нужное сочинение найдены, можно сделать так: caesar &lt;- gutenberg_works(author == &quot;Caesar, Julius&quot;, languages = &quot;la&quot;) caesar ## # A tibble: 3 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 C. Iuli … Caesa… 3621 la Classical Antiquity ## 2 18837 Commenta… Caesa… 3621 la Classical Antiquity ## 3 29645 The Gate… Caesa… 3621 la &lt;NA&gt; ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Чтобы извлечь отдельный текст (тексты): de_bello_gallico &lt;- gutenberg_download(218, meta_fields = &quot;title&quot;, mirror = &quot;ftp://mirrors.xmission.com/gutenberg/&quot;) de_bello_gallico ## # A tibble: 2,552 × 3 ## gutenberg_id text title ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 &quot;CAESAR&#39;S COMMENTARIES IN LATIN&quot; C. Iuli Caesaris De Bello Gall… ## 2 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 3 218 &quot;Books I-IV&quot; C. Iuli Caesaris De Bello Gall… ## 4 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 5 218 &quot;By Julius Caesar&quot; C. Iuli Caesaris De Bello Gall… ## 6 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 7 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 8 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 9 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 10 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## # ℹ 2,542 more rows Существует несколько зеркал библиотеки Gutenberg, и, если при выполнении функции gutenberg_download() возникает ошибка “could not download a book at http://aleph.gutenberg.org/”, то следует использовать аргумент mirror. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL 6.7 RPerseus To be supplied. 6.8 Чтение нескольких файлов To be supplied. 6.9 Экспорт To be supplied. 6.9.1 Rdata To be supplied. Литература "],["воспроизводимые-исследования.html", "Тема 7 Воспроизводимые исследования 7.1 О воспроизводимости 7.2 Markdown 7.3 Синтаксис Markdown 7.4 Публикация html", " Тема 7 Воспроизводимые исследования 7.1 О воспроизводимости Полученный в результате количественных исследований результат должен быть проверяем и воспроизводим. Даже на заре стилометрии, когда все вычисления проводились вручную, ученые стремились максимально подробно задокументировать свои вычисления: какие слова они считали, в каких текстах и т.п. Об одном исключении из этого правила можно прочитать вот здесь. Сегодня к документации исследования предъявляются гораздо более строгие требования: в большинстве случаев недостаточно просто рассказать, что вы проделали. Теоретически читатель должен иметь возможность проделать тот же путь, что и автор: вопроизвести его результаты, но в обратном направлении. Воспроизводимость (reproducibility) – это не то же, что повторяемость (replicability). Ученый, который повторяет исследование, проводит его заново на новых данных. Воспроизведение – гораздо более скромная задача, не требующая таких ресурсов, как повторение (Winter 2020, 47). Иллюстрация ниже заимствована из книги Роджера Пенга, специалиста по биостатистике27. За тем исключением, что вместо квадратика “Nature” в гуманитарном исследовании будет квадратик “Culture”, общие принципы те же: все, что вы делаете – от сбора данных до их оформления в виде графиков – должно быть задокументировано и воспроизводимо. Для этого должны выполняться три основных требования: доступность данных и метаданных; доступность компьютерного кода; доступность программного обеспечения. Именно поэтому всегда, когда возможно, преимущество должно отдаваться свободно распространяемому ПО. В этом смысле R имеет преимущество перед такими программами, как SPSS, SAS, Matlab, STATA и др. Все, что вы делаете на “не имеющем аналогов в мире” закрытом ПО может быть увлекательно лично для вас, но не отвечает научным критериям проверяемости. Поэтому многие разработчики, создающие приложения для анализа текста, тоже выкладывают их на GitHub. (Пример). Правило второ: код имеет преимущество перед GUI (Graphical User Interface): вспомнить, какие кнопки в каком порядке были нажаты, даже самому автору бывает непросто. От скачивания файла до экспорта графиков – все должно быть зафисировано, причем в виде, понятном не только для машины, но и для человека. Некоторые пакеты в R оснащены GUI (например, stylo), но пользоваться им лучше умеренно, пока вы только знакомитесь с инструментом. Правило третье: код и сырые данные для статьи принято публиковать на GitHub. Исследователи, работающие с разными изданиями Аристотеля, могут прийти к разным выводам. Вопроизвести ваше исследование на других данных может быть невозможно. Если вы работаете с материалом, защищенным копирайтом, на GitHub можно настроить доступ к репозиторию: он не будет виден всем, но, например, рецензенты смогут проверить ваши выводы28. Авторитетный International Journal of Digital Humanities прямо пишет в инструкциях для авторов: Please ensure you provide all relevant editable source files at every submission and revision. Failing to submit a complete set of editable source files will result in your article not being considered for review. Уже на этапе планирования исследования очень важно продумать, как вы будете его документировать. Это делается не после того, как вы все выяснили, а в процессе. Правило четвертое: код пишется не только для машин, но и для людей. Важно документировать не только то, что вы делали, но и почему. R дает для этого множество возможностей, главная из которых – это Markdown29. 7.2 Markdown Markdown – это облегчённый язык разметки. Он позволяет создавать документы разного формата – не только HTML (веб-страницы), но и PDF и Word. В R Markdown создается огромное количество документов - сайтов, статей и книг (например, этот курс), презентаций, отчетов, дашбордов и т.п. При этом Markdown позволяет писать код не только на R, но и других языках – например, на Python; это дает возможность создания полностью воспроизводимых документов, сочетающих код и поясняющий текст. Чтобы начать работать с документами .rmd, нужен пакет rmarkdown; в RStudio он уже предустановлен. Создание нового документа .rmd происходит из меню30: По умолчанию документ .rmd снабжен шапкой yaml. Она не обязательна. Здесь содержатся данные об авторе, времени создания, формате, сведения о файле с библиографией и т.п. --- title: &quot;Demo&quot; author: &quot;My name&quot; date: &quot;2023-08-27&quot; output: html_document --- Также в документе .rmd скорее всего будет простой текст и блоки кода. Чтобы “сшить” html (pdf, doc), достаточно нажать кнопку knit. Либо можно запустить в консоли код: rmarkdown::render(\"Demo.Rmd\"). После этого в рабочей директории появится новый файл (html, pdf, или doc), которым можно поделиться с коллегами, грантодателями или друзьями. 7.3 Синтаксис Markdown 7.3.1 Заголовки Заголовки разного уровня задаются при помощи решетки31: # Заголовок первого уровня ## Заголовок второго уровня ### Заголовок третьего уровня #### Заголовок четвёртого уровня Пример заголовка третьего уровня: 7.3.2 Форматирование *курсив* _курсив_ **полужирный** __полужирный__ ***полужирный курсив*** ___полужирный курсив___ ~~зачеркнутый~~ &lt;mark&gt;выделение&lt;/mark&gt; Пример: курсив полужирный уж и не знаю как выделить зачеркнутый выделение 7.3.3 Списки Нумерованный список 1. Пункт первый 2. Пункт второй 3. Пункт третий Пример: Пункт первый Пункт второй Пункт третий Маркированный список - Пункт первый - Пункт второй - Пункт третий Пример: Пункт первый Пункт второй Пункт третий Также Markdown позволяет делать вложенные списки: 1. Пункт первый - Подпункт первый - Подпункт второй 2. Пункт второй Пример: Пункт первый Подпункт первый Подпункт второй Пункт второй Самое удобное, что элементы списка не обязательно нумеровать: (@) Пункт первый. (@) Пункт не знаю какой. Пункт первый. Пункт не знаю какой. 7.3.4 Ссылки [Текст ссылки](http://antibarbari.ru/) Пример: Текст ссылки 7.3.5 Изображения ![Текст описания](http://antibarbari.ru/wp-content/uploads/2023/03/corderius-656x300.png) Пример: Моя картинка Два нюанса: можно давать ссылки на локальные файлы (то есть такие файлы, которые хранятся на компьютере) изображения можно вставлять, пользуясь непосредственно разметкой html. &lt;img src=&quot;images/my_image.jpg&quot; width=40%&gt; 7.3.6 Блоки кода Можно вставлять непосредственно в текст, это будет выглядеть вот так; для этого код выделяют одинарным обратным апострофом (грависом). Но чаще код дают отдельным блоком. Эти блоки можно именовать; тогда в случае ошибки будет сразу понятно, где она случилась32. ```{} some code here ``` В фигурных скобках надо указать язык, например {r}, тогда код будет автоматически подсвечен. Там же в фигурных скобках можно задать следующие параметры: eval = FALSE код будет показан, но не будет выполняться; include = FALSE код будет выполнен, но ни код, ни результат не будут показаны; echo = FALSE код будет выполнен, но не показан, результаты при этом видны; message = FALSE или warning = FALSE прячет сообщения или предупреждения; results = 'hide' не распечатывает результат, а fig.show = 'hide' прячет графики; error = TRUE “сшивание” продолжается, даже если этот блок вернул ошибку. 7.3.7 Цитаты &gt; Omnia praeclara rara. Пример: Omnia praeclara rara. Цитата с подписью может быть оформлена так: &gt; Omnia praeclara rara. &gt; &gt; --- Cicero Пример: Omnia praeclara rara. — Cicero 7.3.8 Разделители Чтобы создать горизонтальную линию, можно использовать ---, *** или ___. Пример: 7.3.9 Таблицы Таблицы можно задать вручную при помощи дефисов - и вертикальных линий |; идеальная точность при этом не нужна. Перед таблицей обязательно оставляйте пустую строку, иначе волшебство не сработает. | Фрукты | Калории | | ----- | ---- | | Яблоко | 52 | | Апельсин | 47 | Пример: Фрукты Калории Яблоко 52 Апельсин 47 По умолчанию Markdown распечатывает таблицы так, как они бы выглядели в консоли. data(&quot;iris&quot;) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Для дополнительного форматирования можно использовать функцию kable::knitr(): knitr::kable(iris[1:6, ], caption = &quot;Таблица knitr&quot;) Table 7.1: Таблица knitr Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Интерактивную таблицу можно создать так: DT::datatable(iris[1:6,]) 7.3.10 Чек-листы - [x] Таблицы - [ ] Графики Пример: Таблицы Графики 7.3.11 Внутренние ссылки Удобны для навигации по документу. К названию любого раздела можно добавить {#id}. [Вернуться к чек-листам](#id) Пример: Вернуться к чек-листам 7.3.12 Графики Markdown позволяет встраивать любые графики. library(ggplot2) ggplot(aes(x = Sepal.Length, y = Petal.Length, col = Species), data = iris) + geom_point(show.legend = F) Для интерактивных графиков понадобится пакет plotly: library(plotly) plot_ly(data=iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species) 7.3.13 Математические формулы Пишутся с использованием синтаксиса LaTeX, о котором можно прочитать подробнее здесь. Формулы заключаются в одинарный $, если пишутся в строку, и в двойной $$, если отдельным блоком. \\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta Вот так это выглядит в тексте: \\(\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\). А вот так – блоком: \\[\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\] 7.3.14 Библиография Чтобы привязать библиографию, нужно указать имя файла в шапке yaml. --- bibliography: bibliography.bib --- Дальше, чтобы добавить ссылку, достаточно ввести ключ публикации после @ (в квадратных скобках, чтобы публикация отражалась в круглых): [@wickham2016]. Пример: (Wickham and Grolemund 2017). 7.3.15 Смайлы Удобнее вставлять через визуальный редактор (“шестеренка” &gt; Use Visual Editor), но можно и без него: # devtools::install_github(&quot;hadley/emo&quot;) library(emo) emo::ji(&quot;apple&quot;) ## 🍎 Код можно записать в строку, тогда смайл появится в тексте: 💀.33 7.4 Публикация html To be supplied later. Литература "],["регулярные-выражения.html", "Тема 8 Регулярные выражения 8.1 Regex в базовом R 8.2 Литералы и классы 8.3 Якоря 8.4 Метасимволы 8.5 Экранирование 8.6 Квантификация 8.7 Жадная и ленивая квантификация 8.8 Regex в stringr: основы 8.9 str_detect() и str_count() 8.10 str_extract() 8.11 str_subset() и str_match() 8.12 str_replace 8.13 str_split", " Тема 8 Регулярные выражения Есть старая шутка, ее приписывают программисту Джейми Завински: если у вас есть проблема, и вы собираетесь ее решать при помощи регулярных выражений, то у вас две проблемы. Регулярные выражения – это формальный язык, который используется для того, чтобы находить, извлекать и заменять части текста. Регулярные выражения (regex, regexp) объединяют буквальные символы (литералы) и метасимволы (символы-джокеры, англ. wildcard characters). Для поиска используется строка-образец (англ. pattern, по-русски её часто называют “шаблоном”, “маской”), которая задает правило поиска. Строка замены также может содержать в себе специальные символы. Отличный путеводитель по миру регулярных выражений можно найти здесь. 8.1 Regex в базовом R В базовом R за работу со строками отвечают, среди прочего, такие функции, как grep() и grepl(). При этом grepl() возвращает TRUE, если шаблон найден в соответствующей символьной строке, а grep() возвращает вектор индексов символьных строк, содержащих паттерн. Обеим функциям необходим аргумент pattern и аргумент x, где pattern - регулярное выражение, по которому производится поиск, а аргумент x - вектор символов, по которым следует искать совпадения. Функция gsub() позволяет производить замену и требует также аргумента replacement. 8.2 Литералы и классы Буквальные символы – это то, что вы ожидаете увидеть (или не увидеть – для управляющих и пробельных символов); можно сказать, что это символы, которые ничего не “имеют в виду”. Их можно объединять в классы при помощи квадратных скобок, например, так: [abc]. vec &lt;- c(&quot;a&quot;, &quot;d&quot;, &quot;c&quot;) grepl(&quot;[abc]&quot;, vec) ## [1] TRUE FALSE TRUE grep(&quot;[abc]&quot;, vec) ## [1] 1 3 Для некоторых классов есть специальные обозначения. Класс Эквивалент Значение [:upper:] [A-Z] Символы верхнего регистра [:lower:] [a-z] Символы нижнего регистра [:alpha:] [[:upper:][:lower:]] Буквы [:digit:] [0-9], т. е. \\d Цифры [:alnum:] [[:alpha:][:digit:]] Буквы и цифры [:word:] [[:alnum:]_], т. е. Символы, образующие «слово» [:punct:] [-!“#$%&amp;’()*+,./:;&lt;=&gt;?@[\\]_`{|}~] Знаки пунктуации [:blank:] [\\s\\t] Пробел и табуляция [:space:] [[:blank:]\\v\\r\\n\\f], т. е. \\s Пробельные символы [:cntrl:] Управляющие символы (перевод строки, табуляция и т.п.) [:graph:] Печатные символы [:print:] Печатные символы с пробелом Эти классы тоже можно задавать в качестве паттерна. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд1&quot;, &quot;0зебра&quot;) gsub( &quot;[[:digit:]]&quot;, &quot;&quot;, vec) ## [1] &quot;жираф&quot; &quot;верблюд&quot; &quot;зебра&quot; В качестве классов можно рассматривать и следующие обозначения: Представление Эквивалент Значение \\d [0-9] Цифра \\D [^\\\\d] Любой символ, кроме цифры \\w [A-Za-zА-Яа-я0-9_] Символы, образующие «слово» (буквы, цифры и символ подчёркивания) \\W [^\\\\w] Символы, не образующие «слово» \\s [ \\t\\v\\r\\n\\f] Пробельный символ \\S [^\\\\s] Непробельный символ gsub( &quot;\\\\d&quot;, &quot;&quot;, vec) # вторая косая черта &quot;экранирует&quot; первую ## [1] &quot;жираф&quot; &quot;верблюд&quot; &quot;зебра&quot; Внутри квадратных скобор знак ^ означает отрицание: gsub( &quot;[^[:digit:]]&quot;, &quot;&quot;, vec) ## [1] &quot;&quot; &quot;1&quot; &quot;0&quot; 8.3 Якоря Якоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($). vec &lt;- c(&quot;The spring is a lovely time&quot;, &quot;Fall is a time of peace&quot;) grepl(&quot;time$&quot;, vec) ## [1] TRUE FALSE 8.4 Метасимволы Все метасимволы представлены в таблице ниже. Описание Символ открывающая квадратная скобка [ закрывающая квадратная скобка ] обратная косая черта \\ карет ^ знак доллара $ точка . вертикальная черта | знак вопроса ? астериск * плюс + открывающая фигурная скобка { закрывающая фигурная скобка } открывающая круглая скобка ( закрывающая круглая скобка ) Квадратные скобки используются для создания классов, карет и знак доллара – это якоря, но карет внутри квадратных скобор может также быть отрицанием. Точка – это любой знак. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд1&quot;, &quot;0зебра&quot;) grep(&quot;.б&quot;, vec) ## [1] 2 3 8.5 Экранирование Если необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую. vec &lt;- c(&quot;жираф?&quot;, &quot;верблюд.&quot;, &quot;зебра&quot;) grep(&quot;\\\\?&quot;, vec) ## [1] 1 grepl(&quot;\\\\.&quot;, vec) ## [1] FALSE TRUE FALSE 8.6 Квантификация Квантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа. Представление Число повторений Эквивалент ? Ноль или одно {0,1} * Ноль или более {0,} + Одно или более {1,} Пример: vec &lt;- c(&quot;color&quot;, &quot;colour&quot;, &quot;colouur&quot;) grepl(&quot;ou?r&quot;, vec) # ноль или одно ## [1] TRUE TRUE FALSE grepl(&quot;ou+r&quot;, vec) # одно или больше ## [1] FALSE TRUE TRUE grepl(&quot;ou*r&quot;, vec) # ноль или больше ## [1] TRUE TRUE TRUE Точное число повторений (интервал) можно задать в фигурных скобках: Представление Число повторений {n} Ровно n раз {m,n} От m до n включительно {m,} Не менее m {,n} Не более n vec &lt;- c(&quot;color&quot;, &quot;colour&quot;, &quot;colouur&quot;, &quot;colouuuur&quot;) grepl(&quot;ou{1}r&quot;, vec) ## [1] FALSE TRUE FALSE FALSE grepl(&quot;ou{1,2}r&quot;, vec) ## [1] FALSE TRUE TRUE FALSE grepl(&quot;ou{,2}r&quot;, vec) # это включает и ноль! ## [1] TRUE TRUE TRUE FALSE Часто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения. 8.7 Жадная и ленивая квантификация В регулярных выражениях квантификаторам соответствует максимально длинная строка из возможных (квантификаторы являются жадными, англ. greedy). Это может оказаться значительной проблемой. Например, часто ожидают, что выражение &lt;.*&gt; найдёт в тексте теги HTML. Однако если в тексте есть более одного HTML-тега, то этому выражению соответствует целиком строка, содержащая множество тегов. vec &lt;- c(&quot;&lt;p&gt;&lt;b&gt;Википедия&lt;/b&gt; — свободная энциклопедия, в которой &lt;i&gt;каждый&lt;/i&gt; может изменить или дополнить любую статью.&lt;/p&gt;&quot;) gsub(&quot;&lt;.*&gt;&quot;, &quot;&quot;, vec) # все исчезло! ## [1] &quot;&quot; Чтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым. regex значение ?? 0 или 1, лучше 0 *? 0 или больше, как можно меньше +? 1 или больше, как можно меньше {n,m}? от n до m, как можно меньше Пример: gsub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, vec) # все получилось! ## [1] &quot;Википедия — свободная энциклопедия, в которой каждый может изменить или дополнить любую статью.&quot; 8.8 Regex в stringr: основы Пакет stringr не является частью tidyverse, хотя и разделяет его принципы34. Его надо загружать отдельно: library(stringr) Это очень удобный инструмент для работы со строками. Вот так можно узнать длину строки или объединить ее с другими строками: vec &lt;- c(&quot;жираф&quot;, &quot;верблюд&quot;) str_length(vec) ## [1] 5 7 str_c(&quot;красивый_&quot;, vec) ## [1] &quot;красивый_жираф&quot; &quot;красивый_верблюд&quot; Элементы вектора можно объединить в одну строку: str_c(vec, collapse = &quot;, &quot;) # теперь у них общие кавычки ## [1] &quot;жираф, верблюд&quot; С помощью str_sub() и str_sub_all() можно выбрать часть строки35. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд&quot;) str_sub(vec, 1, 3) ## [1] &quot;жир&quot; &quot;вер&quot; str_sub(vec, 1, -2) ## [1] &quot;жира&quot; &quot;верблю&quot; Функции ниже меняют начертание с прописного на строчное или наоборот: VEC &lt;- str_to_upper(vec) VEC ## [1] &quot;ЖИРАФ&quot; &quot;ВЕРБЛЮД&quot; str_to_lower(VEC) ## [1] &quot;жираф&quot; &quot;верблюд&quot; str_to_title(vec) ## [1] &quot;Жираф&quot; &quot;Верблюд&quot; Одна из полезнейших функций в этом пакете – str_view(); она помогает увидеть, что поймало регулярное выражение – до того, как вы внесете какие-то изменения в строку. str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;bef&quot;), &quot;a\\\\.c&quot;) ## [2] │ &lt;a.c&gt; Например, с помощью этой функции можно убедиться, что вертикальная черта выступает как логический оператор “или”: str_view(c(&quot;grey&quot;, &quot;gray&quot;), &quot;gr(e|a)y&quot;) ## [1] │ &lt;grey&gt; ## [2] │ &lt;gray&gt; 8.9 str_detect() и str_count() Аналогом grepl() в stringr является функция str_detect() library(rcorpora) data(&quot;fruit&quot;) head(fruit) ## [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; ## [6] &quot;bilberry&quot; str_detect(head(fruit), &quot;[aeiou]$&quot;) ## [1] TRUE FALSE TRUE TRUE FALSE FALSE # какая доля слов заканчивается на гласный? mean(str_detect(fruit, &quot;[aeiou]$&quot;)) ## [1] 0.35 # сколько всего слов заканчивается на гласный? sum(str_detect(fruit, &quot;[aeiou]$&quot;)) ## [1] 28 Отрицание можно задать двумя способами: data(&quot;words&quot;) no_vowels1 &lt;- !str_detect(words, &quot;[aeiou]&quot;) # слова без гласных no_vowels2 &lt;- str_detect(words, &quot;^[^aeiou]+$&quot;) # слова без гласных sum(no_vowels1 != no_vowels2) ## [1] 0 Логический вектор можно использовать для индексирования: words[!str_detect(words, &quot;[aeiou]&quot;)] ## [1] &quot;by&quot; &quot;dry&quot; &quot;fly&quot; &quot;mrs&quot; &quot;try&quot; &quot;why&quot; Эту функцию можно применять вместе с функцией filter() из пакета dplyr: library(dplyr) gods &lt;- corpora(which = &quot;mythology/greek_gods&quot;) df &lt;- tibble(god = as.character(gods$greek_gods), i = seq_along(god) ) df %&gt;% filter(str_detect(god, &quot;s$&quot;)) ## # A tibble: 18 × 2 ## god i ## &lt;chr&gt; &lt;int&gt; ## 1 Ares 3 ## 2 Artemis 4 ## 3 Dionysus 7 ## 4 Hades 8 ## 5 Hephaestus 9 ## 6 Hermes 11 ## 7 Zeus 14 ## 8 Chaos 17 ## 9 Chronos 18 ## 10 Erebus 19 ## 11 Eros 20 ## 12 Hypnos 21 ## 13 Uranus 22 ## 14 Phanes 24 ## 15 Pontus 25 ## 16 Tartarus 26 ## 17 Thanatos 28 ## 18 Nemesis 31 Вариацией этой функции является str_count(): str_count(as.character(gods$greek_gods), &quot;[Aa]&quot;) ## [1] 1 1 1 1 2 0 0 1 1 1 0 1 0 0 1 2 1 0 0 0 0 1 2 1 0 2 3 2 1 0 0 Эту функцию удобно использовать вместе с mutate() из dplyr: df %&gt;% mutate( vowels = str_count(god, &quot;[AEIOYaeiou]&quot;), consonants = str_count(god, &quot;[^AEIOYaeiou]&quot;) ) ## # A tibble: 31 × 4 ## god i vowels consonants ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Aphrodite 1 4 5 ## 2 Apollo 2 3 3 ## 3 Ares 3 2 2 ## 4 Artemis 4 3 4 ## 5 Athena 5 3 3 ## 6 Demeter 6 3 4 ## 7 Dionysus 7 3 5 ## 8 Hades 8 2 3 ## 9 Hephaestus 9 4 6 ## 10 Hera 10 2 2 ## # ℹ 21 more rows 8.10 str_extract() Функция str_extract() извлекает совпадения36. Рассмотрим ее работу на примере небольшого датасета из пакета stringr. data(&quot;sentences&quot;) length(sentences) ## [1] 720 head(sentences) ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;Rice is often served in round bowls.&quot; ## [6] &quot;The juice of lemons makes fine punch.&quot; Сначала зададим паттерн для поиска. colours &lt;- c(&quot; red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) colour_match ## [1] &quot; red|orange|yellow|green|blue|purple&quot; И применим к предложениями. Используем str_extract_all(), т.к. str_extract() возвращает только первое вхождение. has_colour &lt;- str_subset(sentences, colour_match) matches &lt;- str_extract_all(has_colour, colour_match) head(unlist(matches)) ## [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;yellow&quot; &quot;green&quot; &quot; red&quot; 8.11 str_subset() и str_match() Круглые скобки используются для группировки. Например, мы можем задать шаблон для поиска существительного или прилагательного с артиклем. noun &lt;- &quot;(a|the) ([^ ]+)&quot; # как минимум один непробельный символ после пробела has_noun &lt;- sentences %&gt;% str_subset(noun) %&gt;% head(10) has_noun ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;The box was thrown beside the parked truck.&quot; ## [6] &quot;The boy was there when the sun rose.&quot; ## [7] &quot;The source of the huge river is the clear spring.&quot; ## [8] &quot;Kick the ball straight and follow through.&quot; ## [9] &quot;Help the woman get back to her feet.&quot; ## [10] &quot;A pot of tea helps to pass the evening.&quot; Дальше можно воспользоваться уже известной функцией str_extract() или применить str_match. Результат будет немного отличаться: вторая функция вернет матрицу, в которой хранится не только сочетание слов, но и каждый компонент отдельно. has_noun %&gt;% str_extract(noun) ## [1] &quot;the smooth&quot; &quot;the sheet&quot; &quot;the depth&quot; &quot;a chicken&quot; &quot;the parked&quot; ## [6] &quot;the sun&quot; &quot;the huge&quot; &quot;the ball&quot; &quot;the woman&quot; &quot;a helps&quot; has_noun %&gt;% str_match(noun) ## [,1] [,2] [,3] ## [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; ## [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; ## [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; ## [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; ## [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; ## [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; ## [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; ## [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; ## [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; ## [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; Функция tidyr::extract работает похожим образом, но требует дать имена для каждого элемента группы. Этим удобно пользоваться, если ваши данные хранятся в виде тиббла. tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;article&quot;, &quot;noun&quot;), &quot;(a|the) ([^ ]+)&quot;, remove = FALSE ) ## # A tibble: 720 × 3 ## sentence article noun ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The birch canoe slid on the smooth planks. the smooth ## 2 Glue the sheet to the dark blue background. the sheet ## 3 It&#39;s easy to tell the depth of a well. the depth ## 4 These days a chicken leg is a rare dish. a chicken ## 5 Rice is often served in round bowls. &lt;NA&gt; &lt;NA&gt; ## 6 The juice of lemons makes fine punch. &lt;NA&gt; &lt;NA&gt; ## 7 The box was thrown beside the parked truck. the parked ## 8 The hogs were fed chopped corn and garbage. &lt;NA&gt; &lt;NA&gt; ## 9 Four hours of steady work faced us. &lt;NA&gt; &lt;NA&gt; ## 10 A large size in stockings is hard to sell. &lt;NA&gt; &lt;NA&gt; ## # ℹ 710 more rows 8.12 str_replace Функции str_replace() и str_replace_all() позволяют заменять совпадения на новые символы. x &lt;- c(&quot;apple&quot;, &quot;pear&quot;, &quot;banana&quot;) str_replace(x, &quot;[aeiou]&quot;, &quot;-&quot;) ## [1] &quot;-pple&quot; &quot;p-ar&quot; &quot;b-nana&quot; str_replace_all(x, &quot;[aeiou]&quot;, &quot;-&quot;) ## [1] &quot;-ppl-&quot; &quot;p--r&quot; &quot;b-n-n-&quot; Этим можно воспользоваться, если вы хотите, например, удалить из текста все греческие символы. Для стандартного греческого алфавита хватит [Α-Ωα-ω], но для древнегреческого этого, например, не хватит. Попробуем на отрывке из письма Цицерона Аттику, которое содержит греческий текст. cicero &lt;- &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀπεραντολογίας ἀηδοῦς! &quot; str_replace_all(cicero, &quot;[Α-Ωα-ω]&quot;, &quot;&quot;) ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀί ἀῦ! &quot; ὢ ἀί ἀῦ! Не все у нас получилось гладко. Попробуем иначе: str_replace_all(cicero, &quot;[\\u0370-\\u03FF]&quot;, &quot;&quot;) ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀ ἀῦ! &quot; Удалилась (буквально была заменена на пустое место) та диакритика, которая есть в новогреческом (ί). Но остались еще буквы со сложной диакритикой, которой современные греки не пользуются. no_greek &lt;- str_replace_all(cicero, &quot;[[\\u0370-\\u03FF][\\U1F00-\\U1FFF]]&quot;, &quot;&quot;) no_greek ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ! &quot; ! Мы молодцы. Избавились от этого непонятного греческого. На самом деле, конечно, str_replace хорош тем, что он позволяет производить осмысленные замены. Например, мы можем в оставшемся латинском текст заменить гласные с макроном (черточка, означающая долготу) на обычные гласные. str_replace_all(no_greek, c(&quot;ā&quot; = &quot;a&quot;, &quot;ū&quot; = &quot;u&quot;, &quot;ī&quot; = &quot;i&quot;, &quot;ō&quot; = &quot;o&quot;)) ## [1] &quot;nihil hac solitudine iucundius, nisi paulum interpellasset Amyntae filius. ! &quot; Красота. О более сложных заменах с перемещением групп можно посмотреть видео здесь и здесь. Это помогает даже в таком скорбном деле, как переоформление библиографии. 8.13 str_split Функция str_split() помогает разбить текст на предложения, слова или просто на бессмысленные наборы символов. Это важный этап подготовки текста для анализа, и проводится он нередко именно с применением регулярных выражений. sentences %&gt;% head(2) %&gt;% str_split(&quot; &quot;) ## [[1]] ## [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; ## [8] &quot;planks.&quot; ## ## [[2]] ## [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; ## [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; Но можно обойтись и без регулярных выражений. x &lt;- &quot;This is a sentence. This is another sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) ## Warning: `str_view()` was deprecated in stringr 1.5.0. ## ℹ Please use `str_view_all()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. ## [1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;. &lt;This&gt; &lt;is&gt; &lt;another&gt; &lt;sentence&gt;. str_view_all(x, boundary(&quot;sentence&quot;)) ## [1] │ &lt;This is a sentence. &gt;&lt;This is another sentence.&gt; Очень удобно, но убедитесь, что в вашем языке границы слов и предложения выглядят как у людей. С древнегреческим эта штука не справится (как делить на предложения греческие и латинские тексты, я рассказывала здесь): apology &lt;- c(&quot;νῦν δ&#39; ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ&#39; ὅς. “Τίς,” ἦν δ&#39; ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&quot;) str_view_all(apology, boundary(&quot;sentence&quot;)) ## [1] │ &lt;νῦν δ&#39; ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ&#39; ὅς. &gt;&lt;“Τίς,” ἦν δ&#39; ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&gt; Полный крах 💩 https://r4ds.had.co.nz/strings.html↩︎ https://stringr.tidyverse.org/reference/str_sub.html↩︎ https://r4ds.had.co.nz/strings.html#extract-matches↩︎ "],["веб-скрапинг.html", "Тема 9 Веб-скрапинг 9.1 Структура html 9.2 Каскадные таблицы стилей 9.3 Извлечение данных 9.4 Извлечение в тиббл 9.5 Скрапим телеграм-канал 9.6 Эмотиконы 9.7 Рутинная уборка 9.8 Лемматизация 9.9 Таблицы html 9.10 Wikisource", " Тема 9 Веб-скрапинг Выше мы говорили о таком импорте html, когда все теги разом удаляются. Это не всегда удобно, поскольку файл хранит данные в структурированном виде, например, под разными тегами дату, автора и текст. И может быть желательно эту структуру сохранить. В R это позволяет делать пакет rvest. С его помощью мы подготовим для дальнейшего построения тематической модели архив телеграм-канала Antibarbari HSE. Канал публичный, и Telegram дает возможность скачать архив в формате html при помощи кнопки export (однако эта функция может быть недоступна на MacOS). Эта глава опирается в основом на второе издание книги R for Data Science Хадли Викхема. 9.1 Структура html Документы html (HyperText Markup Language) имеют ирархическую структуру, состоящую из элементов. В каждом элементе есть открывающий тег (), опциональные атрибуты (id=‘first’) и закрывающий тег (). Все, что находится между открывающим и закрывающим тегом, называется содержанием элемента. Важнейшие теги, о которых стоит знать: html (есть всегда), с двумя детьми (дочерними элементами): head и body элементы, отвечающие за структуру: h1 (заголовок), section, p (параграф), ol (упорядоченный список) элементы, отвечающие за оформление: b (bold), i (italics), a (ссылка) Чтобы увидеть структуру страницы, надо нажать правой кнопкой мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере). 9.2 Каскадные таблицы стилей У тегов могут быть именованные атрибуты; важнейшие из них – это id и class, которые в сочетании с CSS контролируют внешний вид страницы. CSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML). У этого курса тоже есть свой css, в котором блок infobox (вы его видите как серый квадратик с определением) описано так: .infobox { padding: 1em; background: #ededed; color: black; } Проще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — селектор и блок объявлений. Селектор, расположенный в левой части правила до знака «{», определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;». Селекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так: p выберет все элементы .title выберет элементы с классом “title” #title выберет все элементы с атрибутом id=‘title’ Важно: если изменится структура страницы, откуда вы скрапили информацию, то и код, возможно, придется переписывать. 9.3 Извлечение данных Чтобы прочесть файл html, используем одноименную функцию. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding messages &lt;- read_html(&quot;./files/antibarbari_archive/messages.html&quot;) messages2 &lt;- read_html(&quot;./files/antibarbari_archive/messages2.html&quot;) messages ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;meta http-equiv=&quot;Content-Type&quot; content=&quot;text/html; charset=UTF-8 ... ## [2] &lt;body onload=&quot;CheckLocation();&quot;&gt;\\n\\n &lt;div class=&quot;page_wrap&quot;&gt;\\n\\n &lt;div ... На следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельных html; rvest позволяет это сделать (но внутри двойных кавычек должны быть только одинарные): html &lt;- minimal_html(&quot; &lt;div class=&#39;message default clearfix&#39; id=&#39;message83&#39;&gt; &lt;div class=&#39;pull_left userpic_wrap&#39;&gt; &lt;div class=&#39;userpic userpic2&#39; style=&#39;width: 42px; height: 42px&#39;&gt; &lt;div class=&#39;initials&#39; style=&#39;line-height: 42px&#39;&gt; A &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class=&#39;body&#39;&gt; &lt;div class=&#39;pull_right date details&#39; title=&#39;19.05.2022 11:18:07 UTC+03:00&#39;&gt; 11:18 &lt;/div&gt; &lt;div class=&#39;from_name&#39;&gt; Antibarbari HSE &lt;/div&gt; &lt;div class=&#39;text&#39;&gt; Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. &lt;br&gt;&lt;br&gt;«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).&lt;br&gt;&lt;a href=&#39;http://antibarbari.ru/2022/05/19/digest_1/&#39;&gt;http://antibarbari.ru/2022/05/19/digest_1/&lt;/a&gt; &lt;/div&gt; &lt;div class=&#39;signature details&#39;&gt; Olga Alieva &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &quot;) Из всего этого мне может быть интересно id сообщения ( ), текст сообщения ( ), а также, если указан, автор сообщения ( ). Извлекаем текст: html %&gt;% html_element(&quot;.text&quot;) %&gt;% html_text2() ## [1] &quot;Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора.\\n\\n«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).\\nhttp://antibarbari.ru/2022/05/19/digest_1/&quot; В классе signature details есть пробел, достаточно на его месте поставить точку: html %&gt;% html_element(&quot;.signature.details&quot;) %&gt;% html_text2() ## [1] &quot;Olga Alieva&quot; Важно помнить, что html_element всегда возвращает один элемент. Если их больше, надо использовать html_elements. Осталось добыть message id: html %&gt;% html_element(&quot;div&quot;) %&gt;% html_attr(&quot;id&quot;) ## [1] &quot;message83&quot; 9.4 Извлечение в тиббл library(tidyverse) tibble(id = html %&gt;% html_element(&quot;div&quot;) %&gt;% html_attr(&quot;id&quot;), signature = html %&gt;% html_element(&quot;.signature.details&quot;) %&gt;% html_text2(), text = html %&gt;% html_element(&quot;.text&quot;) %&gt;% html_text2() ) ## # A tibble: 1 × 3 ## id signature text ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 message83 Olga Alieva &quot;Этот пост открывает серию переложений из «Дайджеста пл… 9.5 Скрапим телеграм-канал До сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, нам надо извлечь только такие div, которым соответствует определенный класс: messages %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% head() ## {xml_nodeset (6)} ## [1] &lt;div class=&quot;message default clearfix&quot; id=&quot;message3&quot;&gt;\\n\\n &lt;div class= ... ## [2] &lt;div class=&quot;message default clearfix&quot; id=&quot;message5&quot;&gt;\\n\\n &lt;div class= ... ## [3] &lt;div class=&quot;message default clearfix&quot; id=&quot;message6&quot;&gt;\\n\\n &lt;div class= ... ## [4] &lt;div class=&quot;message default clearfix&quot; id=&quot;message7&quot;&gt;\\n\\n &lt;div class= ... ## [5] &lt;div class=&quot;message default clearfix&quot; id=&quot;message8&quot;&gt;\\n\\n &lt;div class= ... ## [6] &lt;div class=&quot;message default clearfix&quot; id=&quot;message9&quot;&gt;\\n\\n &lt;div class= ... Уже из этого списка можем доставать все остальное. messages_tbl1 &lt;- tibble(id = messages %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_attr(&quot;id&quot;), signature = messages %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_element(&quot;.signature.details&quot;) %&gt;% html_text2(), text = messages %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_element(&quot;.text&quot;) %&gt;% html_text2() ) messages_tbl2 &lt;- tibble(id = messages2 %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_attr(&quot;id&quot;), signature = messages2 %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_element(&quot;.signature.details&quot;) %&gt;% html_text2(), text = messages2 %&gt;% html_elements(&quot;div.message.default&quot;) %&gt;% html_element(&quot;.text&quot;) %&gt;% html_text2() ) Обратите внимание, что мы сначала извлекаем нужные элементы при помощи html_elements(), а потом применяем к каждому из них html_element(). Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция html_element(), если она не может найти, например, подпись, возвращает NA. Сшиваем воедино два тиббла. messages_tbl &lt;- messages_tbl1 %&gt;% bind_rows(messages_tbl2) messages_tbl ## # A tibble: 1,096 × 3 ## id signature text ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 message3 &lt;NA&gt; &quot;Latin never sleeps. Новое видео на канале Antibarbari (… ## 2 message5 &lt;NA&gt; &quot;Подборка видео семинара по медленному чтению \\&quot;О филосо… ## 3 message6 &lt;NA&gt; &quot;Новое видео в плейлисте \\&quot;Латинский язык\\&quot;. Фрагмент се… ## 4 message7 &lt;NA&gt; &quot;🤖 ОТКРЫТА ЗАПИСЬ НА ПРОЕКТ\\n\\nВ рамках проекта участни… ## 5 message8 &lt;NA&gt; &quot;Желающие присоединиться к группе, читающей \\&quot;Филеба\\&quot; П… ## 6 message9 &lt;NA&gt; &quot;https://youtu.be/I-U_lG0mB3M&quot; ## 7 message10 &lt;NA&gt; &quot;В клубе Antibarbari продолжается семинар по чтению и об… ## 8 message11 &lt;NA&gt; &quot;Филеб. Семинар 3 марта 2022. Сократ и Протарх решают сл… ## 9 message12 &lt;NA&gt; &quot;Вышка вернулась в аудитории, поэтому теперь у нас театр… ## 10 message13 &lt;NA&gt; &quot;Воспользовались дополнительным выходным, чтобы забэкапи… ## # ℹ 1,086 more rows Создатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. В некоторых постах только фото, для них в столбце text – NA, их можно сразу отсеять. messages_tbl &lt;- messages_tbl %&gt;% filter(!is.na(text)) set.seed(1234) 9.6 Эмотиконы В постах довольно много эмотиконов. Я их удалю, но сначала скажу о полезном пакете, который позволяет их все извлечь и, например, посчитать. library(emoji) messages_tbl %&gt;% mutate(emoji = emoji_extract_all(text)) %&gt;% pull(emoji) %&gt;% unlist() %&gt;% as_tibble() %&gt;% count(value) %&gt;% arrange(-n) ## # A tibble: 159 × 2 ## value n ## &lt;chr&gt; &lt;int&gt; ## 1 👾 68 ## 2 ⭐ 23 ## 3 👀 21 ## 4 👇 20 ## 5 🍿 18 ## 6 📚 16 ## 7 🔼 12 ## 8 1️⃣ 10 ## 9 📌 10 ## 10 🔗 10 ## # ℹ 149 more rows Заменяем их все на пробелы. messages_tbl &lt;- messages_tbl %&gt;% mutate(text = emoji_replace_all(text, &quot; &quot;)) 9.7 Рутинная уборка Подготовка текста для анализа включает в себя удаление сносок, иногда хэштегов, чисел, имейлов, возможно имен и т.п. В нашем случае ситуация осложняется тем, что тексты включают цитаты на латыни и древнегреческом, некоторые технические сокращения, номера страниц и др. Вот так, например, выглядит типичный пост: example &lt;- messages_tbl$text[340] example ## [1] &quot;🎞 Теэтет #10 149b7-150b8\\nСократ продолжает засыпать семнадцатилетнего математика подробностями из области акушерства и гинекологии, и мы вместе с ним терпеливо изучаем, чем сводничество отличается от сватовства. Верните квадратные корни. #платон #теэтет\\nhttps://vk.com/video-211800158_456239238&quot; Вот так вылавливается гиперссылка. str_extract_all(example, &quot;(http|https)(\\\\S+)&quot;) ## [[1]] ## [1] &quot;https://vk.com/video-211800158_456239238&quot; Вот так вылавливается пагинация и номер семинара (и некоторые другие числа). str_extract_all(example, &quot;#?\\\\d{2,3}\\\\w?\\\\d?-?&quot;) ## [[1]] ## [1] &quot;#10&quot; &quot;149b7-&quot; &quot;150b8&quot; &quot;21180&quot; &quot;0158&quot; &quot;45623&quot; &quot;9238&quot; Похожим образом можно выловить даты, имейлы и т.п. Все это удаляем из текста. messages_clean &lt;- messages_tbl %&gt;% mutate(text = str_replace_all(text, &quot;(http|https)(\\\\S+)&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;\\\\W[-A-Za-z0-9_.%]+\\\\@[-A-Za-z0-9_.%]+\\\\.[A-Za-z]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;#?\\\\d{2,3}\\\\w?\\\\d?-?&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;\\\\n+&quot;, &quot; &quot;)) Остались еще сокращения вроде “г.”, но токены из одной буквы можно будет удалить после разделения на слова. Знаки пунктуации можно оставить или убрать – иногда они бывают интересным стилистическим маркером. В любом случае лучше это делать после лемматизации, т.к. на тексте без знаков препинания анализатор покажет себя хуже. messages_clean %&gt;% filter(row_number() == 340) ## # A tibble: 1 × 3 ## id signature text ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 message465 Olga Alieva &quot;🎞 Теэтет Сократ продолжает засыпать семнадцатиле… Число id и число текстов не совпадает, поскольку для некоторых постов текста нет (NA), а у других он совпадает (“Пост выходного дня”). Это надо сразу исправить, чтобы результат лемматизации можно было потом соединить с данными о подписи. Я просто уберу очень короткие посты, поскольку для анализа они неинтересны. messages_clean &lt;- messages_clean %&gt;% filter(nchar(text) &gt; 19) dim(messages_clean) ## [1] 734 3 Переименуем первый столбец и переназначим id, чтобы можно было потом соединить с результатами лемматизации. messages_clean &lt;- messages_clean %&gt;% rename(doc_id = id) %&gt;% mutate(doc_id = paste0(&quot;doc&quot;, row_number())) 9.8 Лемматизация На лемматизацию мы отдаем вектор с сообщениями. library(udpipe) russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) messages_ann &lt;- udpipe_annotate(russian_syntagrus, messages_clean$text) messages_ann &lt;- as_tibble(messages_ann) messages_ann Убедимся, что после лемматизации число id не изменилось, и соединим этот тиббл с данными о подписи. length(unique(messages_ann$doc_id)) ## [1] 734 messages_signed &lt;- messages_ann %&gt;% left_join(messages_clean, by = &quot;doc_id&quot;) %&gt;% select(-text, -sentence_id, -paragraph_id, -xpos, -feats, -head_token_id, -dep_rel, -deps, -misc) length(unique(messages_signed$doc_id)) ## [1] 734 В постах упоминаются многие коллеги и студенты, чьи имена я бы хотела удалить, чтобы они не появлялись на графиках и т.п., но есть и много древних и новых имен, которые хотелось бы оставить. # valid_names &lt;- messages_ann %&gt;% # filter(upos == &quot;PROPN&quot;) %&gt;% # count(lemma) %&gt;% # arrange(-n) %&gt;% # filter(!str_detect(lemma, &quot;[\\\\.«]&quot;)) %&gt;% # filter(str_detect(lemma, &quot;[[\\u0400-\\u04FF]]&quot;)) %&gt;% # filter(n &gt; 1) # # valid_names_vec &lt;- as.character(valid_names$lemma) # список имен отредактирован вручную # write.table(valid_names_vec, file = &quot;files/names.txt&quot;, # row.names = F, col.names = F) valid_names &lt;- read_table(file = &quot;files/names.txt&quot;, col_names = F) valid_names &lt;- valid_names %&gt;% rename(names = X1) %&gt;% mutate(names = str_remove_all(names, &quot;\\\\W&quot;)) %&gt;% pull(names) messages_signed &lt;- messages_signed %&gt;% filter(upos != &quot;PROPN&quot; | upos == &quot;PROPN&quot; &amp; lemma %in% valid_names) %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% filter(!upos %in% c(&quot;X&quot;, &quot;NUM&quot;)) %&gt;% mutate(lemma = str_replace_all(lemma, &quot;-&quot;, &quot;&quot;)) %&gt;% mutate(lemma = str_remove_all(lemma, &quot;[[:punct:]]&quot;)) После некоторых сомнений, я удалю все, кроме кириллицы – на канале очень много латинского и греческого текста, на этапе создания термдокументной матрицы из-за этого будет почти 100% разреженность. messages_signed &lt;- messages_signed %&gt;% mutate(lemma = str_replace_all(lemma, &quot;[[^\\u0400-\\u04FF]]&quot;, &quot;&quot;)) %&gt;% filter(nchar(lemma) &gt; 0) Дальше были исправлены некоторые ошибки лемматизации. text_tidy &lt;- messages_signed %&gt;% mutate(lemma = tolower(lemma)) %&gt;% mutate_at(vars(lemma), ~ case_when(lemma == &quot;мят&quot; ~ &quot;мята&quot;, str_detect(lemma, &quot;сенек&quot;) ~ &quot;сенека&quot;, str_detect(lemma, &quot;аттик&quot;) ~ &quot;аттик&quot;, str_detect(lemma, &quot;кальвиз&quot;) ~ &quot;кальвизий&quot;, str_detect(lemma, &quot;горац&quot;) ~ &quot;гораций&quot;, str_detect(lemma, &quot;гален&quot;) ~ &quot;гален&quot;, str_detect(lemma, &quot;плотин&quot;) ~ &quot;плотин&quot;, str_detect(lemma, &quot;августин&quot;) ~ &quot;августин&quot;, str_detect(lemma, &quot;абари&quot;) ~ &quot;абарид&quot;, str_detect(lemma, &quot;катулл&quot;) ~ &quot;катулл&quot;, str_detect(lemma, &quot;лукул&quot;) ~ &quot;лукулл&quot;, str_detect(lemma, &quot;посидо&quot;) ~ &quot;посидоний&quot;, str_detect(lemma, &quot;деркилл&quot;) ~ &quot;деркиллид&quot;, str_detect(lemma, &quot;порфир&quot;) ~ &quot;порфирий&quot;, str_detect(lemma, &quot;афин&quot;) ~ &quot;афины&quot;, str_detect(lemma, &quot;локк&quot;) ~ &quot;локк&quot;, str_detect(lemma, &quot;макроб&quot;) ~ &quot;макробий&quot;, str_detect(lemma, &quot;лаэр&quot;) ~ &quot;лаэрций&quot;, str_detect(lemma, &quot;макрин&quot;) ~ &quot;макрина&quot;, str_detect(lemma, &quot;маркиш&quot;) ~ &quot;маркиш&quot;, str_detect(lemma, &quot;маячк&quot;) ~ &quot;маячок&quot;, str_detect(lemma, &quot;очерк&quot;) ~ &quot;очерк&quot;, str_detect(lemma, &quot;птолем&quot;) ~ &quot;птолемей&quot;, str_detect(lemma, &quot;росса&quot;) ~ &quot;росс&quot;, str_detect(lemma, &quot;самосат&quot;) ~ &quot;самосата&quot;, str_detect(lemma, &quot;стрепсиад&quot;) ~ &quot;стрепсиад&quot;, str_detect(lemma, &quot;эпихар&quot;) ~ &quot;эпихарм&quot;, str_detect(lemma, &quot;ямвл&quot;) ~ &quot;ямвлих&quot;, str_detect(lemma, &quot;брумал&quot;) ~ &quot;брумалии&quot;, str_detect(lemma, &quot;иоанн&quot;) ~ &quot;иоанн&quot;, str_detect(lemma, &quot;кинопоиск&quot;) ~ &quot;кинопоиск&quot;, str_detect(lemma, &quot;корнар&quot;) ~ &quot;корнарий&quot;, str_detect(lemma, &quot;луция&quot;) ~ &quot;луций&quot;, str_detect(lemma, &quot;кассия&quot;) ~ &quot;кассий&quot;, str_detect(lemma, &quot;минф&quot;) ~ &quot;минфа&quot;, str_detect(lemma, &quot;персефон&quot;) ~ &quot;персефона&quot;, str_detect(lemma, &quot;пестум&quot;) ~ &quot;пестум&quot;, str_detect(lemma, &quot;платон&quot;) ~ &quot;платон&quot;, str_detect(lemma, &quot;платно&quot;) ~ &quot;платон&quot;, str_detect(lemma, &quot;филеб&quot;) ~ &quot;филеб&quot;, str_detect(lemma, &quot;фульг&quot;) ~ &quot;фульгенций&quot;, str_detect(lemma, &quot;аврел&quot;) ~ &quot;аврелий&quot;, str_detect(lemma, &quot;антигон&quot;) ~ &quot;антигона&quot;, str_detect(lemma, &quot;анция&quot;) ~ &quot;анций&quot;, str_detect(lemma, &quot;борея&quot;) ~ &quot;борей&quot;, str_detect(lemma, &quot;вольтер&quot;) ~ &quot;вольтер&quot;, str_detect(lemma, &quot;гераклид&quot;) ~ &quot;гераклид&quot;, str_detect(lemma, &quot;теэтет&quot;) ~ &quot;теэтет&quot;, str_detect(lemma, &quot;евангел&quot;) ~ &quot;евангелие&quot;, str_detect(lemma, &quot;федон&quot;) ~ &quot;федон&quot;, TRUE ~ .)) Получившийся тиббл сохраняю – он понадобится в главе 13. save(text_tidy, file = &quot;data/AntibarbariTidy.Rdata&quot;) 9.9 Таблицы html Чуть более сложный пример: скрапинг html таблицы c фильмами IMDB. url &lt;- &quot;https://web.archive.org/web/20220201012049/https://www.imdb.com/chart/top/&quot; html &lt;- read_html(url) table &lt;- html %&gt;% html_element(&quot;table&quot;) %&gt;% html_table() table ## # A tibble: 250 × 5 ## `` `Rank &amp; Title` `IMDb Rating` `Your Rating` `` ## &lt;lgl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 NA &quot;1.\\n The Shawshank Redemption\\… 9.2 &quot;12345678910… NA ## 2 NA &quot;2.\\n The Godfather\\n (1… 9.1 &quot;12345678910… NA ## 3 NA &quot;3.\\n The Godfather: Part II\\n … 9 &quot;12345678910… NA ## 4 NA &quot;4.\\n The Dark Knight\\n … 9 &quot;12345678910… NA ## 5 NA &quot;5.\\n 12 Angry Men\\n (19… 8.9 &quot;12345678910… NA ## 6 NA &quot;6.\\n Schindler&#39;s List\\n … 8.9 &quot;12345678910… NA ## 7 NA &quot;7.\\n The Lord of the Rings: Th… 8.9 &quot;12345678910… NA ## 8 NA &quot;8.\\n Pulp Fiction\\n (19… 8.8 &quot;12345678910… NA ## 9 NA &quot;9.\\n The Good, the Bad and the… 8.8 &quot;12345678910… NA ## 10 NA &quot;10.\\n The Lord of the Rings: T… 8.8 &quot;12345678910… NA ## # ℹ 240 more rows Чтобы было удобнее работать с этими данными, их стоит переименовать и трансформировать. ratings &lt;- table |&gt; select( rank_title_year = `Rank &amp; Title`, rating = `IMDb Rating` ) |&gt; mutate( rank_title_year = str_replace_all(rank_title_year, &quot;\\n +&quot;, &quot; &quot;) ) |&gt; separate_wider_regex( rank_title_year, patterns = c( rank = &quot;\\\\d+&quot;, &quot;\\\\. &quot;, title = &quot;.+&quot;, &quot; +\\\\(&quot;, year = &quot;\\\\d+&quot;, &quot;\\\\)&quot; ) ) ratings ## # A tibble: 250 × 4 ## rank title year rating ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 The Shawshank Redemption 1994 9.2 ## 2 2 The Godfather 1972 9.1 ## 3 3 The Godfather: Part II 1974 9 ## 4 4 The Dark Knight 2008 9 ## 5 5 12 Angry Men 1957 8.9 ## 6 6 Schindler&#39;s List 1993 8.9 ## 7 7 The Lord of the Rings: The Return of the King 2003 8.9 ## 8 8 Pulp Fiction 1994 8.8 ## 9 9 The Good, the Bad and the Ugly 1966 8.8 ## 10 10 The Lord of the Rings: The Fellowship of the Ring 2001 8.8 ## # ℹ 240 more rows 9.10 Wikisource Многие тексты доступны на сайте Wikisource.org. Попробуем извлечь все сказки Салтыкова-Щедрина. url &lt;- &quot;https://ru.wikisource.org/wiki/%D0%9C%D0%B8%D1%85%D0%B0%D0%B8%D0%BB_%D0%95%D0%B2%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2%D0%B8%D1%87_%D0%A1%D0%B0%D0%BB%D1%82%D1%8B%D0%BA%D0%BE%D0%B2-%D0%A9%D0%B5%D0%B4%D1%80%D0%B8%D0%BD&quot; html = read_html(url) Для того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот тут можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни. toc &lt;- html %&gt;% html_elements(&quot;ul:nth-child(22) a&quot;) head(toc) ## {xml_nodeset (6)} ## [1] &lt;a href=&quot;/wiki/%D0%9F%D0%BE%D0%B2%D0%B5%D1%81%D1%82%D1%8C_%D0%BE_%D1%82%D ... ## [2] &lt;a href=&quot;/wiki/%D0%93%D0%BE%D0%B4%D0%BE%D0%B2%D1%89%D0%B8%D0%BD%D0%B0_(%D ... ## [3] &lt;a href=&quot;/wiki/%D0%9F%D1%80%D0%BE%D0%BF%D0%B0%D0%BB%D0%B0_%D1%81%D0%BE%D0 ... ## [4] &lt;a href=&quot;/wiki/%D0%94%D0%B8%D0%BA%D0%B8%D0%B9_%D0%BF%D0%BE%D0%BC%D0%B5%D1 ... ## [5] &lt;a href=&quot;/wiki/%D0%9F%D1%80%D0%B5%D0%BC%D1%83%D0%B4%D1%80%D1%8B%D0%B9_%D0 ... ## [6] &lt;a href=&quot;/wiki/%D0%A1%D0%B0%D0%BC%D0%BE%D0%BE%D1%82%D0%B2%D0%B5%D1%80%D0% ... Теперь у нас есть список ссылок. tales &lt;- tibble( title = toc %&gt;% html_attr(&quot;title&quot;), href = toc %&gt;% html_attr(&quot;href&quot;) ) Данных о годе публикации под тегом нет; надо подняться на уровень выше: toc2 &lt;- html %&gt;% html_elements(&quot;ul:nth-child(22) li&quot;) head(toc2) ## {xml_nodeset (6)} ## [1] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%9F%D0%BE%D0%B2%D0%B5%D1%81%D1%82%D1%8C_%D0%BE_%D ... ## [2] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%93%D0%BE%D0%B4%D0%BE%D0%B2%D1%89%D0%B8%D0%BD%D0% ... ## [3] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%9F%D1%80%D0%BE%D0%BF%D0%B0%D0%BB%D0%B0_%D1%81%D0 ... ## [4] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%94%D0%B8%D0%BA%D0%B8%D0%B9_%D0%BF%D0%BE%D0%BC%D0 ... ## [5] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%9F%D1%80%D0%B5%D0%BC%D1%83%D0%B4%D1%80%D1%8B%D0% ... ## [6] &lt;li&gt;\\n&lt;a href=&quot;/wiki/%D0%A1%D0%B0%D0%BC%D0%BE%D0%BE%D1%82%D0%B2%D0%B5%D1% ... toc2 %&gt;% html_text2() ## [1] &quot;Повесть о том, как один мужик двух генералов прокормил, 1869&quot; ## [2] &quot;Годовщина, 1869&quot; ## [3] &quot;Пропала совесть, 1869&quot; ## [4] &quot;Дикий помещик, 1869&quot; ## [5] &quot;Премудрый пискарь, 1883&quot; ## [6] &quot;Самоотверженный заяц, 1883&quot; ## [7] &quot;Бедный волк, 1883&quot; ## [8] &quot;Добродетели и Пороки, 1884&quot; ## [9] &quot;Медведь на воеводстве, 1884&quot; ## [10] &quot;Обманщик-газетчик и легковерный читатель, 1884&quot; ## [11] &quot;Вяленая вобла, 1884&quot; ## [12] &quot;Орёл-меценат, 1884&quot; ## [13] &quot;Карась-идеалист, 1884&quot; ## [14] &quot;Игрушечного дела людишки, 1879, 1886&quot; ## [15] &quot;Чижиково горе, 1884&quot; ## [16] &quot;Верный Трезор, 1885&quot; ## [17] &quot;Недреманное око, конец 1885 или начало 1886&quot; ## [18] &quot;Дурак, 1885&quot; ## [19] &quot;Соседи, 1885&quot; ## [20] &quot;Здравомысленный заяц, 1885&quot; ## [21] &quot;Либерал, 1885&quot; ## [22] &quot;Баран-непомнящий, 1885&quot; ## [23] &quot;Коняга, 1855&quot; ## [24] &quot;Кисель, 1855&quot; ## [25] &quot;Праздный разговор, 1886&quot; ## [26] &quot;Деревенский пожар, 1885&quot; ## [27] &quot;Путём-дорогою, 1886&quot; ## [28] &quot;Богатырь, 1886&quot; ## [29] &quot;Гиена, 1886&quot; ## [30] &quot;Приключение с Крамольниковым, 1886&quot; ## [31] &quot;Христова ночь, 1886&quot; ## [32] &quot;Ворон-челобитчик, 1886&quot; ## [33] &quot;Рождественская сказка, 1886&quot; Соединяем: tales &lt;- tibble( title_year = toc2 %&gt;% html_text2(), href = toc %&gt;% html_attr(&quot;href&quot;) ) tales ## # A tibble: 33 × 2 ## title_year href ## &lt;chr&gt; &lt;chr&gt; ## 1 Повесть о том, как один мужик двух генералов прокормил, 1869 /wiki/%D0%9F%D0… ## 2 Годовщина, 1869 /wiki/%D0%93%D0… ## 3 Пропала совесть, 1869 /wiki/%D0%9F%D1… ## 4 Дикий помещик, 1869 /wiki/%D0%94%D0… ## 5 Премудрый пискарь, 1883 /wiki/%D0%9F%D1… ## 6 Самоотверженный заяц, 1883 /wiki/%D0%A1%D0… ## 7 Бедный волк, 1883 /wiki/%D0%91%D0… ## 8 Добродетели и Пороки, 1884 /wiki/%D0%94%D0… ## 9 Медведь на воеводстве, 1884 /wiki/%D0%9C%D0… ## 10 Обманщик-газетчик и легковерный читатель, 1884 /wiki/%D0%9E%D0… ## # ℹ 23 more rows Дальше можно достать текст для каждой сказки. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила. url_test &lt;- tales %&gt;% filter(row_number() == 1) %&gt;% pull(href) %&gt;% paste0(&quot;https://ru.wikisource.org&quot;, .) text &lt;- read_html(url_test) %&gt;% html_elements(&quot;.indent p&quot;) %&gt;% html_text2() text[1] ## [1] &quot;Жили да были два генерала, и так как оба были легкомысленны, то в скором времени, по щучьему велению, по моему хотению, очутились на необитаемом острове.&quot; text[length(text)] ## [1] &quot;Однако, и об мужике не забыли; выслали ему рюмку водки да пятак серебра: веселись, мужичина!&quot; Первый и последний параграф достали верно; можно обобщать. tales &lt;- tales %&gt;% mutate(href = paste0(&quot;https://ru.wikisource.org&quot;, href)) urls &lt;- tales %&gt;% pull(href) Функция для извлечения текстов. get_text &lt;- function(url) { read_html(url) %&gt;% html_elements(&quot;.indent p&quot;) %&gt;% html_text2() %&gt;% paste(collapse= &quot; &quot;) } tales_text &lt;- map(urls, get_text) Несколько сказок не выловились: там другая структура html, но в целом все получилось. tales_text &lt;- tales_text %&gt;% flatten_chr() %&gt;% as_tibble() tales &lt;- tales %&gt;% bind_cols(tales_text) tales ## # A tibble: 33 × 3 ## title_year href value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Повесть о том, как один мужик двух генералов прокормил, 1869 https://r… &quot;Жил… ## 2 Годовщина, 1869 https://r… &quot;Сег… ## 3 Пропала совесть, 1869 https://r… &quot;Про… ## 4 Дикий помещик, 1869 https://r… &quot;В н… ## 5 Премудрый пискарь, 1883 https://r… &quot;Жил… ## 6 Самоотверженный заяц, 1883 https://r… &quot;Одн… ## 7 Бедный волк, 1883 https://r… &quot;Дру… ## 8 Добродетели и Пороки, 1884 https://r… &quot;Доб… ## 9 Медведь на воеводстве, 1884 https://r… &quot;&quot; ## 10 Обманщик-газетчик и легковерный читатель, 1884 https://r… &quot;Жил… ## # ℹ 23 more rows Дальше можно разделить столбец с названием и годом и, например, удалить ссылку, она больше не нужна. Разделить по запятой не получится, т.к. она есть в некоторых названиях. tales &lt;- tales %&gt;% select(-href) %&gt;% separate(title_year, into = c(&quot;title&quot;, &quot;year&quot;), sep = -5) %&gt;% mutate(title = str_remove(title, &quot;,$&quot;)) tales ## # A tibble: 33 × 3 ## title year value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Повесть о том, как один мужик двух генералов прокормил &quot; 1869&quot; &quot;Жили да были… ## 2 Годовщина &quot; 1869&quot; &quot;Сегодня мне … ## 3 Пропала совесть &quot; 1869&quot; &quot;Пропала сове… ## 4 Дикий помещик &quot; 1869&quot; &quot;В некотором … ## 5 Премудрый пискарь &quot; 1883&quot; &quot;Жил-был писк… ## 6 Самоотверженный заяц &quot; 1883&quot; &quot;Однажды заяц… ## 7 Бедный волк &quot; 1883&quot; &quot;Другой зверь… ## 8 Добродетели и Пороки &quot; 1884&quot; &quot;Добродетели … ## 9 Медведь на воеводстве &quot; 1884&quot; &quot;&quot; ## 10 Обманщик-газетчик и легковерный читатель &quot; 1884&quot; &quot;Жил-был газе… ## # ℹ 23 more rows Недостающие две сказки я не буду пытаться извлечь, но логику вы поняли. "],["токенизация-и-лемматизация.html", "Тема 10 Токенизация и лемматизация 10.1 Токенизация в tidytext 10.2 Токенизация в tokenizers 10.3 Скипграмы 10.4 Лемматизация и частеречная разметка 10.5 Морфологическая разметка 10.6 Распределение частей речи 10.7 Совместная встречаемость слов 10.8 Синтаксическая разметка", " Тема 10 Токенизация и лемматизация Токенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные энграмы (n-grams), то есть сочетания символов или слов, даже предложения или параграфы. Все зависит от того, какие единицы вам нужны для анализа. Визуально процесс токенизации можно представить так37: Токенизировать можно в базовом R, и Jockers (2014) прекрасно показывает, как это можно делать. Но вы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию tidyverse. Оба пакета придется загрузить отдельно. library(tidyverse) library(tidytext) library(tokenizers) Для их освоения рекомендую изучить две книги: Silge and Robinson (2017) и @ Обе доступны бесплатно онлайн. Обе содержат множество примеров для английских текстов. Для разнообразия я покажу, как это работает, на русских текстах (потому что латинские и древнегреческие никому не интересны). Для анализа я снова (ср. урок 6) загружу “Бедную Лизу” Карамзина, на этот раз полностью. liza &lt;- readLines(con = &quot;files/karamzin_liza.txt&quot;) class(liza) ## [1] &quot;character&quot; length(liza) ## [1] 46 nchar(liza) ## [1] 1045 505 1524 218 285 999 254 658 1149 629 121 284 170 252 701 ## [16] 1091 632 936 1726 96 698 167 985 316 1323 1844 763 1104 617 959 ## [31] 1191 305 1433 119 830 414 257 1218 977 225 513 1695 132 214 251 ## [46] 267 liza[1] ## [1] &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели -- куда глаза глядят -- по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москву, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. &quot; 10.1 Токенизация в tidytext Прежде чем передать текст пакету tidytext, его следует трансформировать в тиббл – этого требуют функции на входе. По умолчанию столбец будет называться value, и я его сразу переименую. liza_tbl &lt;- as_tibble(liza) %&gt;% rename(text = value) liza_tbl ## # A tibble: 46 × 1 ## text ## &lt;chr&gt; ## 1 &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей … ## 2 &quot; На другой стороне реки видна дубовая роща, подле которой пасутся мно… ## 3 &quot; Часто прихожу на сие место и почти всегда встречаю там весну; туда ж… ## 4 &quot; Но всего чаще привлекает меня к стенам Си...нова монастыря воспомина… ## 5 &quot; Саженях в семидесяти от монастырской стены, подле березовой рощицы, … ## 6 &quot; Отец Лизин был довольно зажиточный поселянин, потому что он любил ра… ## 7 &quot; \\&quot;Бог дал мне руки, чтобы работать, -- говорила Лиза, -- ты кормила … ## 8 &quot; Но часто нежная Лиза не могла удержать собственных слез своих -- ах!… ## 9 &quot; Прошло два года после смерти отца Лизина. Луга покрылись цветами, и … ## 10 &quot; Лиза, пришедши домой, рассказала матери, что с нею случилось. \\&quot;Ты х… ## # ℹ 36 more rows Этот текст мы передаем функции unnest_tokens(), которая принимает следующие аргументы: unnest_tokens( tbl, output, input, token = &quot;words&quot;, format = c(&quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, &quot;xml&quot;), to_lower = TRUE, drop = TRUE, collapse = NULL, ... ) Аргумент token принимает следующие значения: “words” (default), “characters”, “character_shingles”, “ngrams”, “skip_ngrams”, “sentences”, “lines”, “paragraphs”, “regex”, “ptb” (Penn Treebank). Используя уже знакомую функцию map, можно запустить unnest_tokens() с разными аргументами: params &lt;- tribble( ~tbl, ~output, ~input, ~token, liza_tbl[1,], &quot;word&quot;, &quot;text&quot;, &quot;words&quot;, liza_tbl[1,], &quot;sentence&quot;, &quot;text&quot;, &quot;sentences&quot;, liza_tbl[1,], &quot;char&quot;, &quot;text&quot;, &quot;characters&quot;, ) params %&gt;% pmap(unnest_tokens) %&gt;% head() ## [[1]] ## # A tibble: 159 × 1 ## word ## &lt;chr&gt; ## 1 может ## 2 быть ## 3 никто ## 4 из ## 5 живущих ## 6 в ## 7 москве ## 8 не ## 9 знает ## 10 так ## # ℹ 149 more rows ## ## [[2]] ## # A tibble: 5 × 1 ## sentence ## &lt;chr&gt; ## 1 может быть, никто из живущих в москве не знает так хорошо окрестностей города… ## 2 всякое лето нахожу новые приятные места или в старых новые красоты. ## 3 но всего приятнее для меня то место, на котором возвышаются мрачные, готическ… ## 4 стоя на сей горе, видишь на правой стороне почти всю москву, сию ужасную гром… ## 5 внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым … ## ## [[3]] ## # A tibble: 846 × 1 ## char ## &lt;chr&gt; ## 1 м ## 2 о ## 3 ж ## 4 е ## 5 т ## 6 б ## 7 ы ## 8 т ## 9 ь ## 10 н ## # ℹ 836 more rows Следующие значения аргумента token требуют также аргумента n: params &lt;- tribble( ~tbl, ~output, ~input, ~token, ~n, liza_tbl[1,], &quot;ngram&quot;, &quot;text&quot;, &quot;ngrams&quot;, 3, liza_tbl[1,], &quot;shingles&quot;, &quot;text&quot;, &quot;character_shingles&quot;, 3 ) params %&gt;% pmap(unnest_tokens) %&gt;% head() ## [[1]] ## # A tibble: 157 × 1 ## ngram ## &lt;chr&gt; ## 1 может быть никто ## 2 быть никто из ## 3 никто из живущих ## 4 из живущих в ## 5 живущих в москве ## 6 в москве не ## 7 москве не знает ## 8 не знает так ## 9 знает так хорошо ## 10 так хорошо окрестностей ## # ℹ 147 more rows ## ## [[2]] ## # A tibble: 844 × 1 ## shingles ## &lt;chr&gt; ## 1 мож ## 2 оже ## 3 жет ## 4 етб ## 5 тбы ## 6 быт ## 7 ыть ## 8 тьн ## 9 ьни ## 10 ник ## # ℹ 834 more rows 10.2 Токенизация в tokenizers При работе с данными в текстовом формате unnest_tokens() опирается на пакет tokenizers, но tokenize_words требует на входе вектор, а не тиббл. Несколько полезных аргументов, о которых стоит помнить: strip_non_alphanum (удаляет пробельные символы и пунктуацию), strip_punct (удаляет пунктуацию), strip_numeric (удаляет числа). words_no_punct &lt;- tokenize_words(liza[1], strip_punct = T) words_no_punct[[1]][25:40] ## [1] &quot;поле&quot; &quot;никто&quot; &quot;более&quot; &quot;моего&quot; &quot;не&quot; &quot;бродит&quot; &quot;пешком&quot; &quot;без&quot; ## [9] &quot;плана&quot; &quot;без&quot; &quot;цели&quot; &quot;куда&quot; &quot;глаза&quot; &quot;глядят&quot; &quot;по&quot; &quot;лугам&quot; words_punct &lt;- tokenize_words(liza[1], strip_punct = F) words_punct[[1]][25:40] ## [1] &quot;не&quot; &quot;бывает&quot; &quot;в&quot; &quot;поле&quot; &quot;,&quot; &quot;никто&quot; &quot;более&quot; &quot;моего&quot; ## [9] &quot;не&quot; &quot;бродит&quot; &quot;пешком&quot; &quot;,&quot; &quot;без&quot; &quot;плана&quot; &quot;,&quot; &quot;без&quot; 10.3 Скипграмы Скипграмы, или скользящие окна, применяются в некоторых языковых моделях. skipgrams &lt;- tokenize_skip_ngrams(liza[1], n=3) skipgrams[[1]][1:10] ## [1] &quot;может&quot; &quot;может быть&quot; &quot;может никто&quot; ## [4] &quot;может быть никто&quot; &quot;может быть из&quot; &quot;может никто из&quot; ## [7] &quot;может никто живущих&quot; &quot;быть&quot; &quot;быть никто&quot; ## [10] &quot;быть из&quot; Функция считает все скипграмы длиной до трех включительно; это можно поправить: skipgrams &lt;- tokenize_skip_ngrams(liza[1], n=3, n_min = 3) skipgrams[[1]][1:10] ## [1] &quot;может быть никто&quot; &quot;может быть из&quot; &quot;может никто из&quot; ## [4] &quot;может никто живущих&quot; &quot;быть никто из&quot; &quot;быть никто живущих&quot; ## [7] &quot;быть из живущих&quot; &quot;быть из в&quot; &quot;никто из живущих&quot; ## [10] &quot;никто из в&quot; Важно выбрать правильное значение n при использовании энграм. Использование униграм быстрее и эффективнее, но мы не получаем информации о порядке слов. Чем выше n, тем больше сохраняется информации, но при этом резко увеличивается векторное пространство, а встречаемость отдельных токенов уменьшается38. Уже на этапе токенизации можно удалить стоп-слова (используя аргумент stopwords), но это имеет смысл, если текст изначально лемматизирован (то есть слова даны в начальной форме): это возможно, если леммы хранились, например, в исходном xml. Наша “Лиза” не лемматизирована, поэтому удалять стоп-слова мы не будем. 10.4 Лемматизация и частеречная разметка Помимо деления на токены, предварительная обработка текста может включать в себя лемматизацию, то есть приведение слов к начальной форме (лемме) и синтаксическую разметку. Для аннотации мы воспользуемся морфологическим и синтаксическим анализатором UDPipe (Universal Dependencies Pipeline), который существует в виде одноименного пакета в R. В отличие от других анализаторов, доступных в R, он позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных данных. Прежде всего нужно выбрать и загрузить модель для (список). Модель GSD-Russian39, с которой мы начнем работу, обучена на статьях в Википедии, и, вероятно, не очень подойдет для наших задач – но можно попробовать. library(udpipe) # скачиваем модель в рабочую директорию # udpipe_download_model(language = &quot;russian-gsd&quot;) # загружаем модель russian_gsd &lt;- udpipe_load_model(file = &quot;russian-gsd-ud-2.5-191206.udpipe&quot;) Модели передается вектор с текстом. liza_ann &lt;- udpipe_annotate(russian_gsd, liza) Результат возвращается в формате CONLL-U; это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Формат разбора предложения в Conll-U выглядит так: Cтроки слов содержат следующие поля: 1. ID: индекс слова, целое число, начиная с 1 для каждого нового предложения; может быть диапазоном токенов с несколькими словами. 2. FORM: словоформа или знак препинания. 3. LEMMA: Лемма или основа словоформы. 4. UPOSTAG: универсальный тег части речи. 5. XPOSTAG: тег части речи для конкретного языка. 6. FEATS: список морфологических характеристик. 7. HEAD: заголовок текущего токена, который является либо значением ID, либо нулем (0). 8. DEPREL: Universal Stanford dependency relation к (root iff HEAD = 0) или определенному зависящему от языка подтипу. 9. DEPS: Список вторичных зависимостей. 10. MISC: любая другая аннотация. Для работы данные удобнее трансформировать в прямоугольный формат. liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-sentence, -paragraph_id) str(liza_df) ## tibble [6,447 × 12] (S3: tbl_df/tbl/data.frame) ## $ doc_id : chr [1:6447] &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ... ## $ sentence_id : int [1:6447] 1 1 1 1 1 1 1 1 1 1 ... ## $ token_id : chr [1:6447] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ token : chr [1:6447] &quot;Может&quot; &quot;быть&quot; &quot;,&quot; &quot;никто&quot; ... ## $ lemma : chr [1:6447] &quot;мочь&quot; &quot;быть&quot; &quot;,&quot; &quot;никто&quot; ... ## $ upos : chr [1:6447] &quot;VERB&quot; &quot;AUX&quot; &quot;PUNCT&quot; &quot;PRON&quot; ... ## $ xpos : chr [1:6447] &quot;VBC&quot; &quot;VB&quot; &quot;,&quot; &quot;DT&quot; ... ## $ feats : chr [1:6447] &quot;Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act&quot; &quot;Aspect=Imp|VerbForm=Inf&quot; NA &quot;Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing&quot; ... ## $ head_token_id: chr [1:6447] &quot;0&quot; &quot;4&quot; &quot;2&quot; &quot;10&quot; ... ## $ dep_rel : chr [1:6447] &quot;root&quot; &quot;cop&quot; &quot;punct&quot; &quot;nsubj&quot; ... ## $ deps : chr [1:6447] NA NA NA NA ... ## $ misc : chr [1:6447] &quot;SpacesBefore=\\\\s\\\\s\\\\s\\\\s&quot; &quot;SpaceAfter=No&quot; NA NA ... Выведем часть (!) столбцов для первого предложения: liza_df %&gt;% filter(doc_id == &quot;doc1&quot;) %&gt;% select(-sentence_id, -head_token_id, -deps, -dep_rel, -misc) %&gt;% DT::datatable() Если полистать эту таблицу, можно заметить несколько ошибок, например странное существительное “пешко” (наречие “пешком” понято как форма творительного падежа). Но, как уже говорилось, для некоторых языков, в том числе русского, в uppide представлено несколько моделей, некоторые из которых лучше справляются с текстами определенных жанров. Попробуем использовать другую модель, обученную на корпусе СинТагРус (сокр. от англ. Syntactically Tagged Russian text corpus, «синтаксически аннотированный корпус русских текстов»)40. # скачиваем модель в рабочую директорию # udpipe_download_model(language = &quot;russian-syntagrus&quot;) # загружаем модель russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) liza_ann &lt;- udpipe_annotate(russian_syntagrus, liza) liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-paragraph_id, -sentence, -xpos) liza_df %&gt;% filter(doc_id == &quot;doc1&quot;) %&gt;% select(-sentence_id, -head_token_id, -deps, -dep_rel, -misc) %&gt;% DT::datatable() Здесь “пешком” корректно обозначено как наречие; в целом, кажется, вторая модель лучше справилась с задачей. 10.5 Морфологическая разметка Морфологическая разметка, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, имена и названия: в первом параграфе, который мы проанализировали, их всего 4, причем правильно опознано в качестве собственного имени название Симонова монастыря. propn &lt;- liza_df %&gt;% filter(upos == &quot;PROPN&quot;) propn[,2:6] ## # A tibble: 164 × 5 ## sentence_id token_id token lemma upos ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 8 Москве Москва PROPN ## 2 3 16 Си...нова Си...нов PROPN ## 3 4 12 Москву Москва PROPN ## 4 5 45 Москву Москва PROPN ## 5 2 11 Данилов Данилов PROPN ## 6 2 23 Воробьевы Воробьев PROPN ## 7 3 21 Коломенское Коломенский PROPN ## 8 9 32 Москва Москва PROPN ## 9 1 14 Лизы Лиза PROPN ## 10 2 13 Лиза Лиза PROPN ## # ℹ 154 more rows С помощью функции str_detect() можно выбрать конкретные формы, например, винительный падеж. liza_df %&gt;% filter(str_detect(feats, &quot;Case=Acc&quot;)) ## # A tibble: 558 × 11 ## doc_id sentence_id token_id token lemma upos feats head_token_id dep_rel ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 doc1 1 28 поле поле NOUN Anim… 26 obl ## 2 doc1 2 4 новые новый ADJ Anim… 6 amod ## 3 doc1 2 5 приятные прият… ADJ Anim… 6 amod ## 4 doc1 2 6 места место NOUN Anim… 3 obj ## 5 doc1 2 10 новые новый ADJ Anim… 11 amod ## 6 doc1 2 11 красоты красо… NOUN Anim… 6 conj ## 7 doc1 4 3 сей сей DET Case… 4 det ## 8 doc1 4 4 горе горе NOUN Anim… 1 obl ## 9 doc1 4 11 всю весь DET Case… 12 det ## 10 doc1 4 12 Москву Москва PROPN Anim… 9 nmod ## # ℹ 548 more rows ## # ℹ 2 more variables: deps &lt;chr&gt;, misc &lt;chr&gt; 10.6 Распределение частей речи Литературоведам может быть интересно распределение различных частей речи в повести: так, Бен Блатт задался целью проверить, применительно к англоязычной прозе, знаменитый афоризм Стивена Кинга о том, что «дорога в ад вымощена наречиями». Правда ли, что великие писатели реже используют наречия на -ly? Он получил достаточно любопытные результаты, в частности выяснилось, что Генри Мелвилл и Джейн Остин представляют собой скорее исключение из этого правила, но с двумя важными оговорками: во-первых, в 19 в. наречия в целом используют чаще, чем 20-м; а во-вторых, в признанных шедеврах отдельных авторов наречий, действительно, бывает меньше. Например, в романе Стейнбека «Зима тревоги нашей» их меньше всего. Больше всего наречий у авторов фанфиков, непрофессиональных писателей. Посчитать части речи (расшифровка тегов UPOS по ссылке) можно так: liza_df %&gt;% group_by(upos) %&gt;% count() %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% arrange(-n) ## # A tibble: 14 × 2 ## # Groups: upos [14] ## upos n ## &lt;chr&gt; &lt;int&gt; ## 1 NOUN 1063 ## 2 VERB 988 ## 3 PRON 619 ## 4 ADP 520 ## 5 ADJ 408 ## 6 ADV 321 ## 7 DET 272 ## 8 CCONJ 230 ## 9 PART 187 ## 10 PROPN 164 ## 11 SCONJ 142 ## 12 AUX 71 ## 13 NUM 34 ## 14 INTJ 30 Столбиковая диаграмма позволяет наглядно представить такого рода данные: liza_df %&gt;% group_by(upos) %&gt;% count() %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) + geom_bar(stat = &quot;identity&quot;, show.legend = F) + coord_flip() + theme_bw() Обратите внимание на некоторое заметное число междометий. Какое междометие встречается здесь чаще всего, можно догадаться 😊 Можно отобрать наиболее частотные слова для любой части речи. nouns &lt;- liza_df %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% count(lemma) %&gt;% arrange(-n) head(nouns, 10) ## # A tibble: 10 × 2 ## lemma n ## &lt;chr&gt; &lt;int&gt; ## 1 Лиза 107 ## 2 Эраст 41 ## 3 сердце 24 ## 4 глаз 23 ## 5 мать 21 ## 6 человек 18 ## 7 день 17 ## 8 рука 17 ## 9 друг 16 ## 10 слеза 15 library(wordcloud) ## Loading required package: RColorBrewer library(RColorBrewer) pal &lt;- RColorBrewer::brewer.pal(7, &quot;Dark2&quot;) nouns %&gt;% with(wordcloud(lemma, n, max.words = 50, colors = pal)) Можно заметить, что в тексте часто встречаются слова “мать”, “матушка”, “старушка” (42 раза): Лизина мать упоминается в тексте так же часто, как Эраст, и чаще, чем слово “сердце” (24). В любовной повести Карамзин чуть ли не чаще говорит о матери героини, чем о её возлюбленном! 10.7 Совместная встречаемость слов Функция cooccurence() из пакета udpipe позволяет выяснить, сколько раз некий термин встречается совместно с другим термином, например: слова встречаются в одном и том же документе/предложении/параграфе; слова следуют за другим словом; слова находятся по соседству с другим словом на расстоянии n слов. Код ниже позволяет выяснить, какие слова встречаются в одном предложении: x &lt;- subset(liza_df, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;)) cooc &lt;- cooccurrence(x, term = &quot;lemma&quot;, group = c(&quot;doc_id&quot;, &quot;sentence_id&quot;)) head(cooc) ## term1 term2 cooc ## 1 молодой человек 8 ## 2 глаз друг 6 ## 3 последний раз 6 ## 4 глаз слеза 6 ## 5 друг час 6 ## 6 день другой 4 Этот результат легко визуализировать, используя пакет ggraph: library(igraph) library(ggraph) wordnetwork &lt;- head(cooc, 30) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;pink&quot;) + geom_node_text(aes(label = name), col = &quot;darkblue&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Совместная встречаемость слов&quot;, subtitle = &quot;Существительные и прилагательные&quot;) Милый друг, глубокий пруд. Грустная история! Чтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но с другими аргументами41: cooc &lt;- cooccurrence(x$lemma, relevant = x$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;), skipgram = 1) head(cooc) ## term1 term2 cooc ## 1 молодой человек 8 ## 2 берег река 4 ## 3 друг друг 4 ## 4 последний раз 4 ## 5 Лиза мать 4 ## 6 глаз слеза 3 wordnetwork &lt;- head(cooc, 30) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_colour = &quot;salmon&quot;, edge_alpha=0.7), show.legend = F) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4, angle=15, repel = T) + theme_graph(base_family = &quot;Arial Narrow&quot;) + labs(title = &quot;Слова, стоящие рядом в тексте&quot;, subtitle = &quot;Существительные и прилагательные&quot;) 10.8 Синтаксическая разметка Для анализа выберем одно предложение. liza_synt &lt;- liza_ann %&gt;% as.data.frame() liza_synt_sel &lt;- liza_synt %&gt;% filter(doc_id == &quot;doc17&quot;, sentence_id == 15) %&gt;% filter(token != &quot;-&quot;) liza_synt_sel[,c(&quot;token&quot;, &quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] ## token token_id head_token_id dep_rel ## 1 Лиза 3 5 nsubj ## 2 не 4 5 advmod ## 3 договорила 5 0 root ## 4 речи 6 5 obl ## 5 своей 7 6 det ## 6 . 8 5 punct Связь между токенами определяется в полях token_id и head_token_id, отношение зависимости определено в dep_rel. Корневой токен имеет значение 0, то есть ни от чего не зависит. Графически изобразить связи поможет пакет textplot. library(textplot) textplot_dependencyparser(liza_synt_sel) Построить граф можно и при помощи библиотек igraph и ggraph: liza_synt_sel &lt;- liza_synt %&gt;% filter(doc_id == &quot;doc17&quot;, sentence_id == 1) e &lt;- subset(liza_synt_sel, head_token_id != 0, select = c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)) e ## token_id head_token_id dep_rel ## 2 2 1 obl ## 3 3 7 punct ## 4 4 7 cc ## 5 5 6 amod ## 6 6 7 nsubj ## 7 7 1 conj ## 8 8 9 advmod ## 9 9 7 xcomp ## 10 10 1 punct e$label &lt;- e$dep_rel gr &lt;- graph_from_data_frame(e, vertices = liza_synt_sel[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.1, &quot;inches&quot;)) ggraph(gr, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha=0.7, label = dep_rel), arrow = a, end_cap = circle(0.07, &#39;inches&#39;), show.legend = F, label_colour = &quot;grey30&quot;, edge_color = &quot;grey&quot;) + geom_node_point(color = &quot;lightblue&quot;, size = 4) + theme_void(base_family = &quot;&quot;) + geom_node_text(ggplot2::aes(label = token), nudge_y = 0.2) Литература "],["распределения-слов-и-анализ-частотностей.html", "Тема 11 Распределения слов и анализ частотностей 11.1 Извлечение источников 11.2 Подготовка источников 11.3 Токенизация 11.4 Cтоп-слова 11.5 Абсолютная частотность 11.6 Стемминг 11.7 Относительная частотность 11.8 Распределения слов (токенов) 11.9 Закон Ципфа 11.10 TTR (type-token ratio) 11.11 TF-IDF 11.12 Сравнение при помощи диаграммы рассеяния", " Тема 11 Распределения слов и анализ частотностей В этом уроке мы научимся считать наиболее частотные и наиболее характерные слова, удалять стоп-слова, познакомимся с алгоритмом стемминга, а также узнаем, как считать type-token ratio (и почему этого делать не стоит). За основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты: “Опыт о человеческом разумении” Джона Локка (1690), первые две книги; “Трактат о принципах человеческого знания” Джорджа Беркли (1710); “Исследование о человеческом разумении” Дэвида Юма (1748)42. 11.1 Извлечение источников Источники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить gutenberg_id. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли. # install.packages(&quot;gutenbergr&quot;) library(gutenbergr) library(tidyverse) library(stringr) gutenberg_works(str_detect(author, &quot;Hume&quot;), languages = &quot;en&quot;) ## # A tibble: 99 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2868 &quot;The Gr… Hume,… 1057 en &lt;NA&gt; ## 2 4223 &quot;The My… Hume,… 1057 en &lt;NA&gt; ## 3 4320 &quot;An Enq… Hume,… 1440 en Philosophy ## 4 4531 &quot;The Se… Hume,… 1057 en &lt;NA&gt; ## 5 4583 &quot;Dialog… Hume,… 1440 en Philosophy/Paganism ## 6 4705 &quot;A Trea… Hume,… 1440 en Philosophy ## 7 4946 &quot;Madame… Hume,… 1057 en &lt;NA&gt; ## 8 9662 &quot;An Enq… Hume,… 1440 en Harvard Classics/P… ## 9 10574 &quot;The Hi… Hume,… 1440 en United Kingdom ## 10 13117 &quot;The Ne… Hume,… 4843 en Natural History/An… ## # ℹ 89 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Когда id найдены, gutenbergr позволяет загрузить сочинения; на этом этапе часто возникают ошибки – в таком случае надо воспользоваться одним из зеркал. Список зеркал, как уже говорилось в уроке про импорт данных, доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL. my_corpus &lt;- gutenberg_download(meta_fields = c(&quot;author&quot;, &quot;title&quot;), c(9662, 4723, 10615), mirror = &quot;https://www.gutenberg.org/dirs/&quot;) my_corpus ## # A tibble: 23,844 × 4 ## gutenberg_id text author title ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 4723 &quot;A Treatise Concerning the Principles of Human Kno… Berke… A Tr… ## 2 4723 &quot;&quot; Berke… A Tr… ## 3 4723 &quot;&quot; Berke… A Tr… ## 4 4723 &quot;by&quot; Berke… A Tr… ## 5 4723 &quot;&quot; Berke… A Tr… ## 6 4723 &quot;George Berkeley (1685-1753)&quot; Berke… A Tr… ## 7 4723 &quot;&quot; Berke… A Tr… ## 8 4723 &quot;&quot; Berke… A Tr… ## 9 4723 &quot;WHEREIN THE CHIEF CAUSES OF ERROR AND DIFFICULTY … Berke… A Tr… ## 10 4723 &quot;WITH THE GROUNDS OF SCEPTICISM, ATHEISM, AND IRRE… Berke… A Tr… ## # ℹ 23,834 more rows В этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния и имена можно двумя способами: при помощи функции unique() из базового R или distinct() из tidyverse. unique(my_corpus$title) ## [1] &quot;A Treatise Concerning the Principles of Human Knowledge&quot; ## [2] &quot;An Enquiry Concerning Human Understanding&quot; ## [3] &quot;An Essay Concerning Humane Understanding, Volume 1\\r\\nMDCXC, Based on the 2nd Edition, Books 1 and 2&quot; my_corpus %&gt;% distinct(author) ## # A tibble: 3 × 1 ## author ## &lt;chr&gt; ## 1 Berkeley, George ## 2 Hume, David ## 3 Locke, John 11.2 Подготовка источников Прежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные. my_corpus &lt;- my_corpus %&gt;% select(-gutenberg_id) %&gt;% select(-title) %&gt;% relocate(text, .after = author) %&gt;% mutate(author = str_remove(author, &quot;,.+$&quot;)) %&gt;% filter(text != &quot;&quot;) head(my_corpus, 3) ## # A tibble: 3 × 2 ## author text ## &lt;chr&gt; &lt;chr&gt; ## 1 Berkeley A Treatise Concerning the Principles of Human Knowledge ## 2 Berkeley by ## 3 Berkeley George Berkeley (1685-1753) В случае с Юмом отрезаем предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем. Hume &lt;- my_corpus %&gt;% filter(author == &quot;Hume&quot;) %&gt;% filter(!row_number() %in% c(1:25), !row_number() %in% c(4814:nrow(my_corpus))) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;_&quot;, &quot; &quot;)) %&gt;% filter(!str_detect(text, &quot;SECTION .{1,4}&quot;)) sample_n(Hume, 3) ## # A tibble: 3 × 2 ## author text ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume it is attended with an unavoidable contrariety in our judgements, and ## 2 Hume depart from the primary instincts of nature, and to embrace a new syst… ## 3 Hume superlative intelligence and benevolence are entirely imaginary, or, at В случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: [Note: Vide Hobbes’ Tripos, ch. v. sect. 6.]43, от них тоже следует избавиться. Berkeley &lt;- my_corpus %&gt;% filter(author == &quot;Berkeley&quot;) %&gt;% filter(!row_number() %in% c(1:38)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]+?\\\\.&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;\\\\[.+?\\\\]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]+&quot;, &quot; &quot;)) Что касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение; удаляем подчеркивания вокруг слов. “Письмо к читателю” уже содержит некоторые философские положения, и его можно оставить. Locke &lt;- my_corpus %&gt;% filter(author == &quot;Locke&quot;) %&gt;% filter(!row_number() %in% c(1:135)) %&gt;% mutate(text = str_replace_all(text, &quot;_&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]&quot;, &quot; &quot;)) Соединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем. tidy_corpus &lt;- bind_rows(Hume, Berkeley, Locke) %&gt;% mutate(text = str_replace_all(text, c(&quot;[Mm]an’s&quot; = &quot;man&#39;s&quot;, &quot;[mM]en’s&quot; = &quot;men&#39;s&quot;, &quot;[hH]ath&quot; = &quot;has&quot;))) 11.3 Токенизация После этого делим корпус на слова, как мы уже делали в уроке про токенизацию. library(tidytext) corpus_words &lt;- tidy_corpus %&gt;% unnest_tokens(word, text) corpus_words ## # A tibble: 238,538 × 2 ## author word ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume moral ## 2 Hume philosophy ## 3 Hume or ## 4 Hume the ## 5 Hume science ## 6 Hume of ## 7 Hume human ## 8 Hume nature ## 9 Hume may ## 10 Hume be ## # ℹ 238,528 more rows 11.4 Cтоп-слова Большая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна – это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция anti_join() позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно. Для многих языков стоп-слова доступны в пакете stopwords44. Функция anti_join() работает так: other &lt;- c(&quot;section&quot;, &quot;chapter&quot;, 0:40, &quot;edit&quot;, 1710, &quot;v.g&quot;, &quot;v.g.a&quot;) corpus_words_nosp &lt;- corpus_words %&gt;% anti_join(stop_words) %&gt;% filter(!word %in% other) ## Joining with `by = join_by(word)` corpus_words_nosp ## # A tibble: 73,311 × 2 ## author word ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume moral ## 2 Hume philosophy ## 3 Hume science ## 4 Hume human ## 5 Hume nature ## 6 Hume treated ## 7 Hume manners ## 8 Hume peculiar ## 9 Hume merit ## 10 Hume contribute ## # ℹ 73,301 more rows Уборка закончена, мы готовы к подсчетам. 11.5 Абсолютная частотность Для начала посмотрим на самые частотные слова во всем корпусе. library(ggplot2) corpus_words_nosp %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder(word, n), n, fill = word)) + geom_col(show.legend = F) + coord_flip() Этот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие idea. Однако можно заподозрить, что высокие показатели для слов simple, distinct и powers – это заслуга прежде всего Локка, который вводит понятия “простой идеи” и “отчетливой идеи”, а также говорит о “силах” вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка – это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (innate) также занимает в первую очередь его: вся первая книга “Опыта” – это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм – очень кратко. Кроме того, хотя мы взяли только две книги из “Опыта” Локка – это самый длинный в нашем корпусе, что создает значительный перекос: corpus_words_nosp %&gt;% group_by(author) %&gt;% summarise(sum = n()) ## # A tibble: 3 × 2 ## author sum ## &lt;chr&gt; &lt;int&gt; ## 1 Berkeley 11455 ## 2 Hume 18182 ## 3 Locke 43674 Посмотрим статистику по отдельным авторам. corpus_words_nosp %&gt;% group_by(author) %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder_within(word, n, author), n, fill = word)) + geom_col(show.legend = F) + facet_wrap(~author, scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() Наиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов. Согласно Локку, объектом мышления является идея (желательно отчетливая, но тут уж как получится). Все идеи приобретены умом из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких врожденных идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть простыми и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся пространство, в котором находятся тела, а также продолжительность; измеренная продолжительность представляет собой время. Беркли спорит с мнением, согласно котором ум способен образовывать абстрактные идеи. В том числе, утверждает он, невозможна абстрактная идея движения, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому слова являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет умом и духом) существуют не абстрактные идеи, а ощущения, и существование немыслящих вещей безотносительно к их воспринимаемости совершенно невозможно. Нет иной субстанции, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть материя. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами природы. Действительные вещи – это комбинации ощущений, запечатлеваемые в нас могущественным духом. Согласно Юму, все объекты, доступные человеческому разуму, могут быть разделены на два вида, а именно: на отношения между идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все заключения о фактах основаны на отношениях причины и действия. В свою очередь знание о причинности возникает всецело из опыта: только привычка заставляет нас ожидать наступления одного события при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в философии. 11.6 Стемминг Поскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим – стемминг. Стемминг (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году. В R стеммер Портера доступен в пакете snowball. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть45. Не для всех языков, впрочем, и не для всех задач стемминг – это хорошая идея. Но попробуем применить его к нашему корпусу. library(SnowballC) corpus_stems &lt;- corpus_words_nosp %&gt;% mutate(stem = wordStem(word)) corpus_stems %&gt;% count(stem, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder(stem, n), n, fill = stem)) + geom_col(show.legend = F) + coord_flip() Все слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно ниже: # до стемминга corpus_words_nosp %&gt;% distinct(word) %&gt;% nrow() ## [1] 8132 # после стемминга corpus_stems %&gt;% distinct(stem) %&gt;% nrow() ## [1] 5229 Стемминг применяется в некоторых алгоритмах машинного обучения. 11.7 Относительная частотность Абсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте. Cначала считаем частотность для всех токенов по авторам. author_word_counts &lt;- corpus_words %&gt;% count(author, word, sort = T) %&gt;% ungroup() author_word_counts ## # A tibble: 14,111 × 3 ## author word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Locke the 7431 ## 2 Locke of 7044 ## 3 Locke and 4817 ## 4 Locke to 4603 ## 5 Hume the 3182 ## 6 Locke in 3056 ## 7 Locke that 2954 ## 8 Locke it 2742 ## 9 Locke is 2462 ## 10 Hume of 2461 ## # ℹ 14,101 more rows Затем – число токенов в каждой книге. total_counts &lt;- author_word_counts %&gt;% group_by(author) %&gt;% summarise(total = sum(n)) total_counts ## # A tibble: 3 × 2 ## author total ## &lt;chr&gt; &lt;int&gt; ## 1 Berkeley 36777 ## 2 Hume 53590 ## 3 Locke 148171 Соединяем два тиббла: author_word_counts &lt;- author_word_counts %&gt;% left_join(total_counts) head(author_word_counts) ## # A tibble: 6 × 4 ## author word n total ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Locke the 7431 148171 ## 2 Locke of 7044 148171 ## 3 Locke and 4817 148171 ## 4 Locke to 4603 148171 ## 5 Hume the 3182 53590 ## 6 Locke in 3056 148171 Считаем относительную частотность и умножаем на 100, чтобы получить проценты: author_word_rf &lt;- author_word_counts %&gt;% mutate(rf = round((n / total), 5) * 100) author_word_rf %&gt;% filter(author == &quot;Berkeley&quot;) %&gt;% head() ## # A tibble: 6 × 5 ## author word n total rf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Berkeley the 1915 36777 5.21 ## 2 Berkeley of 1550 36777 4.22 ## 3 Berkeley and 1178 36777 3.20 ## 4 Berkeley to 1057 36777 2.87 ## 5 Berkeley that 816 36777 2.22 ## 6 Berkeley is 772 36777 2.10 11.8 Распределения слов (токенов) Наиболее частотные слова – это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень-очень редко, а слов с высокой частотностью – мало. Тоненький хвост уходит далеко вправо по оси x, для наглядности зададим произвольный предел. author_word_rf %&gt;% ggplot(aes(rf, fill = author)) + geom_histogram(show.legend = FALSE) + facet_wrap(~author, scales = &quot;free&quot;) 11.9 Закон Ципфа Подобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) из Гарвардского университета и утверждает следующее: Если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота n-го слова в списке окажется обратно пропорциональной его порядковому номеру n. Это можно записать так: \\[tf_{r_i} = \\frac{c}{r^α_i}\\] или \\[ tf_{r_i} \\cdot r^α_i = c \\] Извлекая логарифм из обеих частей, получаем: \\[log(tf_{r_i}) = log(c) - α \\cdot log(r_i) \\] На всякий случай: логарифм дроби равен разности логарифмов числителя и знаменателя. Таким образом, мы получаем (почти) линейную зависимость, где c – точка пересечения оси y, a α - коэффициент наклона прямой. Графически это выглядит вот так: author_word_rf_rank &lt;- author_word_rf %&gt;% group_by(author) %&gt;% mutate(rank = row_number()) author_word_rf_rank %&gt;% ggplot(aes(rank, rf, color = author)) + geom_line(size = 1.1, alpha = 0.7) + scale_x_log10() + scale_y_log10() Чтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом подробнее в следующих уроках): lm_zipf &lt;- lm(data = author_word_rf_rank, formula = log10(rf) ~ log10(rank)) coefficients(lm_zipf) ## (Intercept) log10(rank) ## 1.749726 -1.272644 Мы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график: author_word_rf_rank %&gt;% ggplot(aes(rank, rf, color = author)) + geom_line(size = 1.1, alpha = 0.7) + geom_abline(intercept = 1.744, slope = -1.26, linetype = 2, color = &quot;grey50&quot;) + scale_x_log10() + scale_y_log10() Здесь видно, что отклонения наиболее заметны как в области самых частотных слов, так и в области наиболее редких (с высокими рангами). 11.10 TTR (type-token ratio) На практике это означает, что редкие слова (события) случаются очень часто; это явление известно под названием Large Number of Rare Events (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается (чем дальше, тем сложнее встретить слово, которого еще не было). Это значит, что сравнивать тексты с точки зрения лексического разнообразия – дело достаточно рискованное, хотя интуитивно кажется, что некоторые авторы пишут более разнообразно, а другие - менее. Наиболее известная и наиболее проблемная мера лексического разнообразия – это type-token ratio (TTR). \\[ TTR(T) = \\frac{Voc(T)}{n} \\] где n - общее число токенов, а Voc - число уникальных токенов (типов). В пакете languageR, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов. locke_words &lt;- corpus_words %&gt;% filter(author == &quot;Locke&quot;) %&gt;% pull(word) head(locke_words) ## [1] &quot;i&quot; &quot;have&quot; &quot;put&quot; &quot;into&quot; &quot;thy&quot; &quot;hands&quot; length(locke_words) ## [1] 148171 Это немного задумчивая функция, поэтому я создам не 148 тысяч отрывков, а всего 40. library(languageR) locke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40) ## ........................................ head(locke.growth@data$data) ## Chunk Tokens Types HapaxLegomena DisLegomena TrisLegomena Yule Zipf ## 1 1 1000 409 273 64 18 83.8600 -0.6618709 ## 2 2 2000 638 407 100 34 97.8500 -0.7749886 ## 3 3 3000 854 544 121 59 102.3711 -0.8420029 ## 4 4 4000 1013 608 154 72 103.5000 -0.8514063 ## 5 5 5000 1133 663 171 76 103.8696 -0.8742347 ## 6 6 6000 1256 713 193 85 105.2767 -0.8822200 ## TypeTokenRatio Herdan Guiraud Sichel Lognormal ## 1 0.4090000 0.7683545 12.93372 0.1564792 0.4346750 ## 2 0.3190000 0.7405672 14.26611 0.1567398 0.5013140 ## 3 0.2846667 0.7282750 15.59184 0.1416862 0.5191183 ## 4 0.2532500 0.7124681 16.01694 0.1520237 0.5779033 ## 5 0.2266000 0.6992475 16.02304 0.1509267 0.6194817 ## 6 0.2093333 0.6834975 16.21489 0.1536624 0.6548148 plot(locke.growth) Тут много всего интересного, но обратим внимание лишь на два верхних окошка слева: количество типов растет постоянно, но с разной скоростью. Так же меняется числов гапаксов: нечто похожее мы уже замечали, когда говорили о гапаксах у Платона. Подробнее о различных мерах лексического разнообразия см.: (Baayen 2008, 222–36) и (Savoy 2020). 11.11 TF-IDF Мы уже заметили, говоря об абсолютной частотности, что для трех авторов в нашем условном корпусе многие слова общие. Для многих алгоритмов машинного обучения используется другая мера, tf-idf (term frequency - inverse document frequency). Логарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для некоторого документа. Однако относительная частотность тоже учитывается! Например, Беркли один раз упоминает “сахарные бобы”, а Локк – “миндаль”, но из-за редкой частотности tf-idf для подобных слов будет низкий. Функция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова. author_word_tfidf &lt;- author_word_rf %&gt;% filter(!word %in% other) %&gt;% bind_tf_idf(word, author, n) author_word_tfidf ## # A tibble: 14,103 × 8 ## author word n total rf tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Locke the 7431 148171 5.01 0.0502 0 0 ## 2 Locke of 7044 148171 4.75 0.0476 0 0 ## 3 Locke and 4817 148171 3.25 0.0325 0 0 ## 4 Locke to 4603 148171 3.11 0.0311 0 0 ## 5 Hume the 3182 53590 5.94 0.0594 0 0 ## 6 Locke in 3056 148171 2.06 0.0206 0 0 ## 7 Locke that 2954 148171 1.99 0.0199 0 0 ## 8 Locke it 2742 148171 1.85 0.0185 0 0 ## 9 Locke is 2462 148171 1.66 0.0166 0 0 ## 10 Hume of 2461 53590 4.59 0.0459 0 0 ## # ℹ 14,093 more rows Выбираем слова с высокой tf-idf: author_word_tfidf %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 14,103 × 7 ## author word n rf tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hume event 71 0.132 0.00133 1.10 0.00146 ## 2 Locke innate 297 0.2 0.00201 0.405 0.000813 ## 3 Hume sentiment 35 0.065 0.000653 1.10 0.000718 ## 4 Hume reasoning 91 0.17 0.00170 0.405 0.000689 ## 5 Locke duration 249 0.168 0.00168 0.405 0.000682 ## 6 Hume fact 80 0.149 0.00149 0.405 0.000605 ## 7 Hume enquiry 29 0.054 0.000541 1.10 0.000595 ## 8 Hume conjoined 26 0.049 0.000485 1.10 0.000533 ## 9 Hume energy 26 0.049 0.000485 1.10 0.000533 ## 10 Hume witnesses 22 0.041 0.000411 1.10 0.000451 ## # ℹ 14,093 more rows Снова визуализируем. author_word_tfidf %&gt;% arrange(-tf_idf) %&gt;% group_by(author) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) + geom_col(show.legend = F) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~author, scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() ## Selecting by tf_idf На таком графике авторы выглядят гораздо более самобытными, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную. 11.12 Сравнение при помощи диаграммы рассеяния Столбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Функция spread() позволяет разделить один столбец на несколько новых, а gather(), напротив, – собрать. freq &lt;- author_word_rf %&gt;% anti_join(stop_words) %&gt;% mutate(rf = rf / 100) %&gt;% filter(rf &gt; 0.0001) %&gt;% select(-n, -total) %&gt;% spread(author, rf, fill = 0) %&gt;% gather(author, rf, Hume:Locke) ## Joining with `by = join_by(word)` freq ## # A tibble: 2,458 × 4 ## word Berkeley author rf ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 absence 0 Hume 0 ## 2 absent 0 Hume 0 ## 3 absolute 0.00071 Hume 0.00021 ## 4 absolutely 0.00027 Hume 0.00015 ## 5 abstract 0.0025 Hume 0.00035 ## 6 abstracted 0.00046 Hume 0 ## 7 abstracting 0.00014 Hume 0 ## 8 abstraction 0.00049 Hume 0 ## 9 abstruse 0 Hume 0.00017 ## 10 absurd 0.00049 Hume 0.00017 ## # ℹ 2,448 more rows library(scales) freq %&gt;% ggplot(aes(x = rf, y = Berkeley)) + geom_abline(color = &quot;grey40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3, color = &quot;darkblue&quot;) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, color = &quot;grey30&quot;) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + facet_wrap(~author, ncol = 2) + theme(legend.position = &quot;none&quot;) + theme_minimal() + labs(y = &quot;Berkeley&quot;, x = NULL) Эти три трактата нам еще понадобятся, поэтому сохраняем опрятный текст с уже удаленными стоп-словами. save(corpus_words_nosp, file = &quot;data/Idea.Rdata&quot;) Литература "],["эмоциональная-тональность.html", "Тема 12 Эмоциональная тональность 12.1 Анализ тональности 12.2 Подходы 12.3 Тезаурусы 12.4 Лексиконы для русского языка 12.5 Анализ тональности с Tidy Data 12.6 Подготовка текста 12.7 Модификация лексикона 12.8 Соединение лексикона и текста 12.9 Тональность на оси времени 12.10 Для других языков 12.11 Виды эмоций", " Тема 12 Эмоциональная тональность 12.1 Анализ тональности Анализ тональности текста (англ. Sentiment analysis) — задача компьютерной лингвистики, заключающаяся в определении эмоциональной окраски (тональности) текста и, в частности, в выявлении эмоциональной оценки авторов по отношению к объектам, описываемым в тексте. В целом, задача анализа тональности текста эквивалентна задаче классификации текста, где категориями текстов могут быть тональные оценки (позитивная, негативная или нейтральная). 12.2 Подходы Сделав большое обобщение, можно разделить существующие подходы на следующие категории: подходы, основанные на правилах; подходы, основанные на словарях; машинное обучение с учителем; машинное обучение без учителя. В этом уроке мы будем работать только со словарями. Подробнее о других подходах можно прочитать здесь. 12.3 Тезаурусы Подходы, основанные на словарях, используют так называемые тональные словари (англ. affective lexicons) для анализа текста. В простом виде тональный словарь представляет из себя список слов со значением тональности для каждого слова. Сравнивая текст (или отрывок текста) со словарем, мы можем вычислить тональность для всего текста (или отрывка). Словари эмоциональной тональности размечаются вручную, полуавтоматически или автоматически на основании уже существующих тезаурусов. В основном они содержат лексику из соцсетей, отзывов, Википедии, и поэтому не очень подходят для анализа литературных текстов, особенно написанных 100-200 лет назад. Разные тезаурусы используют разные шкалы: бинарную: negative / positive (-1 / 1) тринарную: бинарная + 0 (neutral) ранжированную: например, от -5 до 5 В некоторых случаях дополнительно вводятся различия между оценочной лексикой (“неряшливый”) и негативным фактом (“кража”) и т.п. 12.4 Лексиконы для русского языка Установка пакета с лексиконами. remotes::install_github(&quot;dmafanasyev/rulexicon&quot;) Начало работы. library(rulexicon) library(tidyverse) library(tidytext) 12.4.1 Chen &amp; Skiena Русский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen and Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1). set.seed(0211) chen_skiena &lt;- hash_sentiment_chen_skiena sample_n(chen_skiena, 10) ## token score ## 1: пустошь -1 ## 2: революционный 1 ## 3: расизм -1 ## 4: медленный -1 ## 5: лекарство 1 ## 6: поддержка 1 ## 7: стабилизировать 1 ## 8: лисица -1 ## 9: мягкость 1 ## 10: примесь -1 12.4.2 RuSentLex 2016 Для русского языка в свободном доступе находится РуСентиЛекс (“Создание лексикона оценочных слов русского языка РуСентилекс” 2016). Он содержит около 15000 уникальных слов или фраз, среди которых оценочные слова, а также слова и выражения, не передающие оценочное отношения автора, но имеющие положительную или отрицательную ассоциацию (коннотацию). Возможные значения переменной sentiment: neutral, positive, negative, positive/negative. set.seed(1102) rusenti2016 &lt;- hash_rusentilex_2016 sample_n(rusenti2016, 10) ## token speech.part lemma sentiment source ## 1 любить Verb любить neutral fact ## 2 самозабвенный Adj самозабвенный positive feeling ## 3 потянуть Verb потянуть neutral fact ## 4 рожа Noun рожа negative opinion ## 5 острозаразный Adj острозаразный negative fact ## 6 тюфяк Noun тюфяк neutral fact ## 7 просчитаться Verb просчитаться negative fact ## 8 перехлестнуть Verb перехлестнуть positive/negative opinion ## 9 беспокойный Adj беспокойный negative feeling ## 10 задеревенеть Verb задеревенеть negative opinion ## ambiguity ## 1 ЛЮБИТЬ (НУЖДАТЬСЯ В УСЛОВИЯХ) ## 2 ## 3 ТЯНУТЬ (ТАЩИТЬ НАПРАВЛЯЯ КУДА-ЛИБО) ## 4 ЛИЦО ЧЕЛОВЕКА ## 5 ## 6 МАТРАЦ ## 7 ## 8 ОБУЯТЬ, ОБУРЕВАТЬ ## 9 ## 10 ДЕРЕВЕНЕТЬ, КОСТЕНЕТЬ (НЕМЕТЬ, ВОЗМОЖНО ОТВЕРДЕВАЯ) При работе с этим лексиконом следует учитывать, что для отдельных слов он содержит несколько вхождений, как положительных, так и отрицательных, например: rusenti2016 %&gt;% filter(token == &quot;нежный&quot;) ## token speech.part lemma sentiment source ambiguity ## 1 нежный Adj нежный negative opinion ХРУПКИЙ (СЛИШКОМ СЛАБЫЙ, НЕЖНЫЙ) ## 2 нежный Adj нежный positive opinion ЛАСКОВЫЙ ## 3 нежный Adj нежный positive opinion МЯГКИЙ, НЕЖНЫЙ НА ОЩУПЬ ## 4 нежный Adj нежный positive opinion НЕЖНЫЙ ПО ЗВУЧАНИЮ 12.4.3 AFINN Словарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5. set.seed(0211) afinn &lt;- hash_sentiment_afinn_ru sample_n(afinn, 10) ## token score ## 1: экстатический 1.7 ## 2: знаковый 1.7 ## 3: счастливчик 5.0 ## 4: суматошный -3.3 ## 5: гад -5.0 ## 6: выразительный 5.0 ## 7: жутковатый -5.0 ## 8: креативность 5.0 ## 9: обнадёживать 2.5 ## 10: привлекательно 5.0 12.4.4 NRC Переведенная версия списка положительных и отрицательных слов Mohammad &amp; Turney (2010)46. Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1. set.seed(1102) nrc &lt;- hash_sentiment_nrc_emolex_ru sample_n(nrc, 10) ## token score ## 1: энциклопедия 1 ## 2: подходящий 1 ## 3: издевательство -1 ## 4: пленительный 1 ## 5: утопический 1 ## 6: недоброжелательство -1 ## 7: вероломный -1 ## 8: вирулентность -1 ## 9: победоносный 1 ## 10: незначительность -1 12.5 Анализ тональности с Tidy Data (Silge and Robinson 2017), говоря об анализе эмоциональной тональности в духе tidy data, предлагают следующую иллюстрацию: Авторы предостерегают, впрочем, что современные лексиконы могут быть не слишком информативны применительно к классической литературе (в книге анализируются романы Джейн Остин). Мы попробуем, тем не менее, подвергнуть “сентиментальному анализу” сентиментальную прозу Карамзина. 12.6 Подготовка текста Прежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат, как мы делали в предыдущем уроке. library(udpipe) liza &lt;- readLines(con = &quot;files/karamzin_liza.txt&quot;) russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) liza_ann &lt;- udpipe_annotate(russian_syntagrus, liza) liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-paragraph_id, -sentence, -xpos) Разделим весь текст “Лизы” на отрывки по 100 слов: это позволит проверить, как меняется эмоциональная тональность произведения по мере развития сюжета. liza_tbl &lt;- as_tibble(liza_ann) %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% select(lemma) %&gt;% rename(token = lemma) %&gt;% mutate(chunk = round(((row_number() + 50) / 100), 0)) liza_tbl ## # A tibble: 5,049 × 2 ## token chunk ## &lt;chr&gt; &lt;dbl&gt; ## 1 мочь 1 ## 2 быть 1 ## 3 никто 1 ## 4 из 1 ## 5 жить 1 ## 6 в 1 ## 7 Москва 1 ## 8 не 1 ## 9 знать 1 ## 10 так 1 ## # ℹ 5,039 more rows В тексте чуть более 5000 слов, у нас получился 51 отрывок. 12.7 Модификация лексикона Для анализа эмоциональной тональности возьмем лексикон AFINN, доступный в пакете rulexicon. Слово “старый” имеет в этом лексиконе отрицательную оценку, что не соответствует словоупотреблению Карамзина, и мы его удалили; слову “чувствительный” поменяли знак с минуса на плюс, поскольку для автора “Бедной Лизы” это скорее положительное качество. Код ниже показывает, как вносятся подобные изменения: lex &lt;- hash_sentiment_afinn_ru lex &lt;- lex %&gt;% filter(token != &quot;старый&quot;) lex &lt;- lex %&gt;% mutate_at(vars(score), ~ case_when(token == &quot;чувствительный&quot; ~ 1.7, TRUE ~ .)) 12.8 Соединение лексикона и текста Стоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join() (см. предыдущие уроки), наш текст с одним из лексиконов, и это само по себе отфильтрует ту лексику, которая может быть потенциально интересна. Напомню, что inner_join() работает так: liza_sent &lt;- liza_tbl %&gt;% inner_join(lex) liza_sent ## # A tibble: 461 × 3 ## token chunk score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 хорошо 1 5 ## 2 новый 1 1.7 ## 3 приятный 1 5 ## 4 новый 1 1.7 ## 5 красота 1 5 ## 6 приятный 1 5 ## 7 мрачный 1 -5 ## 8 горе 1 -5 ## 9 ужасный 1 -5 ## 10 величественный 1 3.3 ## # ℹ 451 more rows Сложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности: liza_chunk_sent &lt;- liza_sent %&gt;% group_by(chunk) %&gt;% summarise(sum = sum(score)) %&gt;% arrange(sum) head(liza_chunk_sent, 10) ## # A tibble: 10 × 2 ## chunk sum ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5 -43.3 ## 2 34 -35.8 ## 3 31 -25 ## 4 38 -20 ## 5 42 -20 ## 6 50 -20 ## 7 3 -15.7 ## 8 51 -15 ## 9 4 -14.2 ## 10 46 -13.3 Довольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической ее развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом). Представим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов: library(reshape2) library(wordcloud) # добавляем новый столбец для удобства визуализации liza_sent_class &lt;- liza_sent %&gt;% mutate(tone = case_when( score &gt;= 0 ~ &quot;pos&quot;, score &lt; 0 ~ &quot;neg&quot;)) set.seed(0211) liza_sent_class %&gt;% filter(chunk %in% c(3, 4, 5)) %&gt;% count(token, tone, sort = T) %&gt;% acast(token ~ tone, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;grey20&quot;, &quot;grey80&quot;), max.words = 99) Здесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Таким образом, с количественной точки зрения, самые мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”: Карамзин-историк уже переигрывает Карамзина-новелиста. Приведем небольшой отрывок из этой части повести: Иногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях. 12.9 Тональность на оси времени Таблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах. Каждый отрывок, напомню, – это 100 слов. liza_chunk_sent &lt;- liza_chunk_sent %&gt;% mutate(tone = case_when( sum &gt;= 0 ~ &quot;pos&quot;, sum &lt; 0 ~ &quot;neg&quot;)) library(ggplot2) ggplot(liza_chunk_sent, aes(chunk, sum, fill = tone)) + geom_col(show.legend = F) График получился весьма осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей. У меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти. В отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении. Еще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы: Грозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности. На графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”. 38, 39, 42 – Эраст отправляется на войну. Все, как это принято у Карамзина, плачут, что зафиксировал и наш график. Наконец, в отрывках 49-51 доминирует тема смерти: library(RColorBrewer) pal &lt;- RColorBrewer::brewer.pal(5, &quot;Dark2&quot;) liza_sent_class %&gt;% filter(chunk %in% c(49:51)) %&gt;% filter(tone == &quot;neg&quot;) %&gt;% count(token, sort = T) %&gt;% with(wordcloud(token, n, max.words = 100, colors = pal)) Следует отметить, что часть этих слов относится не к самой девушке, а к ее матери. 12.10 Для других языков Для языков, которые используют латиницу, в R есть отличный пакет под названием syuzhet. Его разработал известный цифровой литературовед Мэтью Джокерс47. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе48. Для анализа возьмем стихотворение Роберта Фроста “Медведи” (перевод). library(syuzhet) ## ## Attaching package: &#39;syuzhet&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## rescale my_example_text &lt;- &quot;The bear puts both arms around the tree above her And draws it down as if it were a lover And its choke cherries lips to kiss good-bye, Then lets it snap back upright in the sky. Her next step rocks a boulder on the wall (She&#39;s making her cross-country in the fall). Her great weight creaks the barbed-wire in its staples As she flings over and off down through the maples, Leaving on one wire moth a lock of hair. Such is the uncaged progress of the bear. The world has room to make a bear feel free; The universe seems cramped to you and me. Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests. He paces back and forth and never rests The toe-nail click and shuffle of his feet, The telescope at one end of his beat - And at the other end the microscope, Two instruments of nearly equal hope, And in conjunction giving quite a spread. Or if he rests from scientific tread, &#39;Tis only to sit back and sway his head Through ninety odd degrees of arc, it seems, Between two metaphysical extremes. He sits back on his fundamental butt With lifted snout and eyes (if any) shut, (he almost looks religious but he&#39;s not), And back and forth he sways from cheek to cheek, At one extreme agreeing with one Greek - At the other agreeing with another Greek Which may be thought, but only so to speak. A baggy figure, equally pathetic When sedentary and when peripatetic.&quot; Пакет позволяет разделить текст на предложения: sent_vec &lt;- get_sentences(my_example_text) sent_vec[1:2] ## [1] &quot;The bear puts both arms around the tree above her\\nAnd draws it down as if it were a lover\\nAnd its choke cherries lips to kiss good-bye,\\nThen lets it snap back upright in the sky.&quot; ## [2] &quot;Her next step rocks a boulder on the wall\\n(She&#39;s making her cross-country in the fall).&quot; Воспользуемся регулярными выражениями, чтобы удалить переносы строк: library(stringr) sent_vec &lt;- str_replace_all(sent_vec, &quot;\\\\n&quot;, &quot; &quot;) sent_vec[1:2] ## [1] &quot;The bear puts both arms around the tree above her And draws it down as if it were a lover And its choke cherries lips to kiss good-bye, Then lets it snap back upright in the sky.&quot; ## [2] &quot;Her next step rocks a boulder on the wall (She&#39;s making her cross-country in the fall).&quot; Если вы хотите прочитать в R большой файл, для этого есть функция get_text_as_string, который необходимо указать путь к файлу на компьютере или url. Функция get_sentiment принимает в качестве аргументов вектор и метод (о методах можно подробнее узнать, вызвав помощь). bing_vector &lt;- get_sentiment(sent_vec, method=&quot;bing&quot;) bing_vector ## [1] 1 -1 0 1 0 -3 0 -1 0 -2 Как видно, не очень радостное стихотворение. Взглянем на самое мрачное предложение: sent_vec[which.min(bing_vector)] ## [1] &quot;Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests.&quot; Результат применения различных методов может несколько отличаться. afinn_vector &lt;- get_sentiment(sent_vec, method=&quot;afinn&quot;) afinn_vector ## [1] 3 0 3 2 1 -5 2 -2 0 -2 sent_vec[which.min(afinn_vector)] ## [1] &quot;Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests.&quot; Но в нашем случае методы согласны: сравнение человека с беспокойным медведем в клетке – эмоциональный минимум стихотворения. Эмоциональная валентность произведения в целом вычисляется либо как сумма, либо как среднее всех значений: sum(afinn_vector) ## [1] 2 mean(afinn_vector) ## [1] 0.2 Не так уж плохо: за счет первой части про медведицу, с удовольствием поедающую черемуху, общая тональность скорее положительная (хотя ваш читательский опыт может говорить об обратном). Результат можно передать функции plot(): plot(afinn_vector, type=&quot;l&quot;, main=&quot;Example Plot Trajectory&quot;, xlab = &quot;Narrative Time&quot;, ylab= &quot;Emotional Valence&quot;) О других способах визуализировать результат можно узнать из виньетки к пакету. 12.11 Виды эмоций Лексикон NRC дает возможность позволяет не просто оценить эмоциональную валентность, но и выявить конкретные эмоции: nrc_data &lt;- get_nrc_sentiment(sent_vec) nrc_data ## anger anticipation disgust fear joy sadness surprise trust negative positive ## 1 3 5 1 1 4 1 3 4 1 6 ## 2 1 0 0 1 0 2 0 0 2 0 ## 3 0 1 1 1 1 1 1 1 1 1 ## 4 1 1 0 1 1 0 0 0 0 1 ## 5 1 0 0 1 0 0 0 0 1 0 ## 6 2 1 0 2 0 1 0 0 3 0 ## 7 0 1 0 0 1 0 1 2 0 2 ## 8 0 0 0 0 0 0 0 1 0 1 ## 9 0 1 0 0 0 0 0 2 1 2 ## 10 0 0 1 0 0 1 0 0 1 1 Выбрать конкретные эмоции можно так (но результат в нашем случае не очень осмысленный): joy_items &lt;- which(nrc_data$joy &gt; 0) sent_vec[joy_items] На графике это можно отразить при помощи базовой barplot: barplot( sort(colSums(prop.table(nrc_data[, 1:8]))), horiz = TRUE, cex.names = 0.7, las = 1, main = &quot;Emotions in Sample text&quot;, xlab=&quot;Percentage&quot; ) Сложно осуждать машину за не вполне верно считанную тональность: Фрост – сложный автор. Для отзывов на Tripadvisor это может сработать лучше. Литература "],["тематическое-моделирование.html", "Тема 13 Тематическое моделирование 13.1 Что такое LDA 13.2 Распределение Дирихле 13.3 Подтоговка данных 13.4 Матрица встречаемости 13.5 Число тем 13.6 Модель LDA 13.7 Слова и темы 13.8 Сравнение топиков 13.9 Темы и документы 13.10 Распределения вероятности для топиков 13.11 Интерактивные визуализации", " Тема 13 Тематическое моделирование 13.1 Что такое LDA Приступая к анализу текстов (текст-майнингу), мы часто хотим разделить большую коллекцию документов на некие естественные группы. Одним из способов такого деления является тематическое моделирование. Латентное размещение Дирихле (LDA) - особенно популярный метод для построения тематической модели. В нем каждый документ рассматривается как смесь тем, а каждая тема - как смесь слов. Это позволяет документам “перекрывать” друг друга по содержанию, а не разделяться на отдельные группы, что отражает типичное использование естественного языка. Например, мы можем представить коллекцию документов по истории искусства, в которой будут тексты о живописи, архитектуре и фотографии. Тема искусства будет представлена во всех документах, где-то может быть будет сочетание 2-3 тем сразу. Источник: Blei, D. M. (2012), Probabilistic topic models Чем-то работа LDA похожа на то, как мы размечаем текст текстовыделителями: например, в этом курсе зеленым можно выделить код, желтым – математические и статистические отступления, а розовым – окологуманитарные сюжеты. Как вы уже поняли, ключевой вопрос в том, сколько у вас текстовыделителей. При тематическом моделировании этот параметр задается вручную, и дальше мы посмотрим, как это делается. 13.2 Распределение Дирихле Математические и статистические основания LDA достаточно хитроумны, но к счастью пользоваться моделью можно и без погружения в интегралы, как можно водить машину, не умея собрать двигатель внутреннего сгорания. Общие принципы на русском языке хорошо изложены в статье “Как понять, о чем текст, не читая его” на сайте “Системный блок”. Альфа и бета на этой схеме - гиперпараметры распределения. Гиперпараметры регулируют распределения тем по документам и слов по темам. Наглядно это можно представить так: При α = 1 получается равномерное распределение: темы распределены равномерно (заметим, что α также называют “параметром концентрации”). При значениях α &gt; 1 выборки начинают концентрироваться в центре треугольника, представляя собой равномерную смесь всех тем. При низких значениях альфа α &lt; 1 большинство наблюдений находится в углах – скорее всего, в в этом случае в документах будет меньше смешения тем49. Распределение документов по топикам θ зависит от значения α, поскольку θ ~ Dir(α). Из θ выбирается конкретная тема Z. Аналогичным образом гиперпараметр 𝛽 управляет распределением слов по темам. При меньших значениях 𝛽 темы, скорее всего, будут больше различаться. Распределение слов φ темы Z ~ Dir(β). Конкретное слово W выбирается уже из этого распределения. Можно представить себе банкетный зал со столами: если их несколько, и они стоят по углам, то вероятность встретить вашего знакомого в углу выше, чем в центре зала. Если он при этом вегетарианец, вы его будете искать у стола с овощами, а не с котлетами. Метафору можно понимать двояко. С одной стороны, ваш знакомый – это слово, а стол – тема, или топик. Он может нечаянно прибиться к столу с котлетами, как и слово “футбол” может оказаться в финансовых новостях. Но это сближение не будет таким устойчивым, как, например, связь слова “банк” с финансовым топиком. С другой стороны, сами “документы” склонны прибиваться к определенным топикам: открыв газету, вы не ожидаете увидеть в одной статье новости вирусологии, педагогики и финансового регулирования такое бывает только в новых медиа. 13.3 Подтоговка данных Чтобы понять возможности алгоритма, мы попробуем передать ему архив телеграм-канала Antibarbari. Мы подготовили его для анализа в одном из уроков выше; сейчас просто загружаем. library(tidyverse) load(&quot;./data/AntibarbariTidy.Rdata&quot;) text_tidy &lt;- text_tidy %&gt;% select(doc_id, lemma) text_tidy ## # A tibble: 73,683 × 2 ## doc_id lemma ## &lt;chr&gt; &lt;chr&gt; ## 1 doc1 новый ## 2 doc1 видео ## 3 doc1 на ## 4 doc1 канал ## 5 doc1 фрагмент ## 6 doc1 семинар ## 7 doc2 подборка ## 8 doc2 видео ## 9 doc2 семинар ## 10 doc2 по ## # ℹ 73,673 more rows Для тематического моделирования не нужны стоп-слова (которые могут быть интересны для атрибуции авторства). Удаляем их. library(stopwords) library(stringr) stop &lt;- stopwords(language = &quot;ru&quot;, source = &quot;stopwords-iso&quot;) %&gt;% as_tibble() %&gt;% rename(lemma = value) text_tidy_nosw &lt;- text_tidy %&gt;% anti_join(stop) %&gt;% filter(!str_detect(lemma, &quot;либо&quot;)) %&gt;% filter(!lemma %in% c(&quot;ибо&quot;, &quot;стр&quot;, &quot;кр&quot;, &quot;нить&quot;, &quot;наверное&quot;, &quot;например&quot;, &quot;фиг&quot;)) %&gt;% filter(!str_detect(lemma, &quot;нибудь&quot;)) ## Joining with `by = join_by(lemma)` text_tidy_nosw ## # A tibble: 38,974 × 2 ## doc_id lemma ## &lt;chr&gt; &lt;chr&gt; ## 1 doc1 видео ## 2 doc1 канал ## 3 doc1 фрагмент ## 4 doc1 семинар ## 5 doc2 подборка ## 6 doc2 видео ## 7 doc2 семинар ## 8 doc2 медленный ## 9 doc2 чтение ## 10 doc2 философия ## # ℹ 38,964 more rows # save(text_tidy_nosw, file = &quot;data/antibarbari_words_tidy.Rdata&quot;) Поскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf50. text_count &lt;- text_tidy_nosw %&gt;% group_by(doc_id, lemma) %&gt;% count(lemma) head(text_count) ## # A tibble: 6 × 3 ## # Groups: doc_id, lemma [6] ## doc_id lemma n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 doc1 видео 1 ## 2 doc1 канал 1 ## 3 doc1 семинар 1 ## 4 doc1 фрагмент 1 ## 5 doc10 автор 1 ## 6 doc10 вшэ 1 Очень короткие слова, если они остались, скорее всего будут неинформативны, их тоже можно удалить. text_count_nchar &lt;- text_count %&gt;% mutate(nchar = nchar(lemma)) %&gt;% filter(nchar &gt; 2) %&gt;% select(-nchar) Кроме того, малоинтересны очень короткие посты (меньше 10 слов). Их тоже удаляем. text_count_long &lt;- text_count_nchar %&gt;% group_by(doc_id) %&gt;% mutate(sum = sum(n)) %&gt;% filter(sum &gt; 10) %&gt;% select(-sum) %&gt;% ungroup() # save(text_count_long, file = &quot;data/antibarbari_count.Rdata&quot;) 13.4 Матрица встречаемости Для работы с LDA в R устанавливаем пакет topicmodels. На входе нужная нам функция этого пакета принимает такую структуру данных, как document-term matrix (dtm), которая используется для хранения сильно разреженных данных и происходит из популярного пакета для текст-майнинга tm. Поэтому “тайдифицированный” текст придется для моделирования преобразовать в этот формат, а полученный результат вернуть в опрятный формат для визуализаций51. Для преобразования подготовленного корпуса в формат dtm воспользуемся возможностями пакета tidytext: library(tidytext) text_dtm &lt;- text_count_long %&gt;% cast_dtm(doc_id, term = lemma, value = n) text_dtm ## &lt;&lt;DocumentTermMatrix (documents: 581, terms: 10207)&gt;&gt; ## Non-/sparse entries: 31356/5898911 ## Sparsity : 99% ## Maximal term length: 25 ## Weighting : term frequency (tf) Убеждаемся, что почти все ячейки в нашей матрице – нули (99% разреженность). 13.5 Число тем Количество тем для модели всегда задается вручную. Мы не всегда заранее знаем, сколько тем в нашем корпусе, и здесь на помощь приходит функция perplexity() из topicmodels. Она показывает, насколько подогнанная модель не соответствует данным – поэтому чем значение меньше, тем лучше. Подгоним сразу несколько моделей с разным количеством тем и посмотрим, какая из них покажет себя лучше. Выполнение кода ниже займет какое-то время. library(topicmodels) n_topics &lt;- c(2, 4, 8, 16, 32, 64) text_lda_compare &lt;- n_topics %&gt;% map(LDA, x = text_dtm, control = list(seed = 0211)) data_frame(k = n_topics, perplex = map_dbl(text_lda_compare, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Оценка LDA модели&quot;, subtitle = &quot;Оптимальное количество топиков&quot;, x = &quot;Число топиков&quot;, y = &quot;Perplexity&quot;) Если верить графику, предпочтительны 64 темы (на самом деле, если подогнать еще больше моделей, то и все 200). Но спешить не стоит. Если эксперт задаст в параметрах своей программы слишком мало тем, то разные самостоятельные топики сольются в один и станут неразличимы для взгляда исследователя. Если будет задано слишком большое число топиков, то помимо реальных тем, присутствующих в корпусе, появятся «паразитные», которые с точки зрения математического аппарата показывают совместно встречающиеся слова, однако на практике эти слова не будут образовывать тематически самостоятельных контекстов. Поэтому процесс тематического моделирования включает этап подбора нужного количества топиков и соизмерение получившихся результатов с разноплановыми соображениями. Источник Мои разноплановые соображения говорят, что больше 10 топиков выделять непродуктивно. Некоторые эксперименты позволяют остановиться на семи. 13.6 Модель LDA text_lda &lt;- LDA(text_dtm, k = 7, control = list(seed = 1111)) Итак, наша тематическая модель готова. Осталось понять, что с ней делать. 13.7 Слова и темы Пакет tidytext дает возможность “тайдифицировать” объект lda с использованием разных методов. Метод β (“бета”) извлекает вероятность того, что слово происходит из данного топика. text_topics &lt;- tidy(text_lda, matrix = &quot;beta&quot;) text_topics %&gt;% filter(term == &quot;огурец&quot;) %&gt;% arrange(-beta) ## # A tibble: 7 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 7 огурец 2.09e- 3 ## 2 6 огурец 4.55e- 4 ## 3 5 огурец 3.91e- 4 ## 4 3 огурец 7.97e-286 ## 5 2 огурец 7.13e-287 ## 6 1 огурец 4.78e-287 ## 7 4 огурец 2.44e-290 Например, слово “огурец” с большей вероятностью порождено темой 1, чем остальными темами 🥒🥒🥒 Посмотрим на главные термины в топиках. text_top_terms &lt;- text_topics %&gt;% group_by(topic) %&gt;% arrange(-beta) %&gt;% slice_head(n = 12) %&gt;% ungroup() head(text_top_terms) ## # A tibble: 6 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 платон 0.0320 ## 2 1 цицерон 0.0224 ## 3 1 аттик 0.0137 ## 4 1 даймон 0.00589 ## 5 1 диалог 0.00555 ## 6 1 греческий 0.00549 text_top_terms %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol=4) + coord_flip() В “огуречной” теме 1 обнаружились также “плод”, “тыква” и “мед”. Видимо, наш алгоритм вполне “узнал” плодово-овощную рубрику Ирины Макаровой, в которой она рассказывает, что и зачем выращивали древние греки и римляне. А вот “бобы” прибились к более философскому топику 4. 13.8 Сравнение топиков Сравним топики 1 и 2 по формуле: \\(log_2\\left(\\frac{β_2}{β_1}\\right)\\). Если \\(β_2\\) в 2 раза больше \\(β_1\\), то логарифм будет равен 1; если наоборот, то -1. На всякий случай: \\(\\frac{1}{2} = 2^{-1}\\). Для подсчетов снова придется трансформировать данные. beta_spread &lt;- text_topics %&gt;% filter(topic %in% c(1, 2)) %&gt;% mutate(topic = paste0(&quot;topic_&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic_1 &gt; .001 | topic_2 &gt; .001) %&gt;% mutate(log_ratio = log2(topic_2 / topic_1)) head(beta_spread) ## # A tibble: 6 × 4 ## term topic_1 topic_2 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 абарид 1.87e-293 0.00236 964. ## 2 автор 1.73e- 3 0.00387 1.17 ## 3 аида 1.87e-286 0.00157 940. ## 4 академия 1.03e- 3 0.000418 -1.31 ## 5 анализ 9.59e- 21 0.00148 57.1 ## 6 античность 1.98e- 3 0.000726 -1.44 На графике выглядит понятнее: beta_log_ratio &lt;- beta_spread %&gt;% mutate(sign = case_when(log_ratio &gt; 0 ~ &quot;pos&quot;, log_ratio &lt; 0 ~ &quot;neg&quot;)) %&gt;% select(-topic_2, -topic_1) %&gt;% group_by(sign) %&gt;% arrange(desc(abs(log_ratio))) %&gt;% slice_head(n = 10) beta_log_ratio %&gt;% ggplot(aes(reorder(term, log_ratio), log_ratio, fill = sign)) + geom_col(show.legend = FALSE) + xlab(&quot;термин&quot;) + ylab(&quot;log2 (beta_2 / beta_1)&quot;) + coord_flip() В теме 1 видим и других обитателей садов и огородов. 13.9 Темы и документы Распределение тем по документам хранит матрица gamma. text_documents &lt;- tidy(text_lda, matrix = &quot;gamma&quot;) text_documents %&gt;% filter(topic == 1) %&gt;% arrange(-gamma) ## # A tibble: 581 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 doc201 1 1.00 ## 2 doc312 1 1.00 ## 3 doc590 1 0.999 ## 4 doc533 1 0.999 ## 5 doc324 1 0.999 ## 6 doc569 1 0.999 ## 7 doc185 1 0.999 ## 8 doc570 1 0.999 ## 9 doc582 1 0.999 ## 10 doc557 1 0.999 ## # ℹ 571 more rows Значение gamma можно понимать как долю слов в документе, происходящую из данного топика. Например, тема 1 представлена в документе 268. Посмотрим на него: doc_268 &lt;- text_tidy %&gt;% filter(doc_id == &quot;doc268&quot;) %&gt;% pull(lemma) %&gt;% paste0(collapse = &quot; &quot;) paste0(substr(doc_268, 1, 279), &quot;...&quot;) ## [1] &quot;тыква закончиться лето подходить к завершение и наш сезонный плодовый ягодный рубрика сегодня мы вспомянуть о тыква ведь грек охотно она выращивать но привычный мы тыква и тот что быть у грек и римлянин не один и тот же хотя весь они относиться к семейство тыквенный тыква с прил...&quot; Каждый документ в рамках LDA рассматривается как собрание тем. Значит, сумма всех гамм для текста должна быть равна единице. Проверим. text_documents %&gt;% filter(document == &quot;doc288&quot;) %&gt;% summarise(sum = sum(gamma)) ## # A tibble: 1 × 1 ## sum ## &lt;dbl&gt; ## 1 1 Все верно! Теперь отберем несколько длинных постов и посмотрим, какие топики в них представлены. long_posts &lt;- text_tidy %&gt;% group_by(doc_id) %&gt;% summarise(nwords = n()) %&gt;% arrange(-nwords) %&gt;% slice_head(n = 6) %&gt;% pull(doc_id) long_posts ## [1] &quot;doc689&quot; &quot;doc201&quot; &quot;doc590&quot; &quot;doc249&quot; &quot;doc268&quot; &quot;doc697&quot; text_documents %&gt;% filter(document %in% long_posts) %&gt;% arrange(-gamma) %&gt;% ggplot(aes(as.factor(topic), gamma, color = document)) + geom_boxplot(show.legend = F) + facet_wrap(~document) 13.10 Распределения вероятности для топиков text_documents %&gt;% ggplot(aes(gamma, fill = as.factor(topic))) + geom_histogram(show.legend = F) + facet_wrap(~ topic, ncol = 4) + scale_y_log10() + labs(title = &quot;Распределение вероятностей для каждого топика&quot;, y = &quot;Число документов&quot;, x = expression(gamma)) Почти ни одна тема не распределена равномерно: гамма чаще всего принимает значения либо около нуля, либо в районе единицы. Тема 2, однако, отклоняется от этого правила. text_top_terms %&gt;% filter(topic == 2) ## # A tibble: 12 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 платон 0.00734 ## 2 2 текст 0.00644 ## 3 2 понедельник 0.00574 ## 4 2 аристотель 0.00538 ## 5 2 цифровой 0.00498 ## 6 2 пифагор 0.00462 ## 7 2 статья 0.00426 ## 8 2 бог 0.00426 ## 9 2 модуль 0.00393 ## 10 2 автор 0.00387 ## 11 2 видео 0.00387 ## 12 2 древний 0.00346 Действительно, здесь встречаются слова, связанные с изучением античности в целом и потому возможные в разных контекстах. Можно сказать, что это своего рода метатопик, характеризующий тематику канала в целом. 13.11 Интерактивные визуализации Более подробно изучить полученную модель можно при помощи интерактивной визуализации. Функция ниже заимстовована отсюда. topicmodels2LDAvis &lt;- function(x, ...){ post &lt;- topicmodels::posterior(x) if (ncol(post[[&quot;topics&quot;]]) &lt; 3) stop(&quot;The model must contain &gt; 2 topics&quot;) mat &lt;- x@wordassignments LDAvis::createJSON( phi = post[[&quot;terms&quot;]], theta = post[[&quot;topics&quot;]], vocab = colnames(post[[&quot;terms&quot;]]), doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE) ) } Свежую верию пакета LDAvis надо устанавливать из репозитория. devtools::install_github(&quot;cpsievert/LDAvis&quot;) library(LDAvis) LDAvis::serVis(topicmodels2LDAvis(text_lda), out.dir = &quot;ldavis&quot;) Ссылка на интерактивную визуализацию топиков. Об этом приложении см. здесь. Значения лямбды, очень близкие к нулю, показывают термины, наиболее специфичные для выбранной темы. Это означает, что вы увидите термины, которые “важны” для данной конкретной темы, но не обязательно “важны” для всего корпуса. Значения лямбды, близкие к единице, показывают те термины, которые имеют наибольшее соотношение между частотой терминов по данной теме и общей частотой терминов из корпуса52. Сами разработчики советуют выставлять значение лямбды в районе 0.6. https://www.mithilaguha.com/post/topic-modeling-and-latent-dirichlet-allocation↩︎ https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf/49704#49704?newreg=c17592380de141cf9064c9c5ef09cdc6↩︎ https://www.tidytextmining.com/topicmodeling.html↩︎ https://stackoverflow.com/questions/50726713/meaning-of-bar-width-for-pyldavis-for-lambda-0↩︎ "],["латентно-семантический-анализ.html", "Тема 14 Латентно-семантический анализ 14.1 Что это такое 14.2 Векторное представление слов 14.3 Сингулярное разложение матрицы 14.4 Простой пример 14.5 Векторы слов 14.6 Пакеты для работы с LSA 14.7 LSA c использованием Tidy Data", " Тема 14 Латентно-семантический анализ 14.1 Что это такое В этом уроке речь пойдет о таком методе тематического моделирования, как латентно-семантический анализ, или LSA (в области информационного поиска называемый также LSI, Latent Semantic Indexing). Как и LDA, это метод метод обработки информации на естественном языке, который позволяет находить взаимосвязь между коллекцией документов и встречающимися в них терминами за счет сопоставления этих документов и терминов с некоторыми темами. Слово “скрытый” в названии указывает на то, что семантические взаимосвязи между документами и терминами, как правило, заранее не известны. В исходной термдокументной матрице, которую мы передаем алгоритму, не видно закономерностей. Как раз задача аналитика – их обнаружить, сгруппировав по темам, с одной стороны, термины, а с другой – документы. Создатели метода называют LSA “теорией смысла” и видят в нем ответ на вопрос о том, как носители языка приходят к пониманию смысла. Как вообще машина может обнаружить “близкие” слова и документы? В 1957 британский лингвист Джон Руперт Фёрс сформулировал это так: you shall know a word by the company it keeps. Томас Ландауер, один из создателей LSA, обобщает этот подход в статье “LSA as a theory of meaning”: синтаксические связи переоценены, и смысл параграфов и целых текстов – это лишь функция от смысла отдельных слов. Смысл – это не статическая константа, а динамическое отношение, или система отношений. На уровне реализации действительно LSA показал способность справляться с задачами, которые до середины 90-х считались прерогативой живого человека. Например, с проблемами полисемии и синонимии. 14.2 Векторное представление слов Мы можем представить термин в виде вектора, который хранит информацию о его встречаемости в документах. Каждый элемент вектора будет координатой в многомерном пространстве. В англоязычной литературе векторные представления называют эмбеддингами (embedding, т.е. «вложение»). Представляя объект в виде вектора, мы как бы «вкладываем» его в векторное пространство, где действуют геометрические законы. Дальше дело за вычислением расстояния (или сходства) между векторами – это уже задача из области линейной алгебры. Как правило, в алгоритме LSA используется косинусное сходство (о том, что это такое, см. видео: часть 1 и часть 2). Трудность в том, что даже для небольших коллекций документов термдокументная матрица является очень разреженной (даже в небольшом примере из предыдущего урока разреженность составляла почти 100%). Чем больше ваш корпус, тем более разреженной будет матрица: это естественно, поскольку в каждом документе встречается лишь небольшая часть всех слов. Любые действия над такими матрицами требуют больших вычислительных затрат, при этом результат не обязательно будет точным из-за синонимии и полисемии: слово apple можно встретить как в плодово-овощной рубрике, так и в заметке об IT. Справиться с этим помогает метод LSA, в основе которого лежит снижение размерности исходной матрицы. 14.3 Сингулярное разложение матрицы Мы можем “спроецировать” исходную матрицу \\(C_r\\) в пространство меньшей размерности. Если r – ранг исходной матрицы, а k – ранг новой матрицы, при этом k значительно ниже r, то матрица \\(C_k\\) называется малоранговой аппроксимацией. Рангом системы строк (столбцов) матрицы A с m строками и n столбцами называется максимальное число линейно независимых строк (столбцов). Несколько строк (столбцов) называются линейно независимыми, если ни одна из них не выражается линейно через другие. Ранг системы строк всегда равен рангу системы столбцов, и это число называется рангом матрицы. Для ее получения новой матрицы применяется трехэтапная процедура: для \\(C_r\\) строится ее сингулярное разложение (SVD (Singular Value Decomposition) по формуле: \\(C = UΣV^t\\) (подробнее см. видео); по матрице Σ строится \\(Σ_k\\): \\(r - k\\) наименьших сингулярных значений на диагонали матрицы заменяются нулями; вычисляется новая матрица \\(C_k = UΣ_kV^t\\). Теперь подробнее. Матричное разложение, или факторизация – представление матрицы в виде произведения нескольких матриц. Сингулярное разложение (SVD) матрицы A равно \\(A=U⋅Σ⋅V^t\\), где U — матрица левых сингулярных векторов матрицы A, Σ — диагональная матрица сингулярных чисел матрицы A, V — матрица правых сингулярных векторов матрицы A. Строки матрицы U соответствуют словам; а в V^t столбцы соответствуют отдельным документам. Следовательно, первая строка матрицы U показывает, в каких документах встречается слово, а первый столбец V^T показывает, какие темы встречаются в документе. Сингулярные значения в диагональной матрице всегда упорядочены по убыванию, и можно без больших потерь отсечь малоинформативные ряды или столбцы. Такое SVD называется усеченным. Сингулярные векторы (они выделены цветом) в матрицах U и V соответствуют темам в тексте, которых в общей сложности k штук. Чему равно k — человек задает вручную при вычислении разложения. Математически это выражается в том, что в диагональной матрице остается k самых больших сингулярных чисел, а остальные становятся нулями. При перемножении матриц это приведет к отсечению \\(r - k\\) столбцов в U и \\(r - k\\) рядов в \\(V^t\\). (Подробнее). Умножение U на Σ дает векторное представление слов; умножение V на Σ– векторное представление документов (Dian I. Martin. Mathematical Foundations Behind Latent Semantic Analysis). Объединяя пространство слов с пространством документов, можно находить ближайшие документы по поисковому запросу (который в рамках этой модели рассматривается как “псевдодокумент”). 14.4 Простой пример Все это будет понятнее на простом примере. (отсюда). Допустим, у нас есть пять документов. d1 : Romeo and Juliet. d2 : Juliet: O happy dagger! d3 : Romeo died by dagger”. d4 : “Live free or die”, that’s the New-Hampshire’s motto. d5 : Did you know, New Hampshire is in New-England. Поисковый запрос: dies, dagger. Очевидно, ближе всего к запросу d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 – ни одного. Составим термдокументную матрицу. df = data.frame(d1 = c(c(1, 1), rep(0, 6)), d2 = c(c(0, 1, 1, 1), rep(0, 4)), d3 = c(1, 0, 0, 1, 0, 1, 0, 0), d4 = c(rep(0, 4), rep(1, 4)), d5 = c(rep(0, 7), c(1))) rownames(df) &lt;- c(&quot;romeo&quot;, &quot;juliet&quot;, &quot;happy&quot;, &quot;dagger&quot;, &quot;live&quot;, &quot;die&quot;, &quot;free&quot;, &quot;new-hampshire&quot;) df ## d1 d2 d3 d4 d5 ## romeo 1 0 1 0 0 ## juliet 1 1 0 0 0 ## happy 0 1 0 0 0 ## dagger 0 1 1 0 0 ## live 0 0 0 1 0 ## die 0 0 1 1 0 ## free 0 0 0 1 0 ## new-hampshire 0 0 0 1 1 И применим SVD. В R для этого есть специальная функция (дальше мы увидим, что то же самое делают и другие функции). result = svd(df) S = round((diag(result$d, nrow = length(result$d))), 3) S ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2.285 0.00 0.000 0.000 0.000 ## [2,] 0.000 2.01 0.000 0.000 0.000 ## [3,] 0.000 0.00 1.361 0.000 0.000 ## [4,] 0.000 0.00 0.000 1.118 0.000 ## [5,] 0.000 0.00 0.000 0.000 0.797 Сингулярные значения меньше двух убираем, остается два сингулярных значения. S_truncated &lt;- S[1:2,1:2] S_truncated ## [,1] [,2] ## [1,] 2.285 0.00 ## [2,] 0.000 2.01 Матрица левых сингулярных векторов выглядит так: U &lt;- round((result$u), 3) U ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.396 0.280 -0.571 0.450 -0.102 ## [2,] -0.314 0.450 0.411 0.513 0.204 ## [3,] -0.178 0.269 0.497 -0.257 0.043 ## [4,] -0.438 0.369 0.013 -0.577 -0.220 ## [5,] -0.264 -0.346 0.146 0.047 0.417 ## [6,] -0.524 -0.246 -0.339 -0.273 0.155 ## [7,] -0.264 -0.346 0.146 0.047 0.417 ## [8,] -0.326 -0.460 0.317 0.237 -0.725 От нее отсекаем все столбцы, кроме первых двух: U_truncated &lt;- U[,1:2] U_truncated ## [,1] [,2] ## [1,] -0.396 0.280 ## [2,] -0.314 0.450 ## [3,] -0.178 0.269 ## [4,] -0.438 0.369 ## [5,] -0.264 -0.346 ## [6,] -0.524 -0.246 ## [7,] -0.264 -0.346 ## [8,] -0.326 -0.460 Матрица правых сингулярных векторов тоже усекается; не забудем ее транспонировать. V &lt;- round((result$v), 3) Vt_truncated &lt;- t(V[,1:2]) Vt_truncated ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.311 -0.407 -0.594 -0.603 -0.143 ## [2,] 0.363 0.541 0.200 -0.695 -0.229 Каждый столбец в этой матрице соответствует одному документу. Умножим U и Vt на S_truncated (усеченную сигму). # эмбеддинги слов word_emb &lt;- round((U_truncated %*% S_truncated), 3) rownames(word_emb) &lt;- c(&quot;romeo&quot;, &quot;juliet&quot;, &quot;happy&quot;, &quot;dagger&quot;, &quot;live&quot;, &quot;die&quot;, &quot;free&quot;, &quot;new-hampshire&quot;) word_emb ## [,1] [,2] ## romeo -0.905 0.563 ## juliet -0.717 0.904 ## happy -0.407 0.541 ## dagger -1.001 0.742 ## live -0.603 -0.695 ## die -1.197 -0.494 ## free -0.603 -0.695 ## new-hampshire -0.745 -0.925 # эмбеддинги документов doc_emb &lt;- round((S_truncated %*% Vt_truncated), 3) colnames(doc_emb) &lt;- c(&quot;d1&quot;, &quot;d2&quot;, &quot;d3&quot;, &quot;d4&quot;, &quot;d5&quot;) doc_emb ## d1 d2 d3 d4 d5 ## [1,] -0.711 -0.930 -1.357 -1.378 -0.327 ## [2,] 0.730 1.087 0.402 -1.397 -0.460 Или, что то же самое: V[,1:2] %*% S_truncated ## [,1] [,2] ## [1,] -0.710635 0.72963 ## [2,] -0.929995 1.08741 ## [3,] -1.357290 0.40200 ## [4,] -1.377855 -1.39695 ## [5,] -0.326755 -0.46029 Умножить матрицу А на В можно лишь в том случае, если число рядов в В равно числу столбцов в А 🤯🤯🤯 Координаты поискового запроса (который рассматриваем как псевдодокумент) считаем как среднее арифметическое координат: q = c(&quot;die&quot;, &quot;dagger&quot;) q_doc &lt;- colSums(word_emb[rownames(word_emb) %in% q, ]) / 2 q_doc ## [1] -1.099 0.124 Объединив все в единый датафрейм, можем визуализировать. library(tidyverse) all_df &lt;- as.data.frame(rbind(word_emb, t(doc_emb), q_doc)) all_tbl &lt;- as_tibble(all_df, rownames = &quot;item&quot;) %&gt;% mutate(type = c(rep(&quot;word&quot;, 8), rep(&quot;doc&quot;, 6))) %&gt;% rename(dim1 = V1, dim2 = V2) all_tbl ## # A tibble: 14 × 4 ## item dim1 dim2 type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 romeo -0.905 0.563 word ## 2 juliet -0.717 0.904 word ## 3 happy -0.407 0.541 word ## 4 dagger -1.00 0.742 word ## 5 live -0.603 -0.695 word ## 6 die -1.20 -0.494 word ## 7 free -0.603 -0.695 word ## 8 new-hampshire -0.745 -0.925 word ## 9 d1 -0.711 0.73 doc ## 10 d2 -0.93 1.09 doc ## 11 d3 -1.36 0.402 doc ## 12 d4 -1.38 -1.40 doc ## 13 d5 -0.327 -0.46 doc ## 14 q_doc -1.10 0.124 doc Теперь строим график. Как видно, поисковый запрос оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию. Мы исследовали наш небольшой корпус графически, теперь посчитаем косинусное расстояние. dist_mx &lt;- all_df %&gt;% filter(row_number() %in% c(9:14)) %&gt;% philentropy::distance(method = &quot;cosine&quot;, use.row.names = T) ## Metric: &#39;cosine&#39;; comparing: 6 vectors. dist_mx ## d1 d2 d3 d4 d5 q_doc ## d1 1.00000000 0.99792078 0.8724625 -0.02002992 -0.1796188 0.7736415 ## d2 0.99792078 1.00000000 0.8391517 -0.08442786 -0.2426496 0.7311943 ## d3 0.87246248 0.83915171 1.0000000 0.47110771 0.3240227 0.9846131 ## d4 -0.02002992 -0.08442786 0.4711077 1.00000000 0.9871367 0.6180004 ## d5 -0.17961877 -0.24264957 0.3240227 0.98713669 1.0000000 0.4843579 ## q_doc 0.77364153 0.73119433 0.9846131 0.61800043 0.4843579 1.0000000 14.5 Векторы слов Текст хранится в опрятном формате, если одному наблюдению (термину) соответствует один ряд: library(tidyr) tidy_corpus &lt;- df %&gt;% as_tibble(rownames = &quot;word&quot;) %&gt;% pivot_longer(d1:d5, names_to = &quot;doc&quot;) tidy_corpus ## # A tibble: 40 × 3 ## word doc value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 romeo d1 1 ## 2 romeo d2 0 ## 3 romeo d3 1 ## 4 romeo d4 0 ## 5 romeo d5 0 ## 6 juliet d1 1 ## 7 juliet d2 1 ## 8 juliet d3 0 ## 9 juliet d4 0 ## 10 juliet d5 0 ## # ℹ 30 more rows Чтобы вычислить svd, такой корпус надо преобразовать в широкий формат, произвести все вычисления, а затем “тайдифицировать”. Для подобных операций существует пакет widyr. Функцию pivot_wider() я добавляю лишь для удобства сравнения с тем, что мы получили выше. library(widyr) tidy_u &lt;- tidy_corpus %&gt;% widely_svd(word, doc, value, nv = 2, maxit = 100) %&gt;% mutate(value = round(value, 3)) %&gt;% pivot_wider(names_from = dimension, values_from = value) tidy_u ## # A tibble: 8 × 3 ## word `1` `2` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 romeo -0.396 0.28 ## 2 juliet -0.314 0.45 ## 3 happy -0.178 0.269 ## 4 dagger -0.438 0.369 ## 5 live -0.264 -0.346 ## 6 die -0.524 -0.246 ## 7 free -0.264 -0.346 ## 8 new-hampshire -0.326 -0.46 Сравним с тем, что у нас получилось выше. U_truncated == tidy_u[,2:3] ## 1 2 ## [1,] TRUE TRUE ## [2,] TRUE TRUE ## [3,] TRUE TRUE ## [4,] TRUE TRUE ## [5,] TRUE TRUE ## [6,] TRUE TRUE ## [7,] TRUE TRUE ## [8,] TRUE TRUE Итак, мы получили только матрицу левых сингулярных векторов. Чтобы получить word_emb, используем аргумент weight_d: tidy_word_emb &lt;- tidy_corpus %&gt;% widely_svd(word, doc, value, nv = 2, maxit = 100, weight_d = T) %&gt;% mutate(value = round(value, 3)) tidy_word_emb %&gt;% pivot_wider(names_from = dimension, values_from = value) ## # A tibble: 8 × 3 ## word `1` `2` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 romeo -0.905 0.563 ## 2 juliet -0.718 0.904 ## 3 happy -0.407 0.541 ## 4 dagger -1.00 0.741 ## 5 live -0.603 -0.695 ## 6 die -1.20 -0.495 ## 7 free -0.603 -0.695 ## 8 new-hampshire -0.746 -0.924 Сравнив с word_emb выше, видим, что округление не всегда с точностью до тысячных совпадает, но в остальном все ок. 14.6 Пакеты для работы с LSA 14.6.1 lsa Имея матрицу документ-термин, мы можем быстро построить модель, воспользовавшись функцией lsa() из одноименного пакета. Она требует на входе матрицу документ-термин, Функция вернет список, в котором хранятся элементы под названием tk, dk, sk. Они соответствуют матрице правых сингулярных векторов V, матрице правых сингулярных векторов U, а также сингулярным значениям. library(lsa) lsaSpace &lt;- lsa(dtm_mx, dims = 2) Чтобы построить эмбеддинги документов, следует перемножит диагональную матрицу и \\(V^t\\): diag(lsaSpace$sk) %*% t(lsaSpace$tk) Еще одна полезная функция из пакет lsa перемножает все три матрицы: as.textmatrix(lsaSpace) Внутри у этой функции происходит следующее: t(lsaSpace$dk %*% diag(lsaSpace$sk) %*% t(lsaSpace$tk)) Эту матрицу тоже можно использовать для вычисления косинусного расстояния между терминами или документами. 14.6.2 TextmineR Все те же задачи решает пакет textmineR, но могут быть проблемы с его установкой на MacOS. Как их решить, см. здесь. # для MacOS # remotes::install_github(&quot;coatless-mac/macrtools&quot;) # macrtools::macos_rtools_install() Если справились с установкой: library(textmineR) lsa_model &lt;- FitLsaModel(dtm_mx, k = 2) Функция вернет список; при этом матрица правых сингулярных векторов здесь называются “тета”, а матрица левых сингулярных векторов называются “фи”; см. виньетку). Работать с этими матрицами можно как описано выше. Те же задачи решаются при помощи пакетов text2vec, word2vec, и некоторых других. Ниже мы рассмотрим, как строить эмбеддинги с использованием принципов tidyverse. 14.7 LSA c использованием Tidy Data Загружаем датасет с постами из телеграм-канала Antibarbari, предварительно подготовленными для анализа, и удаляем очень редкие слова. load(&quot;data/antibarbari_words_tidy.Rdata&quot;) text_pruned &lt;- text_tidy_nosw %&gt;% add_count(lemma) %&gt;% filter(n &gt; 4) %&gt;% select(-n) Функция nest() собирает все слова для каждого документа в отдельный тибл. nested_words &lt;- text_pruned %&gt;% nest(words = c(lemma)) head(nested_words) ## # A tibble: 6 × 2 ## doc_id words ## &lt;chr&gt; &lt;list&gt; ## 1 doc1 &lt;tibble [4 × 1]&gt; ## 2 doc2 &lt;tibble [8 × 1]&gt; ## 3 doc3 &lt;tibble [6 × 1]&gt; ## 4 doc4 &lt;tibble [19 × 1]&gt; ## 5 doc5 &lt;tibble [22 × 1]&gt; ## 6 doc6 &lt;tibble [32 × 1]&gt; На небольших датасетах как наш может быть непродуктивно строить эмбеддинги с использованием абсолютной частотности или tf-idf, поэтому я воспользуюсь решением, предложенным в книге Джулии Силги и Эмиля Хвитфельдта (источник). Тексты делятся на “скользящие окна”, а затем для каждого окна считается PMI (мера ассоциации между словами). slide_windows &lt;- function(tbl, window_size) { skipgrams &lt;- slider::slide( tbl, ~.x, .after = window_size - 1, .step = 1, .complete = TRUE ) safe_mutate &lt;- safely(mutate) out &lt;- map2(skipgrams, 1:length(skipgrams), ~ safe_mutate(.x, window_id = .y)) out %&gt;% transpose() %&gt;% pluck(&quot;result&quot;) %&gt;% compact() %&gt;% bind_rows() } Применив эту функцию к нашим данным, получаем: tidy_pmi &lt;- nested_words %&gt;% mutate(words = map(words, slide_windows, 4L)) %&gt;% unnest(words) %&gt;% unite(window_id, doc_id, window_id) %&gt;% pairwise_pmi(lemma, window_id) tidy_pmi %&gt;% arrange(-abs(pmi)) ## # A tibble: 112,592 × 3 ## item1 item2 pmi ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 исследования фгн 6.83 ## 2 фгн исследования 6.83 ## 3 квалификация повышение 6.76 ## 4 повышение квалификация 6.76 ## 5 густой острый 6.43 ## 6 острый густой 6.43 ## 7 карточка набор 6.31 ## 8 набор карточка 6.31 ## 9 косвенный реальность 6.26 ## 10 реальность косвенный 6.26 ## # ℹ 112,582 more rows Этот тиббл передаем фунции widely_svd() для построения сингулярного разложения. Обратите внимание на аргумент weight_d: если задать ему значение FALSE, то вернутся не эмбеддинги, а матрица левых сингулярных векторов. tidy_word_emb &lt;- tidy_pmi %&gt;% widely_svd(item1, item2, pmi, weight_d = T, nv = 50) Исследуем наши эмбеддинги, используя функцию, которая считает косинусное сходство между словами. nearest_neighbors &lt;- function(df, token) { df %&gt;% widely( ~ { y &lt;- .[rep(token, nrow(.)), ] res &lt;- rowSums(. * y) / (sqrt(rowSums(. ^ 2)) * sqrt(sum(.[token, ] ^ 2))) matrix(res, ncol = 1, dimnames = list(x = names(res))) }, sort = TRUE )(item1, dimension, value) %&gt;% select(-item2) } Результат, кажется, вполне осмысленный. tidy_word_emb %&gt;% nearest_neighbors(&quot;математика&quot;) ## # A tibble: 1,810 × 2 ## item1 value ## &lt;chr&gt; &lt;dbl&gt; ## 1 математика 1 ## 2 теорема 0.710 ## 3 несоизмеримость 0.703 ## 4 математик 0.652 ## 5 феодор 0.650 ## 6 доказывать 0.649 ## 7 пересказ 0.623 ## 8 представитель 0.601 ## 9 чертеж 0.599 ## 10 историк 0.578 ## # ℹ 1,800 more rows Для большей наглядности некоторые из главных компонент (или топиков) можно представить на графике. tidy_word_emb %&gt;% filter(dimension &lt; 10) %&gt;% group_by(dimension) %&gt;% top_n(12, abs(value)) %&gt;% ungroup() %&gt;% mutate(item1 = reorder_within(item1, value, dimension)) %&gt;% ggplot(aes(item1, value, fill = as.factor(dimension))) + geom_col(alpha=0.8, show.legend = F) + facet_wrap(~dimension, scales = &quot;free_y&quot;, ncol=3) + scale_x_reordered() + coord_flip() Имея эмбеддинги слов и матрицу документ-термин, можно создать пространство документов той же размерности. Для этого перемножаем две матрицы. word_mx &lt;- text_pruned %&gt;% count(doc_id, lemma) %&gt;% cast_sparse(doc_id, lemma, n) dim(word_mx) ## [1] 719 1810 embedding_mx &lt;- tidy_word_emb %&gt;% cast_sparse(item1, dimension, value) dim(embedding_mx) ## [1] 1810 50 doc_mx &lt;- word_mx %*% embedding_mx dim(doc_mx) ## [1] 719 50 class(doc_mx) ## [1] &quot;dgCMatrix&quot; ## attr(,&quot;package&quot;) ## [1] &quot;Matrix&quot; Превратим разреженную матрицу обратно в датафрейм и посчитаем косинусное расстояние между рядами. tidy_doc_emb &lt;- as.data.frame(as.matrix(doc_mx)) dist_mx &lt;- tidy_doc_emb %&gt;% philentropy::distance(method = &quot;cosine&quot;, use.row.names = T) ## Metric: &#39;cosine&#39;; comparing: 719 vectors. Небольшой фрагмент этой матрицы. dist_mx[1:10, 1:6] ## doc1 doc10 doc100 doc101 doc102 doc103 ## doc1 1.0000000 0.7605686 0.7745078 0.7564402 0.8680997 0.6893900 ## doc10 0.7605686 1.0000000 0.7793400 0.7294682 0.8687391 0.7985651 ## doc100 0.7745078 0.7793400 1.0000000 0.8741178 0.8483274 0.7927676 ## doc101 0.7564402 0.7294682 0.8741178 1.0000000 0.8428310 0.7821788 ## doc102 0.8680997 0.8687391 0.8483274 0.8428310 1.0000000 0.7901311 ## doc103 0.6893900 0.7985651 0.7927676 0.7821788 0.7901311 1.0000000 ## doc104 0.6680614 0.7093569 0.8846958 0.8617179 0.8061694 0.8180705 ## doc105 0.7110572 0.7250496 0.8507740 0.8965854 0.8352048 0.8029635 ## doc106 0.4115312 0.4874521 0.6176151 0.6926529 0.6055048 0.5170057 ## doc107 0.3091917 0.3801341 0.5143291 0.5547815 0.4564905 0.5198729 Теперь нам нужна функция для поиска ближайших соседей. nearest_doc &lt;- function(dist_mx, doc, n) { idx &lt;- which(rownames(dist_mx) == doc) subset &lt;- dist_mx[idx, 1:ncol(dist_mx), drop = F] ord &lt;- order(subset, decreasing = T) names(subset[, ord])[1:n] } nearest &lt;- nearest_doc(dist_mx, &quot;doc31&quot;, 3) nearest ## [1] &quot;doc31&quot; &quot;doc94&quot; &quot;doc40&quot; Чтобы оценить адекватность этого результата, загрузим датасет с текстом постов (до удаления стоп-слов). Там много всего, распечатаем только нужное. load(&quot;~/R_Workflow/Text_Analysis_2023/data/AntibarbariTidy.Rdata&quot;) print_post &lt;- function(doc) { text_tidy %&gt;% distinct(doc_id, sentence) %&gt;% filter(doc_id == doc) %&gt;% group_by(doc_id) %&gt;% mutate(text = paste0(sentence, collapse = &quot; &quot;)) %&gt;% distinct(doc_id, text) %&gt;% pull(text) } map(nearest, print_post) ## [[1]] ## [1] &quot;Семинар по Филебу. марта г. Искать благо следует в его \\&quot;обители\\&quot;, смешанной жизни, а само смешение должно быть наилучшим. Сократ и Протарх решают для начала включить в смешанную жизнь лишь самые подлинные части удовольствия и разумения —возникает вопрос, что делать не с такими подлинными, но необходимыми частями.&quot; ## ## [[2]] ## [1] &quot;Семинар . Читаем \\&quot;Филеб\\&quot; . Выяснив, что причиной любой благой смеси будут истина, мера и красота, Сократ предлагает сравнить с ними удовольствие и разум. Протарх, не задумываясь, говорит, что и истина, и мера, и красота ближе к уму, чем к удовольствию. При этом он упоминает сильные удовольствия, в первую очередь любовные, хотя еще раньше собеседники договорились, что всем интенсивным наслаждениям нет места в благой жизни. Возможно, как указывают Хакфорт и Делькомминет, дело в том, что удовольствия рассматриваются сами по себе, каковы они по природе (πεφυκός, ), так что умеренные и чистые удовольствия от познания остаются без рассмотрения.&quot; ## ## [[3]] ## [1] &quot;Семинар . Читаем \\&quot;Филеб\\&quot;. Для хорошей жизни знания нужны не только самые чистые, но и необходимые -- да и вообще все безвредные не помешают. С удовольствиями похожая история: Сократ и Протарх добавляют не только наилучшие, но и необходимые. На повестке вопрос, не станет ли жизнь лучше, если добавить вообще все.&quot; "],["кластеризация-и-метод-главных-компонент.html", "Тема 15 Кластеризация и метод главных компонент 15.1 Виды кластерного анализа 15.2 Кластеризация по методу K средних 15.3 K-means в R 15.4 K-means для кластеризации текстов 15.5 Нормализация данных 15.6 Иерархическая кластеризация 15.7 Алгоритм иерархической кластеризации 15.8 Иерархическая кластеризация в R 15.9 Иерархическая кластеризация текстов 15.10 Метод главных компонент 15.11 PCA для визуализации кластеров K-means 15.12 PCA и иерархическая кластеризация 15.13 Многомерное шкалирование 15.14 PCA с FactoMineR", " Тема 15 Кластеризация и метод главных компонент 15.1 Виды кластерного анализа Все методы машинного обучения делятся на методы обучения с учителем и методы обучения без учителя. В первом случае у нас есть некоторое количество признаков X, измеренных у n объектов, и некоторый отклик Y. Задача заключается в предсказании Y по X. Например, мы измерили вес и пушистость у сотни котов известных пород, и хотим предсказать породу других котов, зная их вес и пушистость. Обучение без учителя предназначено для случаев, когда у нас есть только некоторый набор признаков X, но нет значения отклика. Например, есть группа котов, для которых мы измерили вес и пушистость, но мы не знаем, на какие породы они делятся. Кластеризация относится к числу методов для обнаружения неизвестных групп (кластеров) в данных. Точнее, это целый набор методов. Мы рассмотрим два из них: - кластеризация по методу K средних - иерархическая кластеризация В случае с кластеризацией по методу K средних мы пытаемся разбить наблюдения на некоторое заранее заданное число кластеров. Иерархическая кластеризация возвращает результат в виде дерева (дендрограммы), которая позволяет увидеть все возможные кластеры. 15.2 Кластеризация по методу K средних Алгоритм кластеризации: Каждому наблюдению присваивается случайно выбранное число из интервала от 1 до K (число кластеров). Это исходные метки Вычисляется центроид для каждого из кластеров. Центроид k-го класса – вектор из p средних значений признаков, описывающих наблюдения из этого кластера; Каждому наблюдению присваивается метка того кластера, чей центроид находится ближе всего к этому наблюдению (удаленность выражается обычно в виде евклидова расстояния) Повторить шаги 2-3 до тех пор, пока метки классов не станут изменяться. Это дает возможность минимизировать внутрикластерный разброс: хорошей считается такая кластеризация, при которой такой разброс минимален. Когда центроиды двигаются, кластеры приобретают и теряют документы. Внутрикластерный разброс в кластере k – это сумма квадратов евклидовых расстояний между всеми парами наблюдений в этом кластере, разделенная на общее число входящих в него наблюдений. 15.3 K-means в R Рассмотрим это сначала на симулированных, а затем на реальных данных. set.seed(2) x = matrix(rnorm(50 * 2), ncol = 2) x[1:25, 1:2] = x[1:25, 1:2] + 3 x[26:50, 1:2] = x[1:25, 1:2] - 4 km.out &lt;- kmeans(x, centers = 2, nstart = 20) km.out$cluster ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [39] 2 2 2 2 2 2 2 2 2 2 2 2 Наблюдения разделились идеально. Вот так выглядят наши центроиды: library(tidyverse) centers &lt;- km.out$centers %&gt;% as_tibble() ## Warning: The `x` argument of `as_tibble.matrix()` must have unique column ## names if `.name_repair` is omitted as of tibble 2.0.0. ## ℹ Using compatibility `.name_repair`. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. centers ## # A tibble: 2 × 2 ## V1 V2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 3.33 2.92 ## 2 -0.666 -1.08 library(tidyverse) as_tibble(x) %&gt;% ggplot(aes(V1, V2, color = km.out$cluster)) + geom_point(show.legend = F) + geom_point(data = centers, color = &quot;deeppink3&quot;, size = 3, alpha = 0.7) Аргумент nstart позволяет запустить алгоритм функции несколько раз с разными начальными метками кластеров; функциия вернет наилучший результат. 15.4 K-means для кластеризации текстов Я воспользуюсь данными из пакета stylo, где хранятся частотности 3000 наиболее частотных слов для 26 книг 5 авторов. Один из этих авторов – таинственный Роберт Гэлбрейт, как выяснилось – псевдоним Джоан Роулинг. library(stylo) data(&quot;galbraith&quot;) galbraith[1:6, 1:5] ## the and to of a ## coben_breaker 3.592199 1.175108 2.162724 1.375736 2.518600 ## coben_dropshot 3.587836 1.178543 2.122161 1.268598 2.375359 ## coben_fadeaway 3.931392 1.445498 2.200406 1.213044 2.306477 ## coben_falsemove 3.625411 1.613339 2.133533 1.236688 2.400991 ## coben_goneforgood 3.834031 1.816723 2.152941 1.175808 1.961908 ## coben_nosecondchance 4.098293 1.588967 2.271255 1.205863 1.992137 Применим к этим данным функцию kmeans. Этот датасет имеет формат stylo.data, поэтому сначала его трансформируем. Также уменьшим число переменных с 3000 до 200. set.seed(2) galbraith &lt;- as.data.frame.matrix(galbraith)[,1:200] km.out &lt;- kmeans(galbraith, centers = 5, nstart = 20) km.out$cluster ## coben_breaker coben_dropshot coben_fadeaway ## 5 5 5 ## coben_falsemove coben_goneforgood coben_nosecondchance ## 5 1 1 ## coben_tellnoone galbraith_cuckoos lewis_battle ## 1 4 3 ## lewis_caspian lewis_chair lewis_horse ## 3 3 3 ## lewis_lion lewis_nephew lewis_voyage ## 3 3 3 ## rowling_casual rowling_chamber rowling_goblet ## 4 2 2 ## rowling_hallows rowling_order rowling_prince ## 2 2 2 ## rowling_prisoner rowling_stone tolkien_lord1 ## 2 2 3 ## tolkien_lord2 tolkien_lord3 ## 3 3 library(stringr) expected &lt;- str_remove_all(names(km.out$cluster), &quot;_.*&quot;) tibble(expected = expected, predicted = km.out$cluster) %&gt;% group_by(expected) %&gt;% count(predicted) ## # A tibble: 7 × 3 ## # Groups: expected [5] ## expected predicted n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 coben 1 3 ## 2 coben 5 4 ## 3 galbraith 4 1 ## 4 lewis 3 7 ## 5 rowling 2 7 ## 6 rowling 4 1 ## 7 tolkien 3 3 Поскольку у нас многомерные данные, придется использовать метод главных компонент, чтобы изобразить их графически. Об этом методе я скажу ниже. 15.5 Нормализация данных Формула расстояния зависит от способа измерения объектов. Если одни объекты имеют больший разброс значений, чем другие, то при вычислении расстояний будут преобладать элементы с более широкими диапазонами. as_tibble(t(map_df(galbraith, var)), rownames = &quot;word&quot;) ## # A tibble: 200 × 2 ## word V1 ## &lt;chr&gt; &lt;dbl&gt; ## 1 the 0.377 ## 2 and 0.763 ## 3 to 0.0303 ## 4 of 0.227 ## 5 a 0.0654 ## 6 was 0.0290 ## 7 I 0.825 ## 8 in 0.0160 ## 9 he 0.0876 ## 10 said 0.0631 ## # ℹ 190 more rows Перед применением алгоритма в некоторых случаях рекомендуется изменить масштабы признаков, чтобы каждый из них вносил в формулу расстояния примерно равный вклад. Распространенное преобразование называется стандартизацией по Z-оценке: из значения признака Х вычитается среднее арифметическое, а результат делится на стандартное отклонение Х. \\[ X_{new} = \\frac{X - Mean(X)}{StDev(X)}\\] galbraith_norm &lt;- scale(galbraith) Снова проведем кластеризацию: set.seed(2) km_out_norm &lt;- kmeans(galbraith_norm, centers = 5, nstart = 20) tibble(expected = expected, predicted = km_out_norm$cluster) %&gt;% group_by(expected) %&gt;% count(predicted) ## # A tibble: 6 × 3 ## # Groups: expected [5] ## expected predicted n ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 coben 3 7 ## 2 galbraith 5 1 ## 3 lewis 2 7 ## 4 rowling 1 7 ## 5 rowling 5 1 ## 6 tolkien 4 3 Почти все авторы разошлись по разным кластерам (кроме Роулинг), при этом Гэлбрейт в одном кластере с Роулинг. 15.6 Иерархическая кластеризация Одним из недостатков кластеризации по методу K средних является то, что она требует предварительно указать число кластеров. Этого недостатка лишена иерархическая кластеризация. Если такая кластеризация происходит “снизу вверх”, она называется агломеративной. При этом построение дендрограммы начинается с “листьев” и продолжается вплоть до самого “ствола”. При интерпретации дерева надо иметь в виду, что существует \\(2^{n-1}\\) способов упорядочения ветвей дендрограммы, где n – это число листьев. В каждой из точек слияния можно поменять местами наблюдения, не изменяя смысла дендрограммы. Поэтому выводы о сходстве двух наблюдений нельзя делать на основе из близости по горизонтальной оси. См. Рис. 10.10 из книги “Введение в статистическое обучение”, с. 423). На рисунке видно, что наблюдение 9 похоже на наблюдение 2 не больше, чем оно похоже на наблюдения 8, 5 и 7. Вместо этого выводы делаются, исходя из положения на вертикальной оси той точки, где происходит слияние наблюдений. Количество кластеров определеяется высотой, на которой мы разрезаем дендрограмму. Из этого следует, что одну и ту же дендрограмму можно использовать для получения разного числа кластеров. 15.7 Алгоритм иерархической кластеризации Вычислить меру различия для всех пар наблюдений. На первом шаге все наблюдения рассматриваются как отдельный кластер. Найти пару наиболее похожих кластеров и объединить их. Различие между кластерами соответствует высоте, на которой происходит их слияние в дендрограмме. Повторить шаги 1-2, пока не останется 1 кластер. Вид дерева будет зависеть от того, какой тип присоединения вы выберете. На рисунке ниже представлено три способа: полное, одиночное, среднее. Обычно предпочитают среднее и полное, т.к. они приводят к более сбалансированным дендрограммам. Для функции hclust() в R по умолчанию выставлено значение аргумента method = \"complete\". 15.8 Иерархическая кластеризация в R Применим алгоритм к симулированным данным, которые мы создали выше. Функция dist() по умолчанию считает евклидово расстояние. hc.complete &lt;- hclust(dist(x), method = &quot;complete&quot;) plot(hc.complete) На картинке видно, что наблюдения из верхих и нижних рядов почти идеально разделились на два кластера. 15.9 Иерархическая кластеризация текстов Для вычисления расстояния между текстами лучше подойдет не евклидово, а косинусное расстояние на нормализованных данных. В базовой dist() его нет, поэтому воспользуемся пакетом philentropy. dist_mx &lt;- galbraith_norm %&gt;% philentropy::distance(method = &quot;cosine&quot;, use.row.names = T) ## Metric: &#39;cosine&#39;; comparing: 26 vectors. Преобразуем меру сходства в меру расстояния и передадим на кластеризацию. dist_mx &lt;- as.dist(1 - dist_mx) hc &lt;- hclust(dist_mx) plot(hc) Для получения меток кластеров, возникающих в результате рассечения дендрограммы на той или иной высоте, можно воспользоваться функцией cutree(). cutree(hc, 5) ## coben_breaker coben_dropshot coben_fadeaway ## 1 1 1 ## coben_falsemove coben_goneforgood coben_nosecondchance ## 1 1 1 ## coben_tellnoone galbraith_cuckoos lewis_battle ## 1 2 3 ## lewis_caspian lewis_chair lewis_horse ## 3 3 3 ## lewis_lion lewis_nephew lewis_voyage ## 3 3 3 ## rowling_casual rowling_chamber rowling_goblet ## 2 4 4 ## rowling_hallows rowling_order rowling_prince ## 4 4 4 ## rowling_prisoner rowling_stone tolkien_lord1 ## 4 4 5 ## tolkien_lord2 tolkien_lord3 ## 5 5 Этим меткам можно назначить свой цвет. library(dendextend) hcd &lt;- as.dendrogram(hc) par(mar=c(2,2,2,7)) hcd %&gt;% set(&quot;branches_k_color&quot;, k = 5) %&gt;% set(&quot;labels_col&quot;, k=5) %&gt;% plot(horiz = TRUE) abline(v=0.8, col=&quot;pink4&quot;,lty=2) Результат как иерархической кластеризации, так и кластеризации по методу K средних многомерных данных можно визуализировать в двумерном пространстве, если предварительно применить к данным метод главных компонент. 15.10 Метод главных компонент Метод главных компонент (англ. principal component analysis, PCA) — один из основных способов уменьшить размерность данных, потеряв наименьшее количество информации. Этот метод привлекается, в частности, когда надо визуализировать многомерные данные. Общий принцип хорошо объясняется Гаральдом Баайеном (с. 119). Серый цвет верхнего левого куба означает, что точки распределены равномерно – нужны все три измерения для того, чтобы описать положение точки в кубе. Куб справа сверху по-прежнему имеет три измерения, но нам достаточно только двух, вдоль которых рассеяны данные. Куб слева снизу тоже имеет два измерения, но вдоль оси y разброс данных меньше, чем вдоль x. Наконец, для куба справа снизу достаточно только одного измерения. Метод главных компонент ищет такие измерения, вдоль которых наблюдается наибольший разброс данных, причем каждая следующая компонента будет объяснять меньше разброса. pca_fit &lt;- prcomp(galbraith, scale. = T, center = T) names(pca_fit) ## [1] &quot;sdev&quot; &quot;rotation&quot; &quot;center&quot; &quot;scale&quot; &quot;x&quot; Первый элемент хранит данные о стандартном отклонении, соответствующем каждой компоненте. round(pca_fit$sdev, 3) ## [1] 8.118 6.282 4.816 3.480 3.218 2.660 2.236 2.042 1.975 1.875 1.811 1.681 ## [13] 1.555 1.514 1.478 1.339 1.288 1.266 1.173 1.122 1.104 1.031 0.985 0.900 ## [25] 0.747 0.000 Это можно узнать также, вызвав функцию summary. summary(pca_fit) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 ## Standard deviation 8.1180 6.2824 4.8161 3.47962 3.21811 2.65990 2.2362 ## Proportion of Variance 0.3295 0.1973 0.1160 0.06054 0.05178 0.03538 0.0250 ## Cumulative Proportion 0.3295 0.5269 0.6428 0.70336 0.75514 0.79052 0.8155 ## PC8 PC9 PC10 PC11 PC12 PC13 PC14 ## Standard deviation 2.04233 1.9748 1.87495 1.81070 1.68115 1.55528 1.51356 ## Proportion of Variance 0.02086 0.0195 0.01758 0.01639 0.01413 0.01209 0.01145 ## Cumulative Proportion 0.83638 0.8559 0.87345 0.88985 0.90398 0.91607 0.92753 ## PC15 PC16 PC17 PC18 PC19 PC20 PC21 ## Standard deviation 1.47822 1.33897 1.2882 1.26627 1.17319 1.12184 1.1044 ## Proportion of Variance 0.01093 0.00896 0.0083 0.00802 0.00688 0.00629 0.0061 ## Cumulative Proportion 0.93845 0.94742 0.9557 0.96373 0.97061 0.97690 0.9830 ## PC22 PC23 PC24 PC25 PC26 ## Standard deviation 1.03061 0.98464 0.89953 0.74739 0.000000000000007917 ## Proportion of Variance 0.00531 0.00485 0.00405 0.00279 0.000000000000000000 ## Cumulative Proportion 0.98831 0.99316 0.99721 1.00000 1.000000000000000000 Доля дисперсии – это стандартное отклонение в квадрате, деленное на сумму стандартных отклонений в квадрате. Проверим: round(pca_fit$sdev^2 / sum(pca_fit$sdev^2), 4) ## [1] 0.3295 0.1973 0.1160 0.0605 0.0518 0.0354 0.0250 0.0209 0.0195 0.0176 ## [11] 0.0164 0.0141 0.0121 0.0115 0.0109 0.0090 0.0083 0.0080 0.0069 0.0063 ## [21] 0.0061 0.0053 0.0048 0.0040 0.0028 0.0000 Таким образом, первые две компоненты объясняют почти половину дисперсии, а последняя почти не имеет объяснительной ценности. plot(pca_fit) Координаты текстов в новом двумерном пространстве, определяемом первыми двумя компонентами, хранятся в элементе под названием x. head(pca_fit$x[,1:2]) ## PC1 PC2 ## coben_breaker -10.785778 4.404143 ## coben_dropshot -11.398234 5.447163 ## coben_fadeaway -10.965308 5.078609 ## coben_falsemove -10.713951 5.026340 ## coben_goneforgood -9.801369 6.762709 ## coben_nosecondchance -8.863422 7.718594 Нагрузки каждой компоненты хранятся в матрице ротации. Интерпретировать компоненты как таковые невозможно – они представляют собой линейную комбинацию признаков. В математическом смысле главные компоненты представляют собой собственные векторы ковариационной матрицы исходных данных. head(pca_fit$rotation[, 1:3]) ## PC1 PC2 PC3 ## the 0.11444948 -0.010111871 -0.033517351 ## and 0.11909663 -0.003730939 -0.004220942 ## to -0.02633009 -0.120943864 0.054843337 ## of 0.11247527 -0.049333413 -0.024504809 ## a -0.03283274 0.095953943 0.078496580 ## was -0.05525161 0.026979254 0.087428282 Число рядов в этой матрице равно числу исходных измерений (слов); т.е. для каждого слова указаны его координаты в новом измерении. dim(pca_fit$rotation[, 1:3]) ## [1] 200 3 Изобразить вместе главные компоненты и тексты позволяет функция biplot() (на нормализованных данных получается совсем не читаемо, поэтому демонстрирую на исходных). biplot(prcomp(galbraith), scale=0) 15.11 PCA для визуализации кластеров K-means Функция augment() из пакета broom позволяет соединить результат кластеризации (в данном случае – по методу K средних) с исходными данными. library(broom) pca_fit %&gt;% augment(galbraith) %&gt;% ggplot(aes(.fittedPC1, .fittedPC2, color = expected, shape = as.factor(km.out$cluster))) + geom_point(size = 3, alpha = 0.7) В таком же “опрятном” виде можем представить и нагрузки компонент. Для наглядности вывожу только первые 30 слов. library(cowplot) ## ## Attaching package: &#39;cowplot&#39; ## The following object is masked from &#39;package:lubridate&#39;: ## ## stamp library(grid) ## ## Attaching package: &#39;grid&#39; ## The following object is masked from &#39;package:emoji&#39;: ## ## arrow # как будет выглядеть стрелка arrow_style &lt;- arrow( angle = 20, ends = &quot;first&quot;, type = &quot;closed&quot;, length = grid::unit(8, &quot;pt&quot;) ) pca_fit %&gt;% tidy(matrix = &quot;rotation&quot;) %&gt;% pivot_wider(names_from = &quot;PC&quot;, names_prefix = &quot;PC&quot;, values_from = &quot;value&quot;) %&gt;% filter(row_number() %in% c(1:30)) %&gt;% ggplot(aes(PC1, PC2)) + geom_segment(xend = 0, yend = 0, arrow = arrow_style, color = &quot;darkgrey&quot;) + geom_text( aes(label = column), hjust = 1, nudge_x = -0.02, color = &quot;#904C2F&quot;, alpha = 0.7 ) + theme_minimal_grid(10) + coord_fixed() 15.12 PCA и иерархическая кластеризация Код почти как выше, но надо указать, на сколько кластеров мы разрезаем дерево. pca_fit %&gt;% augment(galbraith) %&gt;% ggplot(aes(.fittedPC1, .fittedPC2, color = expected, shape = as.factor(cutree(hc, 5)))) + geom_point(size = 3, alpha = 0.7) 15.13 Многомерное шкалирование Кроме этого, для визуализации многомерных данных применяют многомерное шкалирование. Функция получает на входе матрицу расстояний. cmd_fit &lt;- cmdscale(dist_mx) %&gt;% as_tibble() head(cmd_fit) ## # A tibble: 6 × 2 ## V1 V2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.651 -0.425 ## 2 -0.596 -0.463 ## 3 -0.622 -0.471 ## 4 -0.605 -0.452 ## 5 -0.490 -0.532 ## 6 -0.396 -0.612 cmd_fit %&gt;% ggplot(aes(V1, V2, color = expected, shape = as.factor(cutree(hc, 5)))) + geom_point(size = 3, alpha = 0.7) Многомерное шкалирование стремится отразить расстояния между наблюдениями. 15.14 PCA с FactoMineR Для вычисления главных компонент в R есть множество инструментов помимо базовых. Для наглядности я пока возьму лишь 10 первых переменных (иначе будет сложно объяснить, что происходит на графике). library(FactoMineR) pca_object &lt;- PCA(galbraith_norm[,1:10], graph = FALSE) pca_object ## **Results for the Principal Component Analysis (PCA)** ## The analysis was performed on 26 individuals, described by 10 variables ## *The results are available in the following objects: ## ## name description ## 1 &quot;$eig&quot; &quot;eigenvalues&quot; ## 2 &quot;$var&quot; &quot;results for the variables&quot; ## 3 &quot;$var$coord&quot; &quot;coord. for the variables&quot; ## 4 &quot;$var$cor&quot; &quot;correlations variables - dimensions&quot; ## 5 &quot;$var$cos2&quot; &quot;cos2 for the variables&quot; ## 6 &quot;$var$contrib&quot; &quot;contributions of the variables&quot; ## 7 &quot;$ind&quot; &quot;results for the individuals&quot; ## 8 &quot;$ind$coord&quot; &quot;coord. for the individuals&quot; ## 9 &quot;$ind$cos2&quot; &quot;cos2 for the individuals&quot; ## 10 &quot;$ind$contrib&quot; &quot;contributions of the individuals&quot; ## 11 &quot;$call&quot; &quot;summary statistics&quot; ## 12 &quot;$call$centre&quot; &quot;mean of the variables&quot; ## 13 &quot;$call$ecart.type&quot; &quot;standard error of the variables&quot; ## 14 &quot;$call$row.w&quot; &quot;weights for the individuals&quot; ## 15 &quot;$call$col.w&quot; &quot;weights for the variables&quot; Первый элемент списка указывает, какие компоненты наиболее важны. head(round(pca_object$eig, 2)) ## eigenvalue percentage of variance cumulative percentage of variance ## comp 1 4.52 45.22 45.22 ## comp 2 2.01 20.10 65.32 ## comp 3 1.25 12.49 77.80 ## comp 4 1.00 10.04 87.84 ## comp 5 0.65 6.51 94.36 ## comp 6 0.25 2.48 96.84 barplot(pca_object$eig[,1], names.arg = paste(&quot;comp &quot;, 1:nrow(pca_object$eig))) При визуализации аргументы shadow и autoLab отвечают за то, чтобы график хорошо читался. plot(pca_object, shadow=T, autoLab = &quot;yes&quot;, choix = &quot;var&quot;, new.plot = F, title = &quot;&quot;, col.var = &quot;grey40&quot;) Получившийся график называется круг корреляции, и его следует понимать так: положительно коррелированные переменные находятся рядом; отрицательно коррелированные переменные находятся в противоположных квадрантах. Например, для первого измерения “was” и “said” коррелированы отрицательно. Это можно проверить, достав соответствующую матрицу из объекта pca (в качестве координат используются коэффициенты корреляции между переменными и компонентами): pca_object$var$coord[,1:2] ## Dim.1 Dim.2 ## the 0.8689963 0.3902929 ## and 0.8742646 0.3402297 ## to 0.1047163 -0.7956915 ## of 0.9616110 0.1330286 ## a -0.4990659 0.2415379 ## was -0.5991994 -0.0562630 ## I -0.6696186 0.4825492 ## in 0.5942257 0.4725278 ## he 0.7394596 -0.4860312 ## said 0.3317503 -0.5806990 Построим графикк с наблюдениями, а не с переменными: plot(pca_object, shadow = T, autoLab = &quot;yes&quot;, title = &quot;&quot;) "],["сетевые-данные-и-графы.html", "Тема 16 Сетевые данные и графы 16.1 Базовое описание 16.2 Создание сетевых данных 16.3 Атрибуты вершин (узлов) 16.4 Фильтрация по узлу 16.5 Атрибуты ребер 16.6 Импорт из gexf 16.7 Визуализация с ggraph 16.8 Фильтрация по атрибутам ребер 16.9 Укладка сети", " Тема 16 Сетевые данные и графы Сети – это все, что окружает нас. Люк (2017) Любые сети состоят из отдельных участников (людей или вещей в сети) и отношений между ними. Сети очень часто визуализируются с помощью графов – структур, состоящих из множества точек и линий, отображающих связи между этими точками. Участники представлены в виде узлов сети, а их отношения представлены в виде линий, их связывающих53. Пример исследования сетей соавторства на ФГН НИУ ВШЭ. Это исследование было предметом оживленной дискуссии, частично опубликованной на сайте ВШЭ. 16.1 Базовое описание Мы начнем работу с сетями на небольшом датасете, опубликованном Якобом Морено в 1930-х гг. Этот датасет содержит сеть дружеских отношений между учениками 4-го класса. library(network) library(sna) # devtools::install_github(&quot;DougLuke/UserNetR&quot;) library(UserNetR) data(&quot;Moreno&quot;) summary(Moreno, print.adj = F) ## Network attributes: ## vertices = 33 ## directed = FALSE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges = 46 ## missing edges = 0 ## non-missing edges = 46 ## density = 0.08712121 ## ## Vertex attributes: ## ## gender: ## numeric valued attribute ## attribute summary: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 1.000 2.000 1.515 2.000 2.000 ## vertex.names: ## character valued attribute ## 33 valid vertex names ## ## No edge attributes plot(Moreno) Самая главная характеристика сети – это ее размер. Размер – это количество участников (members), которые называются узлами (nodes), вершинами (vertices) или акторами. network.size(Moreno) ## [1] 33 Еще одна важная характеристика сети – это ее плотность. gden(Moreno) ## [1] 0.08712121 Плотность – это доля имеющихся связей по отношению к максимально возможному количеству связей. Формула плотности будет отличаться для направленных (\\(\\frac{L}{k(k - 1)}\\)) и ненаправленных (\\(\\frac{2L}{k(k-1)}\\)) сетей (где \\(k(k-1)\\) – это максимально возможное число связей). Компонента сети – это подгруппа, где все акторы связаны между собой прямо или косвенно. На графе выше видно две компоненты. Диаметр сети – количество шагов, которые нужно пройти, чтобы попасть из узла А в узел B; для сетей с несколькими компонентами учитывается та, что больше. Геодезическое расстояние – это кратчайший путь между двумя узлами; диаметром считается максимальное расстояние для двух узлов. lgc &lt;- component.largest(Moreno, result = &quot;graph&quot;) gd &lt;- geodist(lgc) max(gd$gdist) ## [1] 11 Коэффициент кластеризации, или транзитивность, отражает тенденцию к созданию закрытых треугольников, т.е. к замыканию. Транзитивность определяется как доля закрытых треугольников по отношению к общему количеству открытых и закрытых треугольников. gtrans(Moreno, mode=&quot;graph&quot;) ## [1] 0.2857143 16.2 Создание сетевых данных 16.2.1 Социоматрица Матрица, хранящая информацию о сети, называется социоматрицей (или матрицей смежности). Ее можно создать вручную. netmat1 &lt;- rbind(c(0,1,1,0,0), c(0,0,1,1,0), c(0,1,0,0,0), c(0,0,0,0,0), c(0,0,1,0,0)) rownames(netmat1) &lt;- letters[1:5] colnames(netmat1) &lt;- letters[1:5] net1 &lt;- network(netmat1, matrix.type =&quot;adjacency&quot;) class(net1) ## [1] &quot;network&quot; summary(net1) ## Network attributes: ## vertices = 5 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges = 6 ## missing edges = 0 ## non-missing edges = 6 ## density = 0.3 ## ## Vertex attributes: ## vertex.names: ## character valued attribute ## 5 valid vertex names ## ## No edge attributes ## ## Network adjacency matrix: ## a b c d e ## a 0 1 1 0 0 ## b 0 0 1 1 0 ## c 0 1 0 0 0 ## d 0 0 0 0 0 ## e 0 0 1 0 0 Функция gplot из пакета sna позволяет визуализировать эту матрицу в виде графа. gplot(net1, vertex.col = 3, displaylabels = TRUE) 16.2.2 Список ребер Ту же матрицу можно построить при помощи списка ребер. Списки ребер меньше по размеру, и собирать сетевые данные в таком формате проще. netmat2 &lt;- rbind(c(1,2), c(1,3), c(2,3), c(2,4), c(3,2), c(5,3)) net2 &lt;- network(netmat2, matrix.type = &quot;edgelist&quot;) network.vertex.names(net2) &lt;- letters[1:5] summary(net2) ## Network attributes: ## vertices = 5 ## directed = TRUE ## hyper = FALSE ## loops = FALSE ## multiple = FALSE ## bipartite = FALSE ## total edges = 6 ## missing edges = 0 ## non-missing edges = 6 ## density = 0.3 ## ## Vertex attributes: ## vertex.names: ## character valued attribute ## 5 valid vertex names ## ## No edge attributes ## ## Network adjacency matrix: ## a b c d e ## a 0 1 1 0 0 ## b 0 0 1 1 0 ## c 0 1 0 0 0 ## d 0 0 0 0 0 ## e 0 0 1 0 0 Граф будет выглядеть точно так же. gplot(net2, vertex.col = 4, displaylabels = TRUE) 16.2.3 Формула Пакет igraph дает возможность создать сеть разными способами, в том числе с использованием формулы. Перед использованием пакета лучше отвязать statnet. Для ненаправленных графов используется --, для направленных -+. detach(package:sna) detach(package:network) library(igraph) g &lt;- graph.formula(a--c, a--b, b--d, b--c, e--c) g ## IGRAPH b19d862 UN-- 5 5 -- ## + attr: name (v/c) ## + edges from b19d862 (vertex names): ## [1] a--c a--b c--b c--e b--d Объекты igraph можно передать напрямую plot54, но ниже мы рассмотрим и другие возможности. plot(g) 16.2.4 Импорт данных Также данные можно импортировать. Мы воспользуемся датасетом, опубликованном на сайте Пушкинского дома, “Словарь русских писателей XVIII века: сеть персоналий”. Датасет представляет собой осмысленные в терминах сетевого анализа междустатейные ссылки в Словаре русских писателей XVIII века (1988–2010. Вып. 1–3). Узлами сети выступают посвященные персоналиям статьи словаря, а ребрами — ссылки на другие статьи в том же словаре. library(readr) dict_data &lt;- read_csv(file = &quot;files/Persons_EDGES.csv&quot;) dict_data ## # A tibble: 4,440 × 4 ## Source Target Weight Type ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Н.И.Ахвердов П.И.Богданович 1 directed ## 2 А.Д.Байбаков А.А.Барсов 1 directed ## 3 А.Д.Кантемир А.К.Барсов 1 directed ## 4 А.Д.Кантемир С.С.Волчков 1 directed ## 5 А.Д.Кантемир И.И.Ильинский 1 directed ## 6 А.Д.Кантемир Ф.Кролик 1 directed ## 7 А.Д.Кантемир М.В.Ломоносов 1 directed ## 8 А.Д.Кантемир Е.Прокопович 1 directed ## 9 А.Д.Кантемир А.П.Сумароков 1 directed ## 10 А.Д.Кантемир В.К.Тредиаковский 1 directed ## # ℹ 4,430 more rows Эту таблицу можно преобразовать в сеть несколькими способами. Функция graph_from_edgelist() ожидает на входе матрицу с двумя столбцами. dict_graph &lt;- graph_from_edgelist(as.matrix(dict_data[,1:2])) summary(dict_graph, print.adj = F) ## IGRAPH 2e50c7d DN-- 780 4440 -- ## + attr: name (v/c) Описание позволяет понять, что граф является направленным (D), а его узлы имеют имена (N). Всего в графе 780 вершин и 4440 связей. Демонстрационная версия интерактивного приложения, построенного на сетевых данных, размещена здесь. Приложение позволяет работать с отдельными узлами сети, изучать их соседей и количественные характеристики. Мы же выведем лишь небольшую часть узлов. 16.3 Атрибуты вершин (узлов) В том же объекте-сети можно хранить дополнительные данные об узлах. В датасете “Словарь…” в качестве такого атрибута хранятся данные об имени автора: names &lt;- names(V(dict_graph)) names[1:12] ## [1] &quot;Н.И.Ахвердов&quot; &quot;П.И.Богданович&quot; &quot;А.Д.Байбаков&quot; &quot;А.А.Барсов&quot; ## [5] &quot;А.Д.Кантемир&quot; &quot;А.К.Барсов&quot; &quot;С.С.Волчков&quot; &quot;И.И.Ильинский&quot; ## [9] &quot;Ф.Кролик&quot; &quot;М.В.Ломоносов&quot; &quot;Е.Прокопович&quot; &quot;А.П.Сумароков&quot; Атрибуты вершин можно использовать для того, чтобы задать новую подсеть для анализа. Например, выбрать только некоторых авторов из “Словаря…”. vert &lt;- which(names(V(dict_graph)) %in% c(&quot;Д.И.Фонвизин&quot;, &quot;А.Д.Кантемир&quot;, &quot;В.К.Тредиаковский&quot;, &quot;Е.Р.Дашкова&quot;, &quot;Н.М.Карамзин&quot;)) dict_sub &lt;- induced_subgraph(dict_graph, vids = vert) dict_sub ## IGRAPH 7c4d303 DN-- 5 8 -- ## + attr: name (v/c) ## + edges from 7c4d303 (vertex names): ## [1] А.Д.Кантемир -&gt;В.К.Тредиаковский В.К.Тредиаковский-&gt;А.Д.Кантемир ## [3] В.К.Тредиаковский-&gt;Н.М.Карамзин В.К.Тредиаковский-&gt;Д.И.Фонвизин ## [5] Н.М.Карамзин -&gt;В.К.Тредиаковский Д.И.Фонвизин -&gt;В.К.Тредиаковский ## [7] Д.И.Фонвизин -&gt;Н.М.Карамзин Д.И.Фонвизин -&gt;Е.Р.Дашкова 16.4 Фильтрация по узлу Для визуализации таких объектов подходит функция ggraph() из одноименного пакета, которая основана на грамматике ggplot. library(ggraph) names &lt;- names(V(dict_sub)) ggraph(dict_sub, layout = &quot;fr&quot;) + geom_edge_link(color = &quot;cadetblue3&quot;) + geom_node_point(size = 5, color = &quot;bisque4&quot;, shape = 18) + geom_node_text(aes(label = names), nudge_y = -0.1) + theme_graph() Немного баловства (но лучше, конечно, другие картинки использовать). library(ggimage) ## ## Attaching package: &#39;ggimage&#39; ## The following object is masked from &#39;package:cowplot&#39;: ## ## theme_nothing ## получаем координаты subgraph.layout &lt;- layout.fruchterman.reingold(dict_sub) x &lt;- subgraph.layout[,1] y &lt;- subgraph.layout[,2] ggraph(dict_sub, layout = subgraph.layout) + geom_edge_link(color = &quot;cadetblue3&quot;) + geom_pokemon(aes(x, y), image=&quot;pikachu&quot;, size = 0.2) + geom_node_text(aes(label = names), nudge_y = -0.1) + theme_graph() Теперь выберем отдельный узел вместе с его соседями. vert &lt;- which(names(V(dict_graph))==&quot;А.Д.Кантемир&quot;) dict_sub &lt;- induced_subgraph(dict_graph, vids = c(vert, neighbors(dict_graph, &quot;А.Д.Кантемир&quot;))) dict_sub ## IGRAPH ee5a3a3 DN-- 9 30 -- ## + attr: name (v/c) ## + edges from ee5a3a3 (vertex names): ## [1] А.Д.Кантемир -&gt;А.К.Барсов А.Д.Кантемир -&gt;С.С.Волчков ## [3] А.Д.Кантемир -&gt;И.И.Ильинский А.Д.Кантемир -&gt;Ф.Кролик ## [5] А.Д.Кантемир -&gt;М.В.Ломоносов А.Д.Кантемир -&gt;Е.Прокопович ## [7] А.Д.Кантемир -&gt;А.П.Сумароков А.Д.Кантемир -&gt;В.К.Тредиаковский ## [9] А.К.Барсов -&gt;Е.Прокопович С.С.Волчков -&gt;М.В.Ломоносов ## [11] С.С.Волчков -&gt;В.К.Тредиаковский Ф.Кролик -&gt;А.Д.Кантемир ## [13] Ф.Кролик -&gt;Е.Прокопович М.В.Ломоносов-&gt;С.С.Волчков ## [15] М.В.Ломоносов-&gt;А.П.Сумароков М.В.Ломоносов-&gt;В.К.Тредиаковский ## + ... omitted several edges Используем другую укладку, также повернем и сдвинем подписи. names &lt;- names(V(dict_sub)) ggraph(dict_sub, layout = &quot;linear&quot;) + geom_edge_arc(color = &quot;cadetblue3&quot;) + geom_node_point(size = 5, color = &quot;bisque4&quot;, shape = 18) + geom_node_text(aes(label = names), angle = 60, nudge_x = 0.1, nudge_y = -0.1) + theme_graph() 16.5 Атрибуты ребер В данных из “Словаря…” у ребер нет атрибутов, в чем легко убедиться. get.edge.attribute(dict_sub) ## list() В некоторых случаях бывает полезно эти атрибуты назначить: например, если вы провели дополнительное исследование и хотите добавить данные. set.seed(1234) dict_sub &lt;- set_edge_attr(dict_sub, &quot;value&quot;, index = E(dict_sub), value = sample(1:3, length(E(dict_sub)), replace = T)) Назначенный атрибут (сейчас это случайное число) можно закодировать на графе: ggraph(dict_sub, layout = &quot;linear&quot;) + geom_edge_arc(aes(width = value), color = &quot;cadetblue3&quot;, alpha = 0.5) + geom_node_point(size = 5, color = &quot;bisque4&quot;, shape = 18) + geom_node_text(aes(label = names), angle = 60, nudge_x = 0.1, nudge_y = -0.1) + theme_graph() 16.6 Импорт из gexf Датасет “«Камер-фурьерский журнал» В. Ходасевича” хранит информацию о встречах, которые В. Ходасевич фиксировал в своем камер-фурьерском журнале, начиная с 1922 года. Данные за каждый месяц хранятся отдельно в файлах формата gexf (Graph Exchange XML Format)), для их чтения нужен особый пакет. library(rgexf) kfj &lt;- read.gexf(&quot;files/1922_Июль_.gexf&quot;) head(kfj) ## &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; ## &lt;gexf xmlns=&quot;http://www.gexf.net/1.1draft&quot; xmlns:viz=&quot;http://www.gexf.net/1.1draft/viz&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; version=&quot;1.1&quot; xsi:schemaLocation=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;&gt; ## &lt;graph defaultedgetype=&quot;undirected&quot; mode=&quot;static&quot;&gt; ## &lt;nodes&gt; ## &lt;node id=&quot;Туся&quot; label=&quot;Туся&quot;/&gt; ## &lt;node id=&quot;Минский&quot; label=&quot;Минский&quot;/&gt; ## &lt;node id=&quot;Шаляпин&quot; label=&quot;Шаляпин&quot;/&gt; ## &lt;node id=&quot;Женя&quot; label=&quot;Женя&quot;/&gt; ## &lt;node id=&quot;Толстой&quot; label=&quot;Толстой&quot;/&gt; ## &lt;node id=&quot;Эренбург&quot; label=&quot;Эренбург&quot;/&gt; ## &lt;node id=&quot;Нина&quot; label=&quot;Нина&quot;/&gt; ## &lt;node id=&quot;Белый&quot; label=&quot;Белый&quot;/&gt; ## &lt;node id=&quot;Добровейн&quot; label=&quot;Добровейн&quot;/&gt; ## &lt;node id=&quot;Горький&quot; label=&quot;Горький&quot;/&gt; ## &lt;node id=&quot;Феррари&quot; label=&quot;Феррари&quot;/&gt; ## &lt;node id=&quot;Ася&quot; label=&quot;Ася&quot;/&gt; ## &lt;node id=&quot;Парнах&quot; label=&quot;Парнах&quot;/&gt; ## &lt;node id=&quot;Кусиков&quot; label=&quot;Кусиков&quot;/&gt; ## &lt;node id=&quot;Зайцева_В&quot; label=&quot;Зайцева_В&quot;/&gt; ## &lt;node id=&quot;Залшупин&quot; label=&quot;Залшупин&quot;/&gt; ## &lt;node id=&quot;Кречетов&quot; label=&quot;Кречетов&quot;/&gt; ## &lt;node id=&quot;Шкловский&quot; label=&quot;Шкловский&quot;/&gt; ## &lt;node id=&quot;Каплун_Б&quot; label=&quot;Каплун_Б&quot;/&gt; ## &lt;node id=&quot;Берберов_Л&quot; label=&quot;Берберов_Л&quot;/&gt; ## &lt;node id=&quot;Венгерова&quot; label=&quot;Венгерова&quot;/&gt; ## &lt;node id=&quot;Маковский&quot; label=&quot;Маковский&quot;/&gt; ## &lt;node id=&quot;Рындина&quot; label=&quot;Рындина&quot;/&gt; ## &lt;node id=&quot;Багратион-Мухранская&quot; label=&quot;Багратион-Мухранская&quot;/&gt; ## &lt;node id=&quot;Цветаева&quot; label=&quot;Цветаева&quot;/&gt; ## &lt;node id=&quot;Ященко&quot; label=&quot;Ященко&quot;/&gt; ## &lt;node id=&quot;Юренева&quot; label=&quot;Юренева&quot;/&gt; ## &lt;node id=&quot;Дроздов&quot; label=&quot;Дроздов&quot;/&gt; ## &lt;node id=&quot;Горный&quot; label=&quot;Горный&quot;/&gt; ## &lt;node id=&quot;Григорович&quot; label=&quot;Григорович&quot;/&gt; ## &lt;node id=&quot;Берберов&quot; label=&quot;Берберов&quot;/&gt; ## &lt;node id=&quot;Вишняк&quot; label=&quot;Вишняк&quot;/&gt; ## &lt;node id=&quot;Постников&quot; label=&quot;Постников&quot;/&gt; ## &lt;node id=&quot;Пуни&quot; label=&quot;Пуни&quot;/&gt; ## &lt;node id=&quot;Слоним&quot; label=&quot;Слоним&quot;/&gt; ## &lt;node id=&quot;Амфитеатров&quot; label=&quot;Амфитеатров&quot;/&gt; ## &lt;node id=&quot;Зайцев_Б&quot; label=&quot;Зайцев_Б&quot;/&gt; ## &lt;node id=&quot;Пешков_М&quot; label=&quot;Пешков_М&quot;/&gt; ## &lt;node id=&quot;Алянский&quot; label=&quot;Алянский&quot;/&gt; ## &lt;node id=&quot;Андреева&quot; label=&quot;Андреева&quot;/&gt; ## &lt;node id=&quot;Каплун&quot; label=&quot;Каплун&quot;/&gt; ## &lt;node id=&quot;Ракицкий&quot; label=&quot;Ракицкий&quot;/&gt; ## ... ## &lt;/nodes&gt; ## &lt;edges&gt; ## &lt;edge id=&quot;0&quot; source=&quot;Туся&quot; target=&quot;Толстой&quot; weight=&quot;1&quot;/&gt; ## &lt;edge id=&quot;1&quot; source=&quot;Минский&quot; target=&quot;Кусиков&quot; weight=&quot;1&quot;/&gt; ## &lt;edge id=&quot;2&quot; source=&quot;Минский&quot; target=&quot;Юренева&quot; weight=&quot;1&quot;/&gt; ## &lt;edge id=&quot;3&quot; source=&quot;Минский&quot; target=&quot;Эренбург&quot; weight=&quot;1&quot;/&gt; ## &lt;edge id=&quot;4&quot; source=&quot;Минский&quot; target=&quot;Шкловский&quot; weight=&quot;1&quot;/&gt; ## &lt;edge id=&quot;5&quot; source=&quot;Минский&quot; target=&quot;Постников&quot; weight=&quot;1&quot;/&gt; ## ... ## &lt;/edges&gt; ## &lt;/graph&gt; ## &lt;/gexf&gt; Трансформируем в формат igraph. kfj_graph &lt;- gexf.to.igraph(kfj) kfj_graph ## IGRAPH d628c1f UNW- 42 86 -- ## + attr: layout (g/n), name (v/c), color (v/c), size (v/n), weight (e/n) ## + edges from d628c1f (vertex names): ## [1] Толстой --Туся Кусиков --Минский ## [3] Минский --Юренева Минский --Эренбург ## [5] Минский --Шкловский Минский --Постников ## [7] Венгерова --Минский Минский --Пуни ## [9] Багратион-Мухранская--Шаляпин Парнах --Шаляпин ## [11] Горький --Шаляпин Кречетов --Шаляпин ## [13] Пешков_М --Шаляпин Добровейн --Шаляпин ## [15] Ракицкий --Шаляпин Женя --Нина ## + ... omitted several edges В данном случае U означает ненаправленный граф, а N говорит о том, что вершины имеют имена. Буква W указывает на то, что ребрам присвоены веса. Далее приводится информация о количестве вершин (42) и ребер (86). Заглянем в атрибуты. # имена узлов V(kfj_graph) ## + 42/42 vertices, named, from d628c1f: ## [1] Алянский Амфитеатров Андреева ## [4] Ася Багратион-Мухранская Белый ## [7] Берберов Берберов_Л Венгерова ## [10] Вишняк Горный Горький ## [13] Григорович Добровейн Дроздов ## [16] Женя Зайцев_Б Зайцева_В ## [19] Залшупин Каплун Каплун_Б ## [22] Кречетов Кусиков Маковский ## [25] Минский Нина Парнах ## [28] Пешков_М Постников Пуни ## + ... omitted several vertices Сохраним имена узлов как вектор. names &lt;- get.vertex.attribute(kfj_graph)$name Зачем-то в объекте хранится еще и цвет узлов (один для всех). color &lt;- V(kfj_graph)$color color ## [1] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [7] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [13] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [19] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [25] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [31] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; ## [37] &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; &quot;#FF6347FF&quot; Атбрибуты ребер. weight &lt;- get.edge.attribute(kfj_graph)$weight weight ## [1] 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 2 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 ## [39] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 ## [77] 1 1 1 1 1 1 1 1 1 1 16.7 Визуализация с ggraph Ширину ребра отрегулируем в зависимости от веса, который отражает частоту контактов. Мы уже видели, что эти данные хранятся в атрибутах ребер. Функция as.factor препятствует дроблению чисел, т.к. в нашем случае это не имеет смысла. ggraph(kfj_graph) + geom_edge_link(alpha = 0.25, aes(width = as.factor(weight)), show.legend = F) + geom_node_point(color = color, show.legend = F, size = 2) + geom_node_text(aes(label = names), repel = TRUE) + theme_graph() 16.8 Фильтрация по атрибутам ребер Атрибуты ребер, как и атрибуты имен, можно использовать для создания подсети. Например, мы можем отобрать только имена тех людей, с которыми Ходасевич встречался больше одного раза в месяц (это август 1922 г.). E(kfj_graph)[weight &gt; 1] ## + 6/86 edges from d628c1f (vertex names): ## [1] Венгерова--Минский Женя --Нина Женя --Шкловский ## [4] Ася --Нина Зайцев_Б --Зайцева_В Берберов --Берберов_Л Визуализируем подсеть. Цветовая раскраска в данном случае ничего не добавляет, просто показываю красивое. Подписи можно добавить не только к узлам, но и к ребрам. kfj_sub &lt;- subgraph.edges(kfj_graph, E(kfj_graph)[weight &gt; 1]) names_sub &lt;- get.vertex.attribute(kfj_sub)$name weight_sub &lt;- get.edge.attribute(kfj_sub)$weight ggraph(kfj_sub, layout = &quot;kk&quot;) + geom_edge_link(alpha = 0.25, linetype = 2, aes(label = weight_sub), label_colour = &quot;sienna&quot;, label_dodge = unit(2.5, &#39;mm&#39;), angle_calc = &#39;along&#39;) + geom_node_point(size = 3, aes(color = names_sub), show.legend = F) + geom_node_text(aes(label = names_sub), repel = TRUE) + theme_graph() 16.9 Укладка сети Графическое представление одной и той же сети будет зависеть от выбранного способа укладки. При построении графиков сетей стремятся следовать следующим принципам: минимизировать пересечения ребер; максимизировать симметричность укладки узлов; минимизировать изменчивость длины ребер; максимизировать угол между ребрами, когда они пересекают или соединяют узлы; минимизировать общее пространство для вывода сети. Для автоматического построения укладок разработано большое количество методов. В пакете igraph для каждого есть особая функция: layout_randomly layout_in_circle layout_on_sphere layout_with_drl (Distributed Recursive Layout) layout_with_fr (Fruchterman-Reingold) layout_with_kk (Kamada-Kawai) layout_with_lgl (Large Graph Layout) layout_as_tree (Reingold-Tilford) layout_nicely Можно выбрать укладку, не вызывая отдельную функцию: ggraph(kfj_graph, layout = &quot;kk&quot;) + geom_edge_link(alpha = 0.25) + geom_node_point(color = color, show.legend = F, size = 2) + geom_node_text(aes(label = names), repel = TRUE) + theme_graph() Поменяем укладку и цвета: ggraph(kfj_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(color = &quot;sienna&quot;), show.legend = F) + geom_node_point(size = 2, color = &quot;olivedrab&quot;) + geom_node_text(aes(label = names), repel = TRUE, color = &quot;grey30&quot;) + theme_graph() Не все укладки одинаково хороши. ggraph(kfj_graph, layout = &quot;linear&quot;, circular = T) + geom_edge_arc(aes(color = &quot;sienna&quot;), show.legend = F) + geom_node_point(size = 2, color = &quot;olivedrab&quot;) + geom_node_text(aes(label = names), repel = TRUE, color = &quot;grey30&quot;) + theme_graph() Подробнее см.: https://www.data-imaginist.com/2017/ggraph-introduction-edges/ Литература "],["анализ-сетей.html", "Тема 17 Анализ сетей 17.1 От conllu к совместной встречаемости 17.2 Граф слов с igraph 17.3 Важность узлов 17.4 Централизация 17.5 Центральность на графике 17.6 Интерактивный граф 17.7 Точки сочленения 17.8 Подгруппы", " Тема 17 Анализ сетей Сети интересны своими паттернами взаимосвязей и тем, как эти паттерны влияют на участников сети. Люк (2017) В качестве участников сети можно рассматривать не только людей, но и, например, понятия. Пример подобного исследования см. в работе Б.В. Орехова “В сети терминов М. М. Бахтина: теория графов о диалоге, карнавале и хронотопе”. Узлами сети при таком моделировании становятся прежде всего важнейшие для системы автора термины, а сам граф отражает их взаимное функционирование в тексте. В частности, как показало это исследование, “кажущийся конструктивно важным элементом терминологической системы Бахтина хронотоп … на самом деле обладает минимумом связей (в терминах сетевого анализа число связей узла называется степенью) внутри графа”. Мы попробуем провести похожее исследование, но за основу возьмем латинский текст “Исповеди” Августина. 17.1 От conllu к совместной встречаемости Файлы “Исповеди” уже с размеченными частями речи, проверенный вручную, доступен в репозитории на GitHub. Эти файлы я заранее скачала в рабочую директорию курса. files &lt;- list.files(&quot;./files/augustinus/&quot;) files ## [1] &quot;Augustinus_Confessiones_Liber1.tsv.conllu&quot; ## [2] &quot;Augustinus_Confessiones_Liber10.tsv.conllu&quot; ## [3] &quot;Augustinus_Confessiones_Liber11.tsv.conllu&quot; ## [4] &quot;Augustinus_Confessiones_Liber12.tsv.conllu&quot; ## [5] &quot;Augustinus_Confessiones_Liber13.tsv.conllu&quot; ## [6] &quot;Augustinus_Confessiones_Liber2.tsv.conllu&quot; ## [7] &quot;Augustinus_Confessiones_Liber3.tsv.conllu&quot; ## [8] &quot;Augustinus_Confessiones_Liber4.tsv.conllu&quot; ## [9] &quot;Augustinus_Confessiones_Liber5.tsv.conllu&quot; ## [10] &quot;Augustinus_Confessiones_Liber6.tsv.conllu&quot; ## [11] &quot;Augustinus_Confessiones_Liber7.tsv.conllu&quot; ## [12] &quot;Augustinus_Confessiones_Liber8.tsv.conllu&quot; ## [13] &quot;Augustinus_Confessiones_Liber9.tsv.conllu&quot; Сначала их нужно прочитать. library(udpipe) filenames &lt;- paste0(&quot;./files/augustinus/&quot;, files) head(filenames) ## [1] &quot;./files/augustinus/Augustinus_Confessiones_Liber1.tsv.conllu&quot; ## [2] &quot;./files/augustinus/Augustinus_Confessiones_Liber10.tsv.conllu&quot; ## [3] &quot;./files/augustinus/Augustinus_Confessiones_Liber11.tsv.conllu&quot; ## [4] &quot;./files/augustinus/Augustinus_Confessiones_Liber12.tsv.conllu&quot; ## [5] &quot;./files/augustinus/Augustinus_Confessiones_Liber13.tsv.conllu&quot; ## [6] &quot;./files/augustinus/Augustinus_Confessiones_Liber2.tsv.conllu&quot; library(tidyverse) library(purrr) confessions_ann &lt;- map_df(filenames, udpipe_read_conllu) confessions_ann &lt;- confessions_ann %&gt;% select(-doc_id, -paragraph_id, -xpos, -feats, -head_token_id, -dep_rel, -deps, -misc) head(confessions_ann) ## sentence_id sentence ## 1 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## 2 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## 3 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## 4 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## 5 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## 6 Augustinus_Confessiones_Liber1-1 magnus es , domine , et laudabilis valde . ## token_id token lemma upos ## 1 1 magnus magnus ADJ ## 2 2 es sum AUX ## 3 3 , , PUNCT ## 4 4 domine dominus NOUN ## 5 5 , , PUNCT ## 6 6 et et CCONJ Теперь можно посчитать совместную встречаемость, например, в рамках одного предложения. Для удобства визуализации я выберу только существительные, которые встречаются чаще 5 раз. confessions_words &lt;- confessions_ann %&gt;% filter(upos == &quot;NOUN&quot;) %&gt;% group_by(lemma) %&gt;% add_count() %&gt;% filter(n &gt; 5) %&gt;% select(-n) confessions_words ## # A tibble: 11,274 × 6 ## # Groups: lemma [431] ## sentence_id sentence token_id token lemma upos ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Augustinus_Confessiones_Liber1-1 magnus es , domi… 4 domi… domi… NOUN ## 2 Augustinus_Confessiones_Liber1-2 magna virtus tua… 2 virt… uirt… NOUN ## 3 Augustinus_Confessiones_Liber1-2 magna virtus tua… 5 sapi… sapi… NOUN ## 4 Augustinus_Confessiones_Liber1-2 magna virtus tua… 9 nume… nume… NOUN ## 5 Augustinus_Confessiones_Liber1-3 et laudare te vu… 5 homo homo NOUN ## 6 Augustinus_Confessiones_Liber1-3 et laudare te vu… 9 crea… crea… NOUN ## 7 Augustinus_Confessiones_Liber1-3 et laudare te vu… 13 homo homo NOUN ## 8 Augustinus_Confessiones_Liber1-3 et laudare te vu… 19 test… test… NOUN ## 9 Augustinus_Confessiones_Liber1-3 et laudare te vu… 20 pecc… pecc… NOUN ## 10 Augustinus_Confessiones_Liber1-3 et laudare te vu… 23 test… test… NOUN ## # ℹ 11,264 more rows Теперь можно составить матрицу совместной встречаемости (и убрать редкие пары). cooc &lt;- cooccurrence(confessions_words, term = &quot;lemma&quot;, group = &quot;sentence_id&quot;) cooc &lt;- cooc %&gt;% as_tibble() %&gt;% filter(cooc &gt; 10) %&gt;% rename(value = cooc) cooc ## # A tibble: 264 × 3 ## term1 term2 value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 caelum terra 274 ## 2 animus memoria 55 ## 3 dies nox 48 ## 4 caelum principium 47 ## 5 dominus homo 45 ## 6 spatium tempus 44 ## 7 nomen terra 44 ## 8 anima corpus 43 ## 9 imago memoria 43 ## 10 principium terra 43 ## # ℹ 254 more rows 17.2 Граф слов с igraph При создании графа уточним, что связи носят ненаправленный характер. library(igraph) confessions_graph &lt;- graph_from_data_frame(cooc, directed = F) confessions_graph ## IGRAPH 7320479 UN-- 102 264 -- ## + attr: name (v/c), value (e/n) ## + edges from 7320479 (vertex names): ## [1] caelum --terra animus --memoria dies --nox ## [4] caelum --principium dominus --homo spatium --tempus ## [7] nomen --terra anima --corpus imago --memoria ## [10] principium--terra caelum --nomen corpus --sensus ## [13] materia --terra homo --uita abyssus --terra ## [16] dominus --deus anima --uita caelum --materia ## [19] homo --uerbum caelum --aqua caelum --creatura ## [22] aqua --terra creatura --terra dies --terra ## + ... omitted several edges Попробуем изобразить (хотя для больших графов лучше строить интерактивную модель, в этом уроке наc больше интересует анализ связей, а не их визуализация). library(ggraph) ggraph(confessions_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = value)) + geom_node_point(color= &quot;lightblue&quot;, size = 2) + geom_node_text(aes(label = name, repel=T), vjust = 1, hjust = 1) + theme_void() ## Интерактивный график Очень просто, но не очень смотрибельно. # install.packages(&quot;networkD3&quot;) library(networkD3) simpleNetwork(cooc) # conf_net &lt;- simpleNetwork(cooc) # saveNetwork(conf_net, file=&quot;Conf_net.html&quot;, selfcontained = T) Можно чуть усложнить55; код ниже, а результат смотрим здесь. 17.3 Важность узлов Важность (prominence) участника (актора, вершины, узла) определяется его положением внутри сети. Применительно к ненаправленным сетям говорят о центральности (центральный актор вовлечен в наибольшее количество связей, прямых или косвенных), а применительно к направленным – о престиже. Престижный актор характеризуется большим количеством входящих связей. Центральность по степени (degree centrality) определяется количеством связи: чем больше прямых связей, тем более важным является узел. degrees &lt;- degree(confessions_graph) sort(degrees, decreasing = T)[1:10] ## homo terra caelum corpus dominus tempus uerbum anima uita res ## 40 36 30 24 23 21 18 16 16 14 Центральным понятием по степени для “Исповеди” является “человек”. Это слово имеет максимальное число связей. Центральность по близости (closeness centrality) говорит о том, насколько близко узел расположен к другим узлам сети. Центральность по близости – это величина, обратная сумме расстояний от узла i до всех остальных узлов сети. closeness &lt;- closeness(confessions_graph) sort(closeness, decreasing = T)[1:10] ## cantus sonus homo terra caelum corpus ## 1.000000000 1.000000000 0.005813953 0.005649718 0.005434783 0.005347594 ## tempus dominus uerbum res ## 0.005291005 0.005263158 0.005128205 0.004975124 Если обратиться к интерактивному графу, можно заметить, что cantus и sonus представляют собой отдельную компоненту. Собственно, только они туда и входят. membership &lt;- components(confessions_graph)$membership table(membership) ## membership ## 1 2 ## 100 2 Поэтому уточним, что центральность по близости должна считаться для большей компоненты. confessions_subgraph = induced_subgraph(confessions_graph, which(membership == 1)) closeness &lt;- closeness(confessions_subgraph) sort(closeness, decreasing = T)[1:10] ## homo terra caelum corpus tempus dominus ## 0.005813953 0.005649718 0.005434783 0.005347594 0.005291005 0.005263158 ## uerbum res anima uita ## 0.005128205 0.004975124 0.004926108 0.004901961 И снова на первом месте “человек”. Центральность по посредничеству (betweenness centrality) характеризует, насколько важную роль данный узел играет на пути “между” парами других узлов сети. betweenness &lt;- betweenness(confessions_graph) sort(betweenness, decreasing = T)[1:10] ## homo terra caelum corpus dies tempus dominus uerbum ## 1476.0830 1055.3757 740.4504 619.8684 548.6346 515.2094 439.5217 414.5475 ## spatium memoria ## 325.8159 312.0012 Хороший пример персонажа с высокой степенью посредничества в корпусе русской драмы — второстепенный персонаж Гаврила Пушкин из пьесы «Борис Годунов» А.С. Пушкина. …По сюжету, он является связующим персонажем между приближёнными Бориса и Григорием. При прочтении легко не заметить важность этого персонажа, однако на визуализации сети пьесы хорошо видно, что Гаврила связывает два кластера — персонажей в Москве и в Польше. Источник Существуют и другие меры центральности, такие как центральность по собственному вектору, центральность по информации, и др. 17.4 Централизация Централизация характеризует сеть в целом. Рассмотрим два крайних случая: круговой граф и звездчатый граф. star_g &lt;- graph.formula(a--b, a--c, a--d, a--e) circle_g &lt;- graph.formula(a--b, b--c, c--d, d--e, a--e) par(mfrow = c(1, 2)) plot(circle_g, vertex.color=2) plot(star_g, vertex.color=3) В случае звездчатого графа централизация максимальна, а для отдельных узлов наблюдается разброс центральности. centralization.closeness(star_g) ## $res ## [1] 1.0000000 0.5714286 0.5714286 0.5714286 0.5714286 ## ## $centralization ## [1] 1 ## ## $theoretical_max ## [1] 1.714286 Во втором случае наборот – разброса нет, а для графа в целом централизация минимальна. centralization.closeness(circle_g) ## $res ## [1] 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 ## ## $centralization ## [1] 0 ## ## $theoretical_max ## [1] 1.714286 Расчитаем централизацию для графа со словами из “Исповеди”. names(centralization.closeness(confessions_graph)) ## [1] &quot;res&quot; &quot;centralization&quot; &quot;theoretical_max&quot; centralization.closeness(confessions_graph)$centralization ## [1] 1.240763 17.5 Центральность на графике График сети, который включает в себя информацию о важности конкретных узлов, может быть эффективным инструментом анализа и визуализации. (Изменение формы на 21 нужно, чтобы можно было контур и заливку узла сделать разными цветами). ggraph(confessions_graph, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = value)) + geom_node_point(fill = &quot;lightblue&quot;, color= &quot;grey30&quot;, size = log2(degrees) * 3, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name, repel=T), vjust = 1, hjust = 1) + theme_void() ## Warning in geom_node_text(aes(label = name, repel = T), vjust = 1, hjust = 1): ## Ignoring unknown aesthetics: repel 17.6 Интерактивный граф Создадим интерактивный график (результат по ссылке выше). #install.packages(&quot;visNetwork&quot;) library(visNetwork) data &lt;- toVisNetworkData(confessions_graph) # масштабированная центральность узла data$nodes$value &lt;- log(degrees)*3 confessions_3d &lt;- visNetwork(nodes = data$nodes, edges = data$edges, width = &quot;100%&quot;, height = 800) Настраиваем и сохраняем график. visOptions(confessions_3d, highlightNearest = list(enabled = T, degree = 1, hover = T), nodesIdSelection = T) %&gt;% visPhysics(maxVelocity = 20, stabilization = F) %&gt;% visInteraction(dragNodes = T) %&gt;% visSave(file = &quot;Confessions.html&quot;) 17.7 Точки сочленения Точка сочленения – это узел, при удалении которого увеличивается число компонент связности. Таким образом, они соединяют разные части сети. При их удалении акторы (узлы, вершины) не могут взаимодействовать друг с другом. articulation_points(confessions_graph) ## + 16/102 vertices, named, from 7320479: ## [1] uita uox caro uerbum ueritas tempus sol memoria imago ## [10] homo anima terra animus spatium annus dies Например, oblivio и expectatio связаны с остальным графом только через “память”. 17.8 Подгруппы Многие сети состоят из относительно плотных подгрупп, которые соединены между собой менее крепкими связями. Один из способов взглянуть на подгруппы сети заключается в исследовании социальной сплочености (cohesion). Сплоченные подгруппы - это множество акторов, которые объединены между собой посредством многочисленных, сильных и прямых связей. 17.8.1 Клики Клика – один из самых простых типов сплоченных подгрупп; это максимально полный подграф, т.е. подмножество узлов со всеми возможными связями между ними. Толковые словари понимают слово иначе. КЛИКА, -и; ж. [от франц. clique - шайка, банда] Неодобр. Группа, сообщество людей, стремящихся к достижению каких-л. корыстных, неблаговидных целей. Придворная к. К. финансовых дельцов. Фашистская к. Вопреки своему названию, функция clique.number возвращает размер наибольшей клики: clique.number(confessions_graph) ## [1] 7 cliques(confessions_graph, min=7) ## [[1]] ## + 7/102 vertices, named, from 7320479: ## [1] caelum dominus homo terra tempus res uerbum ## ## [[2]] ## + 7/102 vertices, named, from 7320479: ## [1] caelum dominus corpus homo terra tempus res Или, что то же самое: largest_cliques(confessions_graph) ## [[1]] ## + 7/102 vertices, named, from 7320479: ## [1] uerbum homo terra tempus res dominus caelum ## ## [[2]] ## + 7/102 vertices, named, from 7320479: ## [1] res corpus tempus terra homo dominus caelum Но клика – это очень строгое определение сплоченной группы. Например, чтобы подграф, состоящий из 7 вершин, считался кликой, нужно, чтобы между ними было проведено \\((7 \\cdot 6) / 2 = 21\\) связей. Если хотя бы одно ребро отсутствует, то условие не выполняется. Такие клики просто очень редко встречаются. 17.8.2 K-ядра Популярным определением социальной сплоченности является k-ядро (k-core). Это максимальный подграф, в котором каждая вершина связана минимум с k другими вершинами этого же подграфа. K-ядра имеют множество преимуществ: они вложены друг в друга (каждый участник 4-ядра является также участником 3-ядра и т.д.); они не перекрываются; их легко определить. Выражение 6-ядро читают как “ядро степени 6”. Ядро степени k+1 является подграфом ядра степени k. Любой узел в ядре степени k имеет степень либо k, либо выше. При этом coreness узла определяется по ядру с наибольшей степенью, к которому они принадлежат. Для определения k-ядерной структуры используется функция graph.coreness(): coreness &lt;- graph.coreness(confessions_graph) head(coreness) ## caelum animus dies dominus spatium nomen ## 7 5 4 7 4 4 Посчитаем количество вершин в ядрах. table(coreness) ## coreness ## 1 2 3 4 5 6 7 ## 34 17 14 14 10 2 11 1-ядро содержит 34 вершины, 2-ядро содержит 17 вершин, и т.д. Для лучшей интерпретации k-ядерной структуры мы можем графически изобразить сеть, используя информацию о множестве k-ядер. Для начала добавим информацию о цвете к атрибутам узлов. Сейчас, как можно убедиться, в списке атрибутов хранятся только имена: names(get.vertex.attribute(confessions_graph)) ## [1] &quot;name&quot; V(confessions_graph)$color &lt;- coreness names(get.vertex.attribute(confessions_graph)) ## [1] &quot;name&quot; &quot;color&quot; ggraph(confessions_graph, layout = &quot;fr&quot;) + geom_edge_link(edge_alpha = 0.2) + geom_node_point(aes(fill = as.factor(color)), color= &quot;grey30&quot;, size = log2(degrees) * 4, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() Сейчас граф не очень читается, но видно, что в центре находятся ядра наивысшей степени. В данном случае речь идет о 7-ядре, которому принадлежит 11 узлов. Т.е. из этих 11 узлов каждое связано не менее чем с 7 другими участниками группы. Чтобы глубже исследовать подгруппы, последовательно удаляют k-ядра более низкой степени. Для этого можно воспользоваться функцией induced_subgraph(). confessions3_7 &lt;- induced_subgraph(confessions_graph, vids=which(coreness &gt; 2)) ggraph(confessions3_7, layout = &quot;fr&quot;) + geom_edge_link(edge_alpha = 0.2) + geom_node_point(aes(fill = as.factor(color)), color= &quot;grey30&quot;, size = 4, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() При интерпретации важно помнить, что ядра являются вложенными. Чем выше степень ядра, тем больше узлы связаны между собой. confessions6_7 &lt;- induced_subgraph(confessions_graph, vids=which(coreness &gt; 5)) ggraph(confessions6_7, layout = &quot;fr&quot;) + geom_edge_link(edge_alpha = 0.2) + geom_node_point(aes(fill = as.factor(color)), color= &quot;grey30&quot;, size = 4, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() Литература "],["dracor.html", "Тема 18 Dracor 18.1 О корпусе Dracor 18.2 Начало работы с Dracor 18.3 Сети Dracor 18.4 Модулярность 18.5 Алгоритмы обнаружения сообществ 18.6 Двудольные графы (биграфы)", " Тема 18 Dracor Мы продолжим исследовать возможности сетевого анализа с использованием корпуса Dracor. В дальнейшем я опираюсь на презентацию Ивана Позднякова, который разрабатывал DraCor Shiny app (https://shiny.dracor.org/). 18.1 О корпусе Dracor DraCor — сокращение от drama corpora — это собрание размеченных по стандарту TEI драматических текстов на 11-ти языках. Здесь есть корпуса пьес на французском, немецком, испанском, русском, итальянском, шведском, португальском (только Кальдерон) и английском (только Шекспир), а также совсем небольшие коллекции эльзасских, татарских и башкирских пьес. Два крупных корпуса пьес в составе собрания — немецкий и русский — были собраны и поддерживаются создателями проекта DraCor (часть команды из России — это Центр цифровых гуманитарных исследований Высшей школы экономики). Остальные корпуса были взяты из сторонних проектов, а затем адаптированы для совместимости с функционалом DraCor. (Отсюда). На сайте проекта “Системный Блок” можно прочитать серию материалов о том, как возможности Dracor используются в литературоведении: о “Ревизоре” тыц о плотности сетей в трагедии и комедии тыц о “зоне смерти” в “Гамлете” и др. тыц 18.2 Начало работы с Dracor # remotes::install_github(&quot;Pozdniakov/rdracor&quot;) library(rdracor) library(tidyverse) get_dracor_meta() %&gt;% summary() ## DraCor hosts 16 corpora comprising 3119 plays. ## ## The last updated corpus was German Drama Corpus (2023-08-20 20:07:23). Извлекаем метаданные. meta &lt;- get_dracor_meta() %&gt;% select(name, title, plays) meta ## # A tibble: 16 × 3 ## name title plays ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 fre French Drama Corpus 1560 ## 2 ger German Drama Corpus 642 ## 3 rus Russian Drama Corpus 212 ## 4 cal Calderón Drama Corpus 205 ## 5 ita Italian Drama Corpus 139 ## 6 swe Swedish Drama Corpus 68 ## 7 hun Hungarian Drama Corpus 41 ## 8 greek Greek Drama Corpus 40 ## 9 u Ukrainian Drama Corpus 40 ## 10 gersh German Shakespeare Drama Corpus 38 ## 11 shake Shakespeare Drama Corpus 37 ## 12 rom Roman Drama Corpus 36 ## 13 als Alsatian Drama Corpus 30 ## 14 span Spanish Drama Corpus 25 ## 15 bash Bashkir Drama Corpus 3 ## 16 tat Tatar Drama Corpus 3 meta %&gt;% plot() shake &lt;- get_dracor(&quot;shake&quot;) summary(shake) ## 37 plays in Shakespeare Drama Corpus ## Corpus id: shake, repository: https://github.com/dracor-org/shakedracor ## Description: Derived from the [Folger Shakespeare Library](https://shakespeare.folger.edu/). Enhancements documented in our [README at GitHub](https://github.com/dracor-org/shakedracor). ## Written years (range): 1591–1613 ## No information on premiere years ## Years of the first printing (range): 1597–1774 shake &lt;- as_tibble(shake) shake ## # A tibble: 37 × 55 ## corpus id playName yearNormalized title subtitle firstAuthorName authors ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; &lt;list&gt; ## 1 shake shake0… the-tem… 1611 The … NA Shakespeare, W… &lt;df&gt; ## 2 shake shake0… two-gen… 1591 Two … NA Shakespeare, W… &lt;df&gt; ## 3 shake shake0… the-mer… 1602 The … NA Shakespeare, W… &lt;df&gt; ## 4 shake shake0… measure… 1604 Meas… NA Shakespeare, W… &lt;df&gt; ## 5 shake shake0… the-com… 1594 The … NA Shakespeare, W… &lt;df&gt; ## 6 shake shake0… much-ad… 1600 Much… NA Shakespeare, W… &lt;df&gt; ## 7 shake shake0… love-s-… 1595 Love… NA Shakespeare, W… &lt;df&gt; ## 8 shake shake0… a-midsu… 1595 A Mi… NA Shakespeare, W… &lt;df&gt; ## 9 shake shake0… the-mer… 1600 The … NA Shakespeare, W… &lt;df&gt; ## 10 shake shake0… as-you-… 1599 As Y… NA Shakespeare, W… &lt;df&gt; ## # ℹ 27 more rows ## # ℹ 47 more variables: source &lt;lgl&gt;, sourceUrl &lt;lgl&gt;, writtenYearStart &lt;lgl&gt;, ## # writtenYearFinish &lt;int&gt;, printYearStart &lt;lgl&gt;, printYearFinish &lt;int&gt;, ## # premiereYearStart &lt;lgl&gt;, premiereYearFinish &lt;lgl&gt;, wikidataId &lt;chr&gt;, ## # networkSize &lt;int&gt;, networkdataCsvUrl &lt;chr&gt;, normalizedGenre &lt;lgl&gt;, ## # size &lt;int&gt;, density &lt;dbl&gt;, diameter &lt;int&gt;, averageClustering &lt;dbl&gt;, ## # averagePathLength &lt;dbl&gt;, averageDegree &lt;dbl&gt;, maxDegree &lt;int&gt;, … Тут хранится очень много всего: размер сети, плотность сети и т.д. Вот так, например, выглядят самые длинные пьесы Шекспира: shake %&gt;% arrange(desc(wordCountText)) %&gt;% select(corpus, firstAuthorName, title, wordCountText, yearWrittenMeta) ## # A tibble: 37 × 5 ## corpus firstAuthorName title wordCountText yearWrittenMeta ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 shake Shakespeare, William Hamlet 32539 1602 ## 2 shake Shakespeare, William Richard III 31765 1593 ## 3 shake Shakespeare, William Cymbeline 30141 1610 ## 4 shake Shakespeare, William Coriolanus 29948 1608 ## 5 shake Shakespeare, William Troilus and Cressi… 28502 1602 ## 6 shake Shakespeare, William Henry IV, Part II 28461 1598 ## 7 shake Shakespeare, William Othello 28452 1604 ## 8 shake Shakespeare, William King Lear 28192 1606 ## 9 shake Shakespeare, William Henry V 27982 1599 ## 10 shake Shakespeare, William Henry VI, Part 2 27902 1591 ## # ℹ 27 more rows Распределение мужских и женских персонажей по годам: gender_ratio &lt;- shake %&gt;% select(title, yearWrittenMeta, numOfSpeakers, numOfSpeakersFemale, numOfSpeakersMale) %&gt;% mutate(male = numOfSpeakersMale / numOfSpeakers, female = numOfSpeakersFemale / numOfSpeakers) %&gt;% select(-numOfSpeakers, -numOfSpeakersMale, -numOfSpeakersFemale) %&gt;% arrange(yearWrittenMeta) %&gt;% pivot_longer(cols = c(male, female), names_to = &quot;gender&quot;) %&gt;% mutate(year = as.factor(yearWrittenMeta)) %&gt;% select(-yearWrittenMeta) gender_ratio ## # A tibble: 74 × 4 ## title gender value year ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 Two Gentlemen of Verona male 0.824 1591 ## 2 Two Gentlemen of Verona female 0.176 1591 ## 3 The Taming of the Shrew male 0.5 1591 ## 4 The Taming of the Shrew female 0.0789 1591 ## 5 Henry VI, Part 1 male 0.238 1591 ## 6 Henry VI, Part 1 female 0.0159 1591 ## 7 Henry VI, Part 2 male 0.583 1591 ## 8 Henry VI, Part 2 female 0.0278 1591 ## 9 Henry VI, Part 3 male 0.531 1591 ## 10 Henry VI, Part 3 female 0.0408 1591 ## # ℹ 64 more rows gender_ratio %&gt;% group_by(year, gender) %&gt;% summarise(mean = mean(value)) %&gt;% ggplot(aes(year, mean, fill = gender)) + geom_col(position = &quot;dodge&quot;) ## `summarise()` has grouped output by &#39;year&#39;. You can override using ## the `.groups` argument. 18.3 Сети Dracor Однако самое интересное – это построение и анализ сетей при помощи Dracor. taming &lt;- get_net_cooccur_igraph(corpus = &quot;shake&quot;, play = &quot;the-taming-of-the-shrew&quot;) taming ## IGRAPH e34dd1f UNW- 38 222 -- ## + attr: name (v/c), isGroup (v/l), gender (v/c), numOfScenes (v/n), ## | numOfSpeechActs (v/n), numOfWords (v/n), degree (v/n), weightedDegree ## | (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n), weight ## | (e/n) ## + edges from e34dd1f (vertex names): ## [1] Christopher Sly--characters in the Induction ## [2] Christopher Sly--characters in the Induction ## [3] Christopher Sly--characters in the Induction ## [4] Christopher Sly--characters in the Induction ## [5] Christopher Sly--characters in the Induction ## + ... omitted several edges library(ggraph) ggraph(taming, layout = &quot;fr&quot;) + geom_edge_link(edge_alpha = 0.5) + geom_node_point(fill = &quot;steelblue&quot;, size = 4, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() Для удобства я удалю некоторые узлы. Например, можно удалить групповых персонажей. В “Укрощении” это некие лица из “индукции” Вместе с ними удалю Кристофера Слая, который появляется только в “индукции”. library(igraph) groups &lt;- get.vertex.attribute(taming)$isGroup groups ## [1] FALSE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE ## [13] TRUE TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [37] FALSE FALSE taming_sub &lt;- induced_subgraph(taming, vids = -c(1:15)) ggraph(taming_sub, layout = &quot;fr&quot;) + geom_edge_link(edge_alpha = 0.5) + geom_node_point(aes(fill = gender), size = 4, alpha = 0.5, shape = 21) + geom_node_text(aes(label = name), vjust = 1, hjust = 1) + theme_void() Теперь на примере этих данных исследуем такую характеристику сети, как модулярность. 18.4 Модулярность Модулярность — одна из мер структуры сетей или графов. Мера была разработана для измерения силы разбиения сети на модули (называемые группами, кластерами или сообществами). Сети с высокой модулярностью имеют плотные связи между узлами внутри модулей, но слабые связи между узлами в различных модулях. Модулярность равна доле рёбер от общего числа рёбер, которые попадают в данные группы минус ожидаемая доля рёбер, которые попали бы в те же группы, если бы они были распределены случайно. Если все узлы принадлежат к одному классу, то модулярность равна нулю. Если разбиение на классы хорошее, то модулярность должна быть высокая. Снова загружим датасет Moreno, где, как мы видели, ученики 4-го класса делятся на две сети в зависимости от пола. library(UserNetR) library(intergraph) data(Moreno) Moreno_graph &lt;- intergraph::asIgraph(Moreno) plot.igraph(Moreno_graph, vertex.color =1+ V(Moreno_graph)$gender) Метрика модулярности подтверждает, что пол учеников действительно объясняет наблюдаемую кластеризацию. modularity(Moreno_graph, V(Moreno_graph)$gender) ## [1] 0.4761342 В графе “Укрощения…” пол явно не является определяющим (он закодирован цветом на графике выше). Чтобы убедиться в этом сначала перекодируем гендер, так как функция modularity() принимает числовую переменную в качестве аргумента. Значение гендера 3 (unknown) в пьесе имеют групповые персонажи. V(taming_sub)$gender ## [1] &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; &quot;MALE&quot; &quot;FEMALE&quot; ## [8] &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;UNKNOWN&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; ## [15] &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;UNKNOWN&quot; &quot;MALE&quot; &quot;MALE&quot; &quot;MALE&quot; ## [22] &quot;MALE&quot; &quot;FEMALE&quot; ## male = 1 idx &lt;- V(taming_sub)$gender==&quot;MALE&quot; V(taming_sub)$gender[idx] &lt;- 1 ## female = 2 idx &lt;- V(taming_sub)$gender==&quot;FEMALE&quot; V(taming_sub)$gender[idx] &lt;- 2 ## unknown for groups idx &lt;- V(taming_sub)$gender==&quot;UNKNOWN&quot; V(taming_sub)$gender[idx] &lt;- 3 gender &lt;- as.numeric(V(taming_sub)$gender) modularity(taming_sub, gender) ## [1] -0.01708984 При выделении сообществ в большинстве случаев наша задача – максимизировать модулярность. 18.5 Алгоритмы обнаружения сообществ В пакете igraph реализовано множество алгоритмов обнаружения сообществ. Обычной практикой является применение нескольких алгоритмов и сравнение результатов. У нас ненаправленная взвешенная сеть. taming_sub ## IGRAPH 075a82c UNW- 23 128 -- ## + attr: name (v/c), isGroup (v/l), gender (v/c), numOfScenes (v/n), ## | numOfSpeechActs (v/n), numOfWords (v/n), degree (v/n), weightedDegree ## | (v/n), closeness (v/n), betweenness (v/n), eigenvector (v/n), weight ## | (e/n) ## + edges from 075a82c (vertex names): ## [1] Lucentio--Tranio Lucentio--Baptista Minola ## [3] Lucentio--Gremio Lucentio--Katherine ## [5] Lucentio--Hortensio Lucentio--Bianca ## [7] Lucentio--Biondello Lucentio--Petruchio ## [9] Lucentio--Grumio Lucentio--SERVANTS.BAPTISTA.1_Shr ## + ... omitted several edges Применим алгоритм “случайного блуждания”. cw &lt;- cluster_walktrap(taming_sub) membership(cw) ## Lucentio Tranio ## 2 2 ## Baptista Minola Gremio ## 2 2 ## Katherine Hortensio ## 1 2 ## Bianca Biondello ## 2 2 ## Petruchio Grumio ## 1 1 ## SERVANTS.BAPTISTA.1_Shr Curtis ## 2 3 ## Nathaniel Phillip ## 3 3 ## Joseph Nicholas ## 3 3 ## Peter SERVANTS.PETRUCHIO.0.4_Shr ## 3 3 ## Merchant_Shr Haberdasher_Shr ## 2 1 ## Tailor_Shr Vincentio ## 1 2 ## Widow_Shr ## 2 plot(cw, taming_sub) Значение модулярности положительное, но не очень высокое. modularity(cw) ## [1] 0.1489342 Поищем другое разбиение. csg &lt;- cluster_spinglass(taming_sub) membership(csg) ## Lucentio Tranio ## 1 1 ## Baptista Minola Gremio ## 1 1 ## Katherine Hortensio ## 2 1 ## Bianca Biondello ## 1 1 ## Petruchio Grumio ## 2 2 ## SERVANTS.BAPTISTA.1_Shr Curtis ## 1 2 ## Nathaniel Phillip ## 2 2 ## Joseph Nicholas ## 2 2 ## Peter SERVANTS.PETRUCHIO.0.4_Shr ## 2 2 ## Merchant_Shr Haberdasher_Shr ## 1 2 ## Tailor_Shr Vincentio ## 2 1 ## Widow_Shr ## 1 На графе видно, что почти все слуги, портной и галантерейщик оказались в отдельном кластере, что вполне осмысленно. Но есть два кластера с одним элементом (вдова и слуги Баптисты), всего 4. plot(csg, taming_sub) Показатели модулярности чуть выше, чем для предыдущего разбиения. modularity(csg) ## [1] 0.2373678 Также используем алгоритм под названием “главный собственный вектор”. Он выделяет всего две группы, но показатели модулярности ниже, чем для предыдущего. cev &lt;- cluster_leading_eigen(taming_sub) plot(cev, taming_sub) modularity(cev) ## [1] 0.1664325 18.6 Двудольные графы (биграфы) Двудольные графы подходят для ситуаций, когда существует возможность возникновения социальных взаимосвязей, но напрямую их наблюдать нельзя. Тем не менее если участники относятся к одной и той же социальной группе, можно сделать вывод, что существует возможность или потенциал развития связей между ними. Такие сети называются сетями аффилированности (affiliation network). Будет дополнено позже. "],["литература.html", "Литература", " Литература "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
