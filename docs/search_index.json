[["index.html", "Компьютерный анализ текста в R Тема 1 О курсе 1.1 Благодарности", " Компьютерный анализ текста в R Ольга Алиева 2023-07-29 Тема 1 О курсе Этот сайт содержит материалы к курсу “Компьютерный анализ текста в R”. Курс находится в разработке, и материалы постоянно обновляются. Если вы заметили ошибку или опечатку, можно писать на адрес oalieva@hse.ru. 1.1 Благодарности Будут добавлены позже "],["начало-работы-с-r.html", "Тема 2 Начало работы с R 2.1 Что такое R? 2.2 Пакеты и виньетки 2.3 Если не хватает пакетов 2.4 О воспроизводимости 2.5 Что мы (не) будем делать? 2.6 RStudio 2.7 Установка 2.8 Начало работы 2.9 R как калькулятор 2.10 Операторы присваивания 2.11 Векторы 2.12 Списки 2.13 Матрицы 2.14 Таблицы 2.15 Шорткаты", " Тема 2 Начало работы с R 2.1 Что такое R? R — это язык программирования для статистической обработки данных и работы с графикой. Он создан в 90-х гг. на факультете статистики Оклендского университета. Иными словами, его делали статистики и для статистиков. Поэтому он прекрасно подходит для анализа данных, статистических вычислений и машинного обучения, а значит востребован в науке. Язык R — один из самых распространённых в научной среде. Им пользуются математики, биологи, генетики и другие учёные, которым нужно проводить статистические исследования и строить модели. Поэтому язык R нужно изучать тем, кто планирует заниматься научными исследованиями. — Яндекс Практикум Блог После установки R вы получите доступ к уже готовым методам статистического анализа и инструментам для визуализации. Но за счет того, что R распространяется свободно, постоянно появляются новые алгоритмы, созданные внутри экспертного сообщества и тоже доступные для всех. Как и любой язык, R растет и развивается. 2.2 Пакеты и виньетки Если в базовой инсталляции R нет нужного решения – имеет смысл поискать в библиотеке пакетов. Пакет – это набор функций и иногда датасетов, созданный пользователями. На 1 июля 2023 г. в репозитории CRAN доступно 19789 пакетов. И это далеко не все: многие пакеты доступны только на GitHub, например пакет Dracor, к которому я буду обращаться в рамках этого курса. Некоторые функции, которые вы найдете в пакетах, частично дублируют друг друга – это нормально, как и в естественном языке, “сказать” что-то можно разными способами. Несмотря на то, что R создавался изначально для работы со статистикой, система свободно распространяемых модулей значительно расширяет круг задач, которые можно решать на этом языке. Например, благодаря модулю Shiny можно создавать приложения и встраивать их в веб-страницы, а модуль Leaflet позволяет создавать интерактивные карты. Одну из них мы сделали в рамках проекта Antibarbari HSE. В рамках этого курса мы познакомимся и с модулями для машинного обучения. Нейросети в R тоже можно строить, но мы пока не будем. По технической документации и так называемым “виньеткам” можно понять, какой пакет вам нужен. Например, вот так выглядит виньетка пакета RPerseus, при помощи которого можно получить доступ к корпусу греческой и латинской литературы. Бывают еще “пакеты пакетов”, то есть очень большие семейства функций, своего рода “диалекты” R. Таково, например, семейство tidyverse, объединяемое идеологией “опрятных” данных. Про него мы еще будем говорить. 2.3 Если не хватает пакетов Это самое интересное. Если вы работаете в программе с графическим интерфейсом (SPSS, Minitab), то вы вынуждены формулировать свою задачу так, чтобы “вписаться” в набор кнопок, предусмотренных разработчиком. В R, столкнувшись с особой задачей, вы просто пишете под нее особую функцию. Новую функцию не обязательно публиковать в составе пакета – можно сохранить в рабочую директорию (с расширением .R) и наслаждаться самому. По мере того, как развиваются ваши навыки программирования, вы можете ставить и решать все более сложные и интересные задачи. 2.4 О воспроизводимости Когда вы решите опубликовать свое исследование, то и код к нему придется опубликовать (как правило, для этого используется GitHub) – поэтому надо сразу привыкать кодить так, чтобы ваш код был понятен другим. Например, добавлять пояснения при помощи знака # (как в Python) # случайный набор чисел из нормального распределения x &lt;- rnorm(1000) # случайная выборка из этого набора y &lt;- sample(x, 100) В идеале, впрочем, вы поясняете не то, что код делает (при грамотном кодинге это должно быть самоочевидо), а зачем. В этом примере код, правда, настолько простой, что не требует особых пояснений. Но в больших проектах от “читабельности” кода зависит не только то, поймет ли вас потенциальный рецензент, но и сможете ли вы сами вспомнить, какая строчка за что отвечает. Также это позволит вернуться к проекту через некоторое время и быстро вспомнить, что там происходит. Если вы получите интересные результаты и решите их опубликовать, то выложить в открытый доступ придется не только код, но и данные (если они не защищены копирайтом или другими ограничениями). Таким образом рецензент или другие ученые, которые будут читать вашу статью, сможет перепроверить ваши выводы. Ученые так делают! И это еще один довод в пользу того, чтобы научиться программировать, а не полагаться на ПО с графическим интерфейсом. 2.5 Что мы (не) будем делать? Хотя возможности R очень широки, мы будем заниматься в основном анализом текстовых данных. “Текст” в данном случае можно понимать как зафиксированную (в машиночитаемом виде) речь: от отзыва на товар до романа. Но в основном данные я подбираю таким образом, чтобы они были интересны гуманитариям. Мы не будем анализировать звучащую речь (хотя это тоже можно делать в R). И мы не будем заниматься распознаванием рукописных символов, для этого есть гораздо другие мощные инструменты. Веб-скрапинг и нейронные сети тоже не входят в число тем этого курса. Курс включает в себя три основных блока и 24 урока: общее введение в R (темы 1-6) text-mining (темы 7-13) статистика и статистическое обучение (14-22) Еще два урока посвящены модулям Plotly и Leaflet. Если вы плохо представляете, на что вообще способны количественные методы в гуманитаристике, посмотрите видео панельной дискуссии “Цифровые инструменты и методы: в чем их польза и как им обучить гуманитария?” (НИУ ВШЭ, 2023 г.). Это видео о том, зачем. О том, как – дальше. 2.6 RStudio Работать в R мы будем с использованием RStudio, которая представляет собой свободную среду разработки (IDE) программного обеспечения с открытым исходным кодом для языка программирования R. Наша задача в этом уроке – установить R и R Studio и убедиться, что все работает; научиться самостоятельно находить помощь, совершать несложные вычисления. 2.7 Установка Установить R Скачать R для Windows: https://cran.r-project.org/bin/windows/ Скачать R для Mac: https://cran.r-project.org/bin/macosx/ Установить R Studio Скачать: https://www.rstudio.com/products/rstudio/download/ (достаточно бесплатной версии) На MacOS для работы библиотеки Stylo также понадобится установить XQuartz: https://www.xquartz.org/ 2.8 Начало работы После установки и запуска RStudio вы увидите вот такие четыре панели (их названия подписаны на картинке): RStudio Panes По ссылке можно подробнее прочитать, что за что отвечает (и как это поменять). Для начала попробуйте получить информацию о сессии, введя в консоли такую команду: sessionInfo() sessionInfo() – это функция. За названием функции всегда следуют круглые скобки, внутри которых могут находиться аргументы функции. О функциях можно думать как о глаголах (“сделай то-то!”). Аргументы – это что-то вроде дополнений и обстоятельств. (Кстати, в “диалекте” tidyverse есть функции-наречия, так что аналогия законная.) Аргументы могут быть обязательные и необязательные. Чтобы узнать, каких аргументов требует функция, надо вызывать help: ?mean(). Также можно (и нужно) читать техническую документацию к пакетам. Уточнить свою рабочую директорию (в которой R будет искать и сохранять файлы) можно при помощи функции getwd() без аргументов. Установить рабочую директорию можно при помощи функции setwd(), указав в качестве аргумента путь к рабочей директории на вашем компьютере (в кавычках, так как это символьный вектор). В моем случае это выглядит так: setwd(&quot;/Users/olga/R_Workflow/&quot;) Также для выбора рабочей директории можно использовать меню R Session &gt; Set Working Directory. Пакеты для работы устанавливаются один раз, однако подключать их надо во время каждой сессии. Чтобы установить новый пакет, можно воспользоваться меню Tools &gt; Install Packages. Также можно устанавливать пакеты из консоли. Установим пакет для стилометрического анализа: install.packages(&quot;stylo&quot;) Для подключения используем функцию library(), которой передаем в качестве аргумента название пакета без кавычек: library(stylo) Что еще надо знать: как создавать проекты в R и почему это удобно 1 как создавать и хранить файлы с кодом 2.9 R как калькулятор Можно использовать R как калькулятор. Для этого вводим данные рядом с символом приглашения &gt;, который называется prompt. sqrt(4) # квадратный корень ## [1] 2 2^3 # степень ## [1] 8 log10(100) #логарифм ## [1] 2 Если в начале консольной строки стоит +, значит предыдущий код не завершен. Например, вы забыли закрыть скобку функции. Ее можно дописать на следующей строке. Попробуйте набрать sqrt(2 в консоли. 2.10 Операторы присваивания Чтобы в окружении появился новый объект, надо присвоить результат вычислений какой-нибудь переменной при помощи оператора присваивания &lt;- (Alt + - (Windows) или Option + - (Mac)). Знак = также работает как оператор присваивания, но не во всех контекстах, поэтому им лучше не пользоваться. x &lt;- 2 + 2 # создаем переменную y &lt;- 0.1 # создаем еще одну переменную x &lt;- y # переназначаем x + y ## [1] 0.2 Имя переменной, как и имя функции, может содержать прописные и строчные буквы, точку и знак подчеркивания. Функция c() (concatenation) позволяет собрать несколько элементов в единый вектор: x &lt;- c(3, 5, 7) x_mean &lt;- mean(x) # также возможно x.mean или xMean x_mean ## [1] 5 В диалекте tidyverse предпочтение отдается подчеркиванию, а не точке; здесь сказывается влияние синтаксиса Python, где через точку получают доступ к методам объекта. Будьте внимательны: R чувствительна к регистру! Объекты, предназначенные для хранения данных, – это отдельные переменные, векторы, матрицы и массивы, списки, факторы, таблицы данных. Функции – это поименованные программы, предназначенные для создания новых объектов или выполнения определенных действий над ними (С. Мастицкий и В. Шитиков 2015, 24) Чтобы получить список всех объектов в окружении, используется функция ls(). Удалять объекты можно при помощи rm(). Функции можно вкладывать друг в друга: rm(list = ls()) # удаляет все объекты в окружении 2.11 Векторы В языке R нет скаляров (отдельных чисел). Числа считаются векторами из одного элемента. x &lt;- 2 class(x) # числовой вектор ## [1] &quot;numeric&quot; length(x) # длина вектора ## [1] 1 y &lt;- c() # создадим пустой вектор y # при попытке распечатать получаем NULL ## NULL length(y) # длина равна 0 ## [1] 0 NULL означает, что значение не существует; NA (not available) – что оно существует, но неизвестно. Поэтому mean(c(1, NA, 2)) выдаст ошибку, а mean(c(1, NULL, 2)) вернет среднее. В первом случае можно использовать дополнительный аргумент: mean(c(1, NA, 2), na.rm=T). Подробнее см. (Мэтлофф 2019). Основные типы данных, с которыми мы будем работать, следующие: целое число (integer) число с плавающей точкой (numeric, также называются double, то есть число двойной точности) строка (character) логическая переменная (logical) категориальная переменная, или фактор (factor) # проверить тип данных x &lt;- sqrt(2) class(x) ## [1] &quot;numeric&quot; is.integer(x) ## [1] FALSE is.numeric(x) ## [1] TRUE При попытке объединить в единый вектор данные разных типов, они будут принудительно приведены к одному типу: x &lt;- c(TRUE, 1, 3, FALSE) x # логические значения переработаны в числовые ## [1] 1 1 3 0 y &lt;- c(1, &quot;a&quot;, 2, &quot;лукоморье&quot;) # строки всегда в кавычках y # числа превратились в строки ## [1] &quot;1&quot; &quot;a&quot; &quot;2&quot; &quot;лукоморье&quot; Типы векторов в R Логические векторы можно получить в результате применения логических выражений (== “равно”, != “не равно”, &lt;= “меньше или равно”) к данным других типов: x &lt;- c(1:10) # числа от 1 до 10 y &lt;- x &gt; 5 y # значения TRUE соответствуют единице, поэтому их можно складывать ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE sum(y) ## [1] 5 Функции all() и any() также возвращают логические значения: x &lt;- 10:20 any(x == 15) ## [1] TRUE all(x &gt; 9) ## [1] TRUE Существуют различные способы сгенерировать векторы: seq(1, 5, 0.5) ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 rep(&quot;foo&quot;, 5) ## [1] &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; &quot;foo&quot; Векторы можно индексировать, то есть забирать из них какие-то элементы: x &lt;- seq(1, 5, 0.5) x[4:5] # индексы начинаются с 1 (в отличие от Python) ## [1] 2.5 3.0 Над векторами можно совершать арифметические операции, но будьте внимательны, применяя операции к векторам разной длины: в этом случае более короткий вектор будет переработан, то есть повторен до тех пор, пока его длина не сравняется с длиной вектора большей длины. x &lt;- 2; y &lt;- c(10, 20, 30); z &lt;- c(5, 6, 7) y / x ## [1] 5 10 15 x + y ## [1] 12 22 32 y + z ## [1] 15 26 37 Факторы внешне похожи на строки, но в отличие от них хранят информацию об уровнях категориальных переменных. Уровень может обозначаться как числом (например, 1 и 0), так и строкой. t &lt;- factor(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), levels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) t ## [1] A B C ## Levels: A B C 2.12 Списки Списки, или рекурсивные векторы (в отличие от атомарных векторов), могут хранить данные разных типов. list = list(a = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), b = c(1, 2, 3), c = c(T, F, T)) list ## $a ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; ## ## $b ## [1] 1 2 3 ## ## $c ## [1] TRUE FALSE TRUE Можно получить доступ как к элементам списка целиком, так и к их содержимому. list$a # обращение к поименованным элементам ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; list[2] # одинарные квадратные скобки извлекают элемент списка целиком ## $b ## [1] 1 2 3 class(list[2]) ## [1] &quot;list&quot; list[[2]] # элементы второго элемента ## [1] 1 2 3 class(list[[2]]) ## [1] &quot;numeric&quot; list$c[1]# первый элемент второго элемента ## [1] TRUE Обратите внимание, что list[2] и list[[2]] возвращают объекты разных классов. Нам это еще понадобится при работе с XML. Индексирование списка в R Если пройти по ссылке, можно увидеть еще несколько замечательных иллюстраций этой мысли🧂 2.13 Матрицы Матрица – это вектор, который имеет два дополнительных атрибута: количество строк и количество столбцов. Из этого следует, что матрица, как и вектор, может хранить данные одного типа. Проверим. M = matrix(c(1, 2, 3, 4), nrow = 2) M # все ок ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 4 M = matrix(c(1, 2, 3, &quot;a&quot;), nrow = 2) M # все превратилось в строку! ## [,1] [,2] ## [1,] &quot;1&quot; &quot;3&quot; ## [2,] &quot;2&quot; &quot;a&quot; В матрице есть строки и столбцы. Их количество определяет размер (порядок) матрицы. Выше мы создали матрицу 2 x 2. Элементы матрицы, как и элементы вектора, можно извлекать по индексу. Сначала указывается номер строки, потом номер столбца. M = matrix(c(1, 2, 3, 4), nrow = 2) M[1, ] # первая строка полностью ## [1] 1 3 M[,2] # второй столбец полностью ## [1] 3 4 M[1,1] # одно значение ## [1] 1 Обратите внимание, как меняется размерность при индексировании. M = matrix(c(1, 2, 3, 4), nrow = 2) class(M) ## [1] &quot;matrix&quot; &quot;array&quot; dim(M) # функция для извлечения измерений ## [1] 2 2 class(M[1, ]) # первая строка полностью ## [1] &quot;numeric&quot; dim(M[1, ]) ## NULL Попытка узнать измерения вектора возвращает NULL, потому что с точки зрения R векторы не являются матрицами из одного столбца или одной строки, и потому не имеют измерений. С другой стороны, можно создать матрицу, в которой будет одна строка или один столбцец. При выводе они выглядят не так, как обычные векторы. Хотя казалось бы. # вектор-строка C = matrix(c(1, 2, 3), nrow = 1) C ## [,1] [,2] [,3] ## [1,] 1 2 3 # вектор-столбец D = matrix(c(1, 2, 3), nrow = 3) D ## [,1] ## [1,] 1 ## [2,] 2 ## [3,] 3 Над числовыми матрицами в R можно совершать разные операции из линейной алгебры; многие из них нам понадобятся, когда мы будем говорить о латентно-семантическом анализе. Пока лишь несколько полезных функций. # в квадратной матрице есть главная и побочная диагонали M = matrix(c(1, 2, 3, 4), nrow = 2) # ее мы распечатывали выше diag(M) ## [1] 1 4 # если поставить матрицу на бок, то получится транспонированная матрица t(M) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 # матрицу можно умножить на скаляр, то есть на обычное число. M * 3 ## [,1] [,2] ## [1,] 3 9 ## [2,] 6 12 # матрицы одного размера можно складывать M + M ## [,1] [,2] ## [1,] 2 6 ## [2,] 4 8 Матрицы также можно умножать на другие матрицы и на векторы. Но это уже линан, и мы вернемся к этому в другой раз. Пока, если хотите, можете посмотреть видео. Подробнее об элементах линейной алгебры в R см. (А. Буховец и П. Москалев 2015). 2.14 Таблицы Таблицы (кадры данных, data frames) – это двумерные объекты (как и матрицы). Датафреймы отличаются от матриц тем, что их столбцы могут хранить данные разного типа. Если списки являются разнородными аналогами векторов в одном измерении, кадры данных являются разнородными аналогами матриц для двухмерных данных (Мэтлофф 2019, 133). # создание датафрейма df &lt;- data.frame(names = c(&quot;A&quot;, &quot;B&quot;), age = c(10, 11)) df ## names age ## 1 A 10 ## 2 B 11 # извлечение элементов df$names # забирает весь столбец ## [1] &quot;A&quot; &quot;B&quot; df[,&quot;names&quot;] # то же самое, другой способ ## [1] &quot;A&quot; &quot;B&quot; df[1, ] # забирает ряд ## names age ## 1 A 10 Потренируемся на датасете с данными о гапаксах в диалогах Платона. Гапакс – это слово, которое встречается один раз в корпусе или тексте. Этот датасет позволяет перепроверить выводы Льюиса Кэмпбелла, профессора Сент-Эндрюсского университета в Шотландии. Еще 1867 г., впервые применив количественный метод для датировки диалогов Платона, он пришел к выводу, что для “позднего” стиля Платона, среди прочего, характерно обилие редкой лексики (Campbell 1867, xxxi). В корпус подлинных диалогов Кэмпбелл включал 26 текстов, которые делил на три хронологические группы. Свои вычисления он делал вручную, а мы можем попробовать все пересчитать в R. ## dialogue words hapax ratio group ## 1 Apology 8745 36 0.004 1 ## 2 Charmides 8311 31 0.004 1 ## 3 Cratylus 17944 122 0.007 1 ## 4 Critias 4950 104 0.021 3 ## 5 Crito 4169 19 0.005 1 ## 6 Euthydemus 12453 87 0.007 1 Вот так выглядят наши данные. Функция class() позволяет убедиться, что это датафрейм. ## [1] &quot;data.frame&quot; Потренируемся работать с данными в таблицах. # узнать имена столбцов colnames(hapax_plato) ## [1] &quot;dialogue&quot; &quot;words&quot; &quot;hapax&quot; &quot;ratio&quot; &quot;group&quot; # извлечь ряд(ы) по значению hapax_plato[hapax_plato$dialogue == &quot;Parmenides&quot;, ] ## dialogue words hapax ratio group ## 16 Parmenides 15155 20 0.001 2 # узнать тип данных в столбцах str(hapax_plato) ## &#39;data.frame&#39;: 26 obs. of 5 variables: ## $ dialogue: chr &quot;Apology&quot; &quot;Charmides&quot; &quot;Cratylus&quot; &quot;Critias&quot; ... ## $ words : chr &quot;8745&quot; &quot;8311&quot; &quot;17944&quot; &quot;4950&quot; ... ## $ hapax : chr &quot;36&quot; &quot;31&quot; &quot;122&quot; &quot;104&quot; ... ## $ ratio : chr &quot;0.004&quot; &quot;0.004&quot; &quot;0.007&quot; &quot;0.021&quot; ... ## $ group : num 1 1 1 3 1 1 1 1 1 1 ... # отобрать ряды по количеству слов hapax_plato[hapax_plato$words &gt; 10000, ] ## dialogue words hapax ratio group ## 1 Apology 8745 36 0.004 1 ## 2 Charmides 8311 31 0.004 1 ## 3 Cratylus 17944 122 0.007 1 ## 4 Critias 4950 104 0.021 3 ## 5 Crito 4169 19 0.005 1 ## 6 Euthydemus 12453 87 0.007 1 ## 7 Euthyphro 5181 15 0.003 1 ## 8 Gorgias 26337 125 0.005 1 ## 9 HippiasMinor 4360 12 0.003 1 ## 10 Ion 4024 32 0.008 1 ## 11 Laches 7674 27 0.004 1 ## 12 Laws 103193 914 0.009 3 ## 13 Lysis 6980 49 0.007 1 ## 14 Menexenus 4808 43 0.009 1 ## 15 Meno 9791 30 0.003 1 ## 16 Parmenides 15155 20 0.001 2 ## 17 Phaedo 21825 140 0.006 1 ## 18 Phaedrus 16645 228 0.014 2 ## 19 Philebus 17668 64 0.004 3 ## 20 Protagoras 17795 102 0.006 1 ## 21 Republic 88878 668 0.008 2 ## 22 Sophist 16024 107 0.007 3 ## 23 Statesman 16953 180 0.011 3 ## 24 Symposium 17461 127 0.007 1 ## 25 Theaetetus 22489 162 0.007 2 ## 26 Timaeus 23662 370 0.016 3 # преобразовать тип данных в столбцах hapax_plato$group &lt;- as.factor(hapax_plato$group) hapax_plato[,2:4] &lt;- sapply(hapax_plato[,2:4],as.numeric) # подробнее о функции `sapply()` в уроке про итерации И еще с датафреймами полезна функция summary(): summary(hapax_plato) ## dialogue words hapax ratio group ## Length:26 Min. : 4024 Min. : 12.00 Min. :0.001000 1:16 ## Class :character 1st Qu.: 7154 1st Qu.: 31.25 1st Qu.:0.004000 2: 4 ## Mode :character Median : 15590 Median : 94.50 Median :0.007000 3: 6 ## Mean : 19364 Mean :146.69 Mean :0.007154 ## 3rd Qu.: 17907 3rd Qu.:136.75 3rd Qu.:0.008000 ## Max. :103193 Max. :914.00 Max. :0.021000 2.15 Шорткаты Этот раздел я допишу позже. Литература "],["визуализации.html", "Тема 3 Визуализации 3.1 Базовый R 3.2 Lattice 3.3 Ggplot2 3.4 Экспорт графиков из среды R", " Тема 3 Визуализации 3.1 Базовый R В R существуют три основные системы построения графиков, которые могут быть полезны для достижения разных целей. Базовый R – это самая старая система, и в ее основе лежит идея палитры художника2. Идея заключается в том, что у вас есть чистый холст, на который вы добавляете что-то одно за другим: например, сначала вы создаете диаграмму рассеяния с несколькими точками, затем вы добавляете метки, линию регрессии, заголовки и т.п. Каждая деталь графика занимает еще одну строчку кода. Это интуитивно понятная модель, потому что часто в самом начале, исследуя данные, мы часто не знаем, какой график мы хотим построить. Обычно мы начинаем это построение с функции plot(), а затем добавляем функции, которые аннотируют график. Вот простой пример на данных о гапаксах у Платона, которые мы видели раньше. Чтобы построить диаграмму рассеяния (scatter plot), нужно передать функции plot() в качестве аргументов названия тех столбцов, которые мы хотим изобразить по осям x и y. Это можно записать так: plot(x, y). Или так: plot(y ~ x). Знак ~ (тильда) указывает на функцию. attach(hapax_plato) plot(hapax ~ words) Это можно записать и иначе: plot(hapax_plato$hapax ~ hapax_plato$words). Результат будет одинаковый. Теперь беремся за палитру. Данные скучились в левом нижнем углу и потому плохо читаются. Мы можем пожертвовать двумя очень длинными диалогами (это “Государство” и “Законы”) и сделать zoom in, указав вручную границы осей. attach(hapax_plato) plot(hapax ~ words, xlim = c(0, 30000), ylim = c(0, 500)) Но так мы все-таки теряем какую-то информацию – а вдруг она важная? Еще один способ справиться со слипшимися данными – преобразовать их. Применим логарифмическое преобразование. Обратите внимание, как меняются значения на осях. attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;) # добавим текст text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7) Уже гораздо интереснее! Попробуем обозначить цветом и формой пересказанные и прямые диалоги. Форма задается внутри функции plot() при помощи атрибута pch. Числовые значения этого атрибута соответствуют следующим значкам. Мы используем 2, 3 и 5. Значения атрибута pch Перестраиваем наш график. attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], pch = c(2, 3, 5)[group]) text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group]) Некоторые названия перекрываютcя (с этим мы научимся бороться позже), но все равно намного понятнее. Теперь можем поменять шрифт и, например, добавить линию регрессии (не хватает легенды, но что-то уже нет сил). attach(hapax_plato) options(scipen=999) # избавляет от научной нотации plot(words, hapax, log = &quot;xy&quot;, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], pch = c(2, 3, 5)[group], family = &quot;serif&quot;) text(hapax ~ words, labels = dialogue, pos = 2, cex = 0.7, col = c(&quot;darkblue&quot;, &quot;darkgreen&quot;, &quot;darkred&quot;)[group], family = &quot;serif&quot;) # добавим линию регрессии my_lm &lt;- lm(hapax_plato$hapax ~ hapax_plato$words) abline(my_lm, lty = &quot;dashed&quot;, col = &quot;darkgrey&quot;, untf = T) # и заголовок title(main = &quot;Число гапаксов в зависимости от длины диалога&quot;) При помощи графических параметров3 можно контролировать множество настроек. Но в этом и недостаток базовой графики. Не всем хватает терпения и вкуса этим заниматься, поэтому эта система сейчас не очень употребительна. Попробуйте интерпретировать график, который у нас получился. Прав ли был профессор Кэмпбелл, утверждая, что высокая доля гапаксов характерна для “поздних” текстов? Исходите из того, что единственный текст, о котором точно известно, что он поздний – это “Законы”. Судя по графику, количество гапаксов зависит от количества слов в тексте. Чем длиннее текст, тем больше вероятность встретить там редкое слово. 3.2 Lattice Система Lattice (букв. “Решетка”) была разработана специально для анализа многомерных данных (Sarkar 2008). Тут должны быть графики цветочки Например, мы сравниваем точность классификации текстов в зависимости от длины отрывка и количества слов-предикторов. Это уже три переменные (длина – количество слов – точность). Система решеток, или панелей, позволяет представить такие многомерные данные. Многомерно нет слов! В базовом R это тоже можно сделать, изменив графические параметры: par(mfrow = c(1,2)) # вот тут указываем число рядов и столбцов plot(hapax_plato$hapax ~ hapax_plato$words) plot(hapax_plato$ratio ~ hapax_plato$group) Но видно, что пространство при этом расходуется неэффективно. Кроме того, к таким графикам сложно создавать заголовки и подзаголовки, подбирать подписи и т.п. Все эти задачи решает Lattice. Идея этой системы в том, что каждый график строится с помощью одного вызова функции. При этом необходимо сразу указать большое количество информации, чтобы у фунцкии было достаточно данных для построения графика. library(lattice) attach(hapax_plato) # после вертикальной черты указана переменная, которая используется для группировки данных; в нашем случае номер группы (по Кэмпбеллу) xyplot(hapax ~ words | group, data = hapax_plato, scales=list(x=list(log=10))) # трансформация по одной оси Недостаток Lattice, однако, в том, что бывает сложно аннотировать отдельные панели, а также приходится сразу задавать весь график в одном вызове функции. Это не всегда удобно. После создания графика уже ничего нельзя добавить или убавить. 3.3 Ggplot2 Но настоящая графическая сила R – это пакет ggplot2. В его основе лежит идея “грамматики графических элементов” Лиланда Уилкинсона (Мастицкий 2017), и он позволяет объединить достоинства базовой графики R и Lattice. С одной стороны, вы можете постепенно достраивать график, добавляя элемент за элементом; с другой стороны, множество параметров подбираются автоматически, как в Lattice. 3.3.1 Быстрое решение: qplot() Настройки по умолчанию хорошо видно на графике ниже; их легко перенастроить. library(ggplot2) # загружается сразу с tidyverse options(scipen = 999) qplot(words, hapax, data = hapax_plato, log = &quot;xy&quot;) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Функция qplot() – это быстрое решение для задач визуализации. В современных версиях ggplot использование функции qplot() не рекомендуется (deprecated), чтобы побудить пользователей изучать ggplot() как более совершенный инструмент. В данном случае мы построили диаграмму рассеяния, используя логарифмическую трансформацию по двум осям. Можно также выделить цветом различные типы диалогов, изменить размер точек, их прозначность и т.п. qplot(words, hapax, data = hapax_plato, log = &quot;xy&quot;, col = group, size = 1.5) + theme(legend.position = &quot;none&quot;) Диаграмма размаха (о ней подробнее можно посмотреть здесь) удобна в тех случаях, когда необходимо представить обобщенную статистическую информацию о распределении значений количественной переменной в разных группах. attach(hapax_plato) qplot(group, ratio, data = hapax_plato, geom = &quot;boxplot&quot;, color = group) Диаграмму размаха можно совместить с одномерной диаграммой рассеяния. qplot(group, ratio, data = hapax_plato, geom = c(&quot;boxplot&quot;, &quot;jitter&quot;), color = group) # вместо color можно использовать shape, который отвечает за форму элементов 3.3.2 Слой за слоем: ggplot() Для более детальной настройки графика рекомендууется использовать функцию ggplot(), которая имеет два основных аргумента: data и aes (англ. aesthetics); последняя присваивает эстетические атрибуты геометрическим объектам, которые используются на графике. Эти объекты могут слоями накладываться друг на друга (Wickham and Grolemund 2017). Посмотрим, как это работает, на примере, столбиковой диаграммы. Такая диаграмма позволяет представить распределение как количественных, так и качественных переменных. Для примера возьмем датасет diorisis_meta, который хранит данные о древнегреческих текстах, доступных в репозитории Diorisis4. ## # A tibble: 6 × 5 ## name title date genre subgenre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Achilles Tatius Leucippe and Clitophon 120 Narrative Novel ## 2 Aelian De Natura Animalium 230 Technical Natural History ## 3 Aelian Epistulae Rusticae 230 Letters Letters ## 4 Aelian Varia Historia 200 Essays Miscellanea ## 5 Aeneas Tacticus Poliorcetica -350 Technical Military ## 6 Aeschines Against Ctesiphon -330 Oratory Oratory Столбиковая диаграмма позволяет увидеть, тексты каких жанров чаще всего встречаются в этом корпусе. library(tidyverse) diorisis_meta %&gt;% group_by(genre) %&gt;% count() %&gt;% ggplot(aes(reorder(genre, n), n, fill = genre)) + geom_bar(stat = &quot;identity&quot;) + coord_flip() Точечная диаграмма, или dotplot, подходит для тех случаев, когда мы исследуем распределение наблюдений для разных групп данных, причем наблюдений не очень много. Например, мы можем отразить распределение текстов в корпусе по годам. Категориальную переменную (например, жанр) можно дополнительно закодировать цветом (зд. подробнее о том, что можно увидеть на этом графике). diorisis_meta %&gt;% ggplot(aes(date, fill = factor(genre))) + geom_dotplot(binwidth = 10, stackdir = &quot;centerwhole&quot;, binpositions = &quot;all&quot;) + scale_y_continuous(NULL, breaks = NULL) + scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) Различные группы данных можно выделять не только цветом и формой, но и помещать каждую в свое окошко (facet). Попробуем выяснить: сколько поджанров в каждом жанре? diorisis_meta %&gt;% group_by(genre, subgenre) %&gt;% count %&gt;% filter(genre %in% c(&quot;Poetry&quot;, &quot;Technical&quot;)) %&gt;% ggplot(aes(reorder(subgenre, n), n, fill = subgenre)) + geom_col(show.legend = F) + facet_wrap(~genre, scales = &quot;free&quot;) + # вот здесь задаем группы coord_flip() Подробнее с разными видами графиков мы познакомимся дальше, но напоследок о том, как сделать так, чтобы текстовые подписи не наезжали друг на друга. library(ggrepel) hapax_plato %&gt;% ggplot(aes(words, hapax, col = group)) + geom_point(size = 1.2, alpha = 0.7, show.legend = F) + geom_label_repel(label = dialogue) + scale_x_log10() + scale_y_log10() + theme_bw() Ай, красота. 3.4 Экспорт графиков из среды R Способы: реализованные в R драйверы стандартных графических устройств; функция ggsave() меню программы RStudio. # код сохранит pdf в рабочую директорию pdf(file = &quot;Diorisis.pdf&quot;) diorisis_meta %&gt;% group_by(genre, subgenre) %&gt;% count %&gt;% filter(genre %in% c(&quot;Poetry&quot;, &quot;Technical&quot;)) %&gt;% ggplot(aes(reorder(subgenre, n), n, fill = subgenre)) + geom_col(show.legend = F) + facet_wrap(~genre, scales = &quot;free&quot;) + coord_flip() dev.off() # еще один способ сохранить последний график ggsave( filename = &quot;Diorisis.png&quot;, plot = last_plot(), device = &quot;png&quot;, scale = 1, width = NA, height = 500, units = &quot;px&quot;, dpi = 300 ) Литература "],["опрятные-данные.html", "Тема 4 Опрятные данные 4.1 Синтаксис tidyverse 4.2 Опрятные данные 4.3 Пример: буккроссинг", " Тема 4 Опрятные данные Tidy datasets are all alike, but every messy dataset is messy in its own way. — Hadley Wickham 4.1 Синтаксис tidyverse Существуют два основных “диалекта” R, один из которых опирается главным образом на функции и структуры данных базового R, а другой пользуется синтаксисом tidyverse (Winter 2020). Tidyverse – это семейство пакетов (метапакет), разработанных Хадли Уикхемом и др., которое включает в себя в том числе пакеты dplyr, ggplot2 и многие другие. # загрузить все семейство library(tidyverse) 4.1.1 Tibble Основная структура данных в tidyverse – это tibble, современный вариант датафрейма5. Тиббл, как говорят его разработчики, это ленивые и недовольные датафреймы: они делают меньше и жалуются больше6. Это позволяет решать проблемы на более ранних этапах, что, как правило, приводит к созданию более чистого и выразительного кода. Основные отличия от обычного датафрейма: текст по умолчанию конвертируется в строки, а не в факторы;7 усовершенствованный метод print(), не нужно постоянно вызывать head(); нет имен рядов; допускает синтаксически “неправильные” имена столбцов; при индексировании не меняет тип данных на вектор и др. load(&quot;./data/DiorisisMeta.Rdata&quot;) # распечатывает только первые 10 рядов, для каждого столбца указан тип данных, строки пронумерованы as_tibble(diorisis_meta) ## # A tibble: 784 × 5 ## name title date genre subgenre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Achilles Tatius Leucippe and Clitophon 120 Narrative Novel ## 2 Aelian De Natura Animalium 230 Technical Natural History ## 3 Aelian Epistulae Rusticae 230 Letters Letters ## 4 Aelian Varia Historia 200 Essays Miscellanea ## 5 Aeneas Tacticus Poliorcetica -350 Technical Military ## 6 Aeschines Against Ctesiphon -330 Oratory Oratory ## 7 Aeschines Against Timarchus -347 Oratory Oratory ## 8 Aeschines The Speech on the Embassy -336 Oratory Oratory ## 9 Aeschylus Agamemnon -458 Tragedy Tragedy ## 10 Aeschylus Eumenides -458 Tragedy Tragedy ## # ℹ 774 more rows # индексирование head(as.data.frame(diorisis_meta)[, 1]) # возвращает вектор ## [1] &quot;Achilles Tatius&quot; &quot;Aelian&quot; &quot;Aelian&quot; &quot;Aelian&quot; ## [5] &quot;Aeneas Tacticus&quot; &quot;Aeschines&quot; as_tibble(diorisis_meta)[,1] # возвращает тиббл ## # A tibble: 784 × 1 ## name ## &lt;chr&gt; ## 1 Achilles Tatius ## 2 Aelian ## 3 Aelian ## 4 Aelian ## 5 Aeneas Tacticus ## 6 Aeschines ## 7 Aeschines ## 8 Aeschines ## 9 Aeschylus ## 10 Aeschylus ## # ℹ 774 more rows # имена столбцов df &lt;- data.frame(&#39;var 1&#39; = 1:2, two = 3:4) df ## var.1 two ## 1 1 3 ## 2 2 4 tbl &lt;- tibble(&#39;var 1&#39; = 1:2, two = 3:4) tbl ## # A tibble: 2 × 2 ## `var 1` two ## &lt;int&gt; &lt;int&gt; ## 1 1 3 ## 2 2 4 4.1.2 Dplyr Но самое главное, tibble подходит для “грамматики манипуляции данных”, лежащей в основе dplyr8. Эта грамматика предоставляет последовательный набор глаголов, которые помогают решать наиболее распространенные задачи манипулирования данными: mutate() добавляет новые переменные, которые являются функциями существующих переменных; select() выбирает переменные на основе их имен; filter() выбирает наблюдения на основе их значений; summarise() обобщает значения; arrange() изменяет порядок следования строк. Все эти глаголы естественным образом сочетаются с функцией group_by(), которая позволяет выполнять любые операции “по группам”, и с оператором pipe %&gt;% из пакета magrittr. В итоге получается более лаконичный и читаемый код, что можно показать на примере. diorisis_meta %&gt;% select(-subgenre) %&gt;% filter(genre == &quot;Narrative&quot;) %&gt;% # не нужны кавычки! group_by(name) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 20 × 2 ## # Groups: name [20] ## name n ## &lt;chr&gt; &lt;int&gt; ## 1 Plutarch 71 ## 2 Appian 14 ## 3 Flavius Josephus 4 ## 4 Xenophon 4 ## 5 Arrian 3 ## 6 Diodorus Siculus 3 ## 7 Philostratus the Athenian 2 ## 8 Achilles Tatius 1 ## 9 Cassius Dio 1 ## 10 Chariton 1 ## 11 Diogenes Laertius 1 ## 12 Dionysius of Halicarnassus 1 ## 13 Eusebius of Caesarea 1 ## 14 Herodotus 1 ## 15 Longus 1 ## 16 Lucian 1 ## 17 Polybius 1 ## 18 Pseudo Apollodorus 1 ## 19 Thucydides 1 ## 20 Xenophon of Ephesus 1 В базовом R мы бы делали то же самое вот так: diorisis_df &lt;- as.data.frame(diorisis_meta) diorisis_select &lt;- diorisis_df[,-5] # remove column diorisis_filter &lt;- diorisis_select[diorisis_select$genre == &quot;Narrative&quot;, ] diorisis_names &lt;- diorisis_filter$name diorisis_count &lt;- as.data.frame(table(diorisis_names)) diorisis_sort &lt;- diorisis_count[order(diorisis_count$Freq, decreasing =T),] head(diorisis_sort) ## diorisis_names Freq ## 15 Plutarch 71 ## 2 Appian 14 ## 10 Flavius Josephus 4 ## 19 Xenophon 4 ## 3 Arrian 3 ## 6 Diodorus Siculus 3 Тут должен быть какой-то поучительный вывод. 4.2 Опрятные данные Но tidyverse – это не только особый синтаксис, но и отдельная идеология “опрятных данных”. “Сырые” данные, с которыми мы работаем, редко бывают опрятны, и перед анализом их следует “почистить” и преобразовать9. Основные правила опрятных данных: отдельный столбец для каждой переменной; отдельный ряд для каждого наблюдения; у каждого значения отдельная ячейка; один датасет – одна таблица. Принципы опрятных данных Посмотрите на учебные тибблы из пакета tidyr и подумайте, какое из этих правил нарушено в каждом случае. data(&quot;table2&quot;) table2 ## # A tibble: 12 × 4 ## country year type count ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Afghanistan 1999 cases 745 ## 2 Afghanistan 1999 population 19987071 ## 3 Afghanistan 2000 cases 2666 ## 4 Afghanistan 2000 population 20595360 ## 5 Brazil 1999 cases 37737 ## 6 Brazil 1999 population 172006362 ## 7 Brazil 2000 cases 80488 ## 8 Brazil 2000 population 174504898 ## 9 China 1999 cases 212258 ## 10 China 1999 population 1272915272 ## 11 China 2000 cases 213766 ## 12 China 2000 population 1280428583 data(&quot;table3&quot;) table3 ## # A tibble: 6 × 3 ## country year rate ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan 1999 745/19987071 ## 2 Afghanistan 2000 2666/20595360 ## 3 Brazil 1999 37737/172006362 ## 4 Brazil 2000 80488/174504898 ## 5 China 1999 212258/1272915272 ## 6 China 2000 213766/1280428583 data(&quot;table4a&quot;) table4a ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 745 2666 ## 2 Brazil 37737 80488 ## 3 China 212258 213766 data(&quot;table4b&quot;) table4b ## # A tibble: 3 × 3 ## country `1999` `2000` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 19987071 20595360 ## 2 Brazil 172006362 174504898 ## 3 China 1272915272 1280428583 Важные функции для преобразования данных из пакета tidyr:10 separate() делит один столбец на новые; unite() объединяет столбцы; pivot_longer() удлиняет таблицу; pivot_wider() расширяет таблицу; drop_na() и replace_na() указывают, что делать с NA и др. Также упомянем функцию distinct() из dplyr, которая оставляет только уникальные наблюдения и предсталяет собой аналог базовой unique() для таблиц. Кроме того, в dplyr есть полезное семейство функций _join, позволяющих объединять данные в различных таблицах.11 Дальше мы потренируемся с ними работать. 4.3 Пример: буккроссинг 4.3.1 Смотрим на данные Загрузим пример неопрятных данных и попробуем их преобразовать для анализа. Book-Crossing – датасет с рейтингами миллионов книг и обезличенными демографическими данными о более 250 тысячах их читателей. Этот датасет хранится в трех разных таблицах. head(ratings) ## # A tibble: 6 × 3 ## `User-ID` ISBN `Book-Rating` ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 276725 034545104X 0 ## 2 276726 0155061224 5 ## 3 276727 0446520802 0 ## 4 276729 052165615X 3 ## 5 276729 0521795028 6 ## 6 276733 2080674722 0 head(users) ## # A tibble: 6 × 3 ## `User-ID` Location Age ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 nyc, new york, usa NULL ## 2 2 stockton, california, usa 18 ## 3 3 moscow, yukon territory, russia NULL ## 4 4 porto, v.n.gaia, portugal 17 ## 5 5 farnborough, hants, united kingdom NULL ## 6 6 santa monica, california, usa 61 head(books) ## # A tibble: 6 × 8 ## ISBN `Book-Title` `Book-Author` `Year-Of-Publication` Publisher `Image-URL-S` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 0195… Classical M… Mark P. O. M… 2002 Oxford U… http://image… ## 2 0002… Clara Callan Richard Bruc… 2001 HarperFl… http://image… ## 3 0060… Decision in… Carlo D&#39;Este 1991 HarperPe… http://image… ## 4 0374… Flu: The St… Gina Bari Ko… 1999 Farrar S… http://image… ## 5 0393… The Mummies… E. J. W. Bar… 1999 W. W. No… http://image… ## 6 0399… The Kitchen… Amy Tan 1991 Putnam P… http://image… ## # ℹ 2 more variables: `Image-URL-M` &lt;chr&gt;, `Image-URL-L` &lt;chr&gt; Что не так с этими данными? users содержит больше одного значения в столбце Location много отсутствующих значений данные вводятся самими пользователями через сайт https://www.bookcrossing.com/ ; они могут содержать недостоверную информацию, см. напр. moscow, yukon territory, russia (Юкон – это территория Канады). Age представляет собой строку и др. Прежде чем начинать преобразование, надо сформулировать примерный вопрос и понять, что для нас важно, а что нет. Например: - Сколько читателей старше 30 лет пользуются сервисом в Австралии? - В какие года опубликованы самые популярные книги? - Кто популярнее у читателей, Роулинг или Толкин? - Какой процент пользователей никогда не оставляет отзывы? - Есть ли связь между возрастом и количеством оценок? и т.п. Чтобы объединить данные, надо понять, через какие переменные они связаны. Ответ: ratings и books связаны через переменную isbn, ratings и users связаны через переменную User-ID. 4.3.2 Трансформируем данные Начнем с пользователей. users_separated &lt;- users %&gt;% mutate(Age = as.numeric(Age)) %&gt;% filter(!is.na(Age)) %&gt;% # drop_na(Age) тоже решил бы нашу задачу separate(Location, into = c(NA, NA, &quot;country&quot;), sep = &quot;,&quot;) head(users_separated) # можно было бы не сохранять, но так нагляднее ## # A tibble: 6 × 3 ## `User-ID` country Age ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 &quot; usa&quot; 18 ## 2 4 &quot; portugal&quot; 17 ## 3 6 &quot; usa&quot; 61 ## 4 10 &quot; spain&quot; 26 ## 5 11 &quot; australia&quot; 14 ## 6 13 &quot; spain&quot; 26 Здесь можно сразу посмотреть, из каких стран и какого возраста пользователи. users_separated %&gt;% group_by(country) %&gt;% count() %&gt;% arrange(-n) ## # A tibble: 543 × 2 ## # Groups: country [543] ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot; usa&quot; 67138 ## 2 &quot; united kingdom&quot; 10935 ## 3 &quot; canada&quot; 9877 ## 4 &quot; spain&quot; 9505 ## 5 &quot; germany&quot; 8016 ## 6 &quot; australia&quot; 7824 ## 7 &lt;NA&gt; 5914 ## 8 &quot; italy&quot; 4754 ## 9 &quot; france&quot; 2395 ## 10 &quot; portugal&quot; 2175 ## # ℹ 533 more rows Последние ряды этого тибла выглядят достаточно причудливо: users_separated %&gt;% group_by(country) %&gt;% count() %&gt;% arrange(n) ## # A tibble: 543 × 2 ## # Groups: country [543] ## country n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot; pasig city.&quot; 1 ## 2 &quot; &amp;#20013;&amp;#22269;&quot; 1 ## 3 &quot; &amp;#32654;&amp;#22269;&quot; 1 ## 4 &quot; 5057chadwick ct.&quot; 1 ## 5 &quot; 600 083&quot; 1 ## 6 &quot; \\\\n/a\\\\\\&quot;&quot; 1 ## 7 &quot; a new year is ahead&quot; 1 ## 8 &quot; aberdeenshire&quot; 1 ## 9 &quot; agusan del sur&quot; 1 ## 10 &quot; alabama&quot; 1 ## # ℹ 533 more rows Здесь возможно несколько стратегий. Можно выбрать все ряды с названиями реальных стран либо (если это соответствует исследовательской задаче) какую-то одну страну. Можно и проигнорировать, если происхождение пользователей не так важно. Допустим, мы решаем сосредоточиться на Испании. Обратите внимание, что в название страны после разделения функцией separate() попали пробелы, и от них надо избавиться. Это делается при помощи регулярных выражений (о них в другой раз) и функции mutate(). spain_data &lt;- users_separated %&gt;% mutate(country = str_replace_all(country, pattern = &quot;\\\\s+&quot;, &quot;&quot;)) %&gt;% # это означает, что пробел мы меняем на &quot;ничто&quot;, т.е. убираем filter(country == &quot;spain&quot;) %&gt;% group_by(Age) %&gt;% count() %&gt;% arrange(-n) head(spain_data) ## # A tibble: 6 × 2 ## # Groups: Age [6] ## Age n ## &lt;dbl&gt; &lt;int&gt; ## 1 25 514 ## 2 26 510 ## 3 23 480 ## 4 24 467 ## 5 28 459 ## 6 27 450 Столбиковая диаграмма подходит для визуализации подобных данных: spain_data %&gt;% ggplot(aes(Age, n)) + geom_bar(stat = &quot;identity&quot;, col = &quot;blue&quot;, fill = &quot;white&quot;) + theme_bw() Какие целеустремленные испанцы! Читают от 0 до 183 лет 😵 После того, как мы убрали лишние пробелы из названий стран, можно фильтровать: spain_id &lt;- users_separated %&gt;% mutate(country = str_replace_all(country, pattern = &quot;\\\\s+&quot;, &quot;&quot;)) %&gt;% filter(country == &quot;spain&quot;) # на этот раз мы не считаем число наблюдений в группе, а забираем все ряды, которые отвечают условию 4.3.3 Объединяем данные Мы уже выяснили, что ratings и users связаны через переменную User-ID, и в ratings хотели бы оставить только те id, которые отвечают заданному условию (страна, возраст и т.п.). Для такого рода объединений как раз подходят функции _join12. Функции семейства _join spain_ratings &lt;- spain_id %&gt;% left_join(ratings) %&gt;% filter(!is.na(ISBN)) %&gt;% filter(`Book-Rating` &gt; 7) %&gt;% # имена синтаксически неправильные, поэтому требуется знак &quot;`&quot; group_by(ISBN) %&gt;% count() %&gt;% arrange(-n) ## Joining with `by = join_by(`User-ID`)` spain_ratings ## # A tibble: 1,281 × 2 ## # Groups: ISBN [1,281] ## ISBN n ## &lt;chr&gt; &lt;int&gt; ## 1 8432206407 4 ## 2 8433969978 4 ## 3 846630679X 4 ## 4 8472236552 4 ## 5 8495501198 4 ## 6 840149186X 3 ## 7 8401499585 3 ## 8 8423310353 3 ## 9 8423662152 3 ## 10 8432215007 3 ## # ℹ 1,271 more rows Осталось выяснить, что это за книги. Для этого объединяем spain_ratings и books. spain_books &lt;- spain_ratings %&gt;% filter(n &gt; 2) %&gt;% left_join(books) %&gt;% filter(!is.na(`Book-Title`), !is.na(`Book-Author`)) %&gt;% ungroup() spain_books ## # A tibble: 15 × 9 ## ISBN n `Book-Title` `Book-Author` `Year-Of-Publication` Publisher ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 8432206407 4 Sin Noticias … Eduardo Mend… 1995 Planeta … ## 2 8433969978 4 El Libro de L… Paul Auster 2003 Anagrama ## 3 846630679X 4 La caverna = … Jose Saramago 2002 Punto de… ## 4 8472236552 4 UN Viejo Que … Luis Sepulve… 1993 Tusquets… ## 5 8495501198 4 Memorias de u… Arthur Golden 2001 Suma de … ## 6 840149186X 3 El Club de Lo… N. H. Kleinb… 1995 Plaza &amp;a… ## 7 8401499585 3 Los Pilares d… Ken Follett 1995 Plaza &amp;a… ## 8 8423310353 3 El Camino (Co… Miguel Delib… 1991 Continen… ## 9 8432215007 3 El perfume Patrick Susk… 1997 Editoria… ## 10 8445071408 3 El Senor De L… J. R. R. Tol… 2001 Minotauro ## 11 8445071416 3 El Hobbit J. R. R. Tol… 1991 Minotauro ## 12 8477204055 3 El caballero … Robert Fisher 2000 Obelisco ## 13 8478884459 3 Harry Potter … J. K. Rowling 1999 Lectorum… ## 14 8484602508 3 Diario de Un … Antonio Salas 2003 Temas de… ## 15 8495501112 3 Son De Mar Manuel Vicent 2002 Suma de … ## # ℹ 3 more variables: `Image-URL-S` &lt;chr&gt;, `Image-URL-M` &lt;chr&gt;, ## # `Image-URL-L` &lt;chr&gt; Как минимум мы выяснили, что испанцы предпочитают читать по-испански! (Здесь снова можно подумать. Возможно, у одной книги разные ISBN, и стоило группировать не по ISBN, а по названию или автору?) Осталось избавиться от неинформативных столбцов (это ссылки, часто битые, на изображения обложки). Если мы знаем номера этих столбцов, то это можно сделать по индексу: spain_books %&gt;% select(3:5) %&gt;% rename(title = `Book-Title`, author = `Book-Author`) ## # A tibble: 15 × 3 ## title author `Year-Of-Publication` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Sin Noticias De Gurb (Biblioteca breve) Eduar… 1995 ## 2 El Libro de Las Ilusiones Paul … 2003 ## 3 La caverna = A caverna Jose … 2002 ## 4 UN Viejo Que Leia Novelas De Amor/the Old Men W… Luis … 1993 ## 5 Memorias de una geisha Arthu… 2001 ## 6 El Club de Los Poetas Muertos N. H.… 1995 ## 7 Los Pilares de La Tierra Ken F… 1995 ## 8 El Camino (Coleccion Destinolibro) Migue… 1991 ## 9 El perfume Patri… 1997 ## 10 El Senor De Los Anillos: LA Comunidad Del Anill… J. R.… 2001 ## 11 El Hobbit J. R.… 1991 ## 12 El caballero de la armadura oxidada Rober… 2000 ## 13 Harry Potter y la piedra filosofal J. K.… 1999 ## 14 Diario de Un Skin: Un Topo En El Movimiento Neo… Anton… 2003 ## 15 Son De Mar Manue… 2002 Однако у select() есть функции-помощники13, которые подходят для таких случаев: starts_with() ends_with() contains() matches() num_range() spain_books %&gt;% select(-contains(&quot;URL&quot;), -matches(&quot;Publisher&quot;)) %&gt;% # удалим заодно и издателя rename(title = `Book-Title`, author = `Book-Author`, published = `Year-Of-Publication`) # чиним имена ## # A tibble: 15 × 5 ## ISBN n title author published ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 8432206407 4 Sin Noticias De Gurb (Biblioteca breve) Eduar… 1995 ## 2 8433969978 4 El Libro de Las Ilusiones Paul … 2003 ## 3 846630679X 4 La caverna = A caverna Jose … 2002 ## 4 8472236552 4 UN Viejo Que Leia Novelas De Amor/the Old … Luis … 1993 ## 5 8495501198 4 Memorias de una geisha Arthu… 2001 ## 6 840149186X 3 El Club de Los Poetas Muertos N. H.… 1995 ## 7 8401499585 3 Los Pilares de La Tierra Ken F… 1995 ## 8 8423310353 3 El Camino (Coleccion Destinolibro) Migue… 1991 ## 9 8432215007 3 El perfume Patri… 1997 ## 10 8445071408 3 El Senor De Los Anillos: LA Comunidad Del … J. R.… 2001 ## 11 8445071416 3 El Hobbit J. R.… 1991 ## 12 8477204055 3 El caballero de la armadura oxidada Rober… 2000 ## 13 8478884459 3 Harry Potter y la piedra filosofal J. K.… 1999 ## 14 8484602508 3 Diario de Un Skin: Un Topo En El Movimient… Anton… 2003 ## 15 8495501112 3 Son De Mar Manue… 2002 Возможно, сюда стоит добавить что-то про работу с факторами. Но не сейчас. Литература "],["функциональное-программирование.html", "Тема 5 Функциональное программирование 5.1 Зачем писать функции? 5.2 Область видимости переменных 5.3 Аргументы функции 5.4 Векторизируй это 5.5 Векторизованные конструкции 5.6 Вместо циклов: базовый R и tidyverse 5.7 Purrr 5.8 Пример итерации 5.9 Furrr", " Тема 5 Функциональное программирование 5.1 Зачем писать функции? Программировать на R – прежде всего значит писать функции. Несмотря на десятки тысяч функций, обитающих в тысячах пакетов, рано или поздно вам понадобится своя функция, которая будет подходить для решения именно ваших задач. Функция и код – не одно и то же. Чтобы стать функцией, кусок кода должен, как минимум, получить имя. Зачем давать имя коду, который и так работает? Вот три причины, которые приводит Хадли Уикхем: у функции есть выразительное имя, которое облегчает понимание кода; при изменении требований необходимо обновлять код только в одном месте, а не во многих; меньше вероятность случайных ошибок при копировании (например, обновление имени переменной в одном месте, но не в другом) Writing good functions is a lifetime journey. — Hadley Wickham Чтобы определить функцию, необходимо дать ей имя. Машине все равно, как вы назовете функцию, но тем, кто будет читать код, не все равно. Имена должны быть информативы (поэтому функция f() – плохая идея). Также не стоит переписывать уже существующие в R имена! Далее следует определить формальные аргументы и, при желании, значения по умолчанию. Тело функции пишется в фигурных скобках. В конце кода функции располагается команда return(); если ее нет, то функция возвращает последнее вычисленное значение (см. здесь о том, когда что предпочесть). Написание функций – навык, который можно бесконечно совершенствовать. Начать проще всего с обычного кода. Убедившись, что он работает как надо, вы можете упаковать его в функцию. Например, нам нужна функция, которая ищет совпадения в двух векторах и возвращает совпавшие элементы. Сначала решим задачу для двух векторов. x &lt;- c(&quot;гнев&quot;, &quot;богиня&quot;, &quot;воспой&quot;) y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) idx &lt;- which(x %in% y) # 2 x[idx] ## [1] &quot;богиня&quot; Теперь заменяем фактические переменные на формальные. common_words &lt;- function(x, y){ idx &lt;- which(x %in% y) x[idx] } И применяем к новым данным. x &lt;- c(&quot;лишь&quot;, &quot;явилась&quot;, &quot;заря&quot;, &quot;розоперстая&quot;, &quot;вестница&quot;, &quot;утра&quot;) y &lt;- c(&quot;вестница&quot;, &quot;утра&quot;, &quot;заря&quot;, &quot;на&quot;, &quot;великий&quot;, &quot;олимп&quot;, &quot;восходила&quot;) common_words(x, y) ## [1] &quot;заря&quot; &quot;вестница&quot; &quot;утра&quot; Ура, все работает! Запомните простое правило: если вы трижды скопировали код, пора писать функцию! 5.2 Область видимости переменных Напишем функцию, которая будет центрировать данные, то есть вычитать среднее из каждого значения (забудем на время, что это уже делает базовая scale()): center &lt;- function(x){ n = x - mean(x) return(n) } x &lt;- c(5, 10, 15) center(x) ## [1] -5 0 5 Внутри нашей функции есть переменная n, которую не видно в глобальном окружении. Это локальная переменная. Область ее видимости – тело функции. Когда функция возвращает управление, переменная исчезает. Обратное неверно: глобальные переменные доступны в теле функции. 5.3 Аргументы функции Функция может принимать произвольное число аргументов. Доработаем наш код: center &lt;- function(x, na.rm = F){ if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие x - mean(x) # на этот раз без return() } x &lt;- c(5, 10, NA) center(x) ## [1] NA NA NA Что произошло? Почему следующий код выдает другой результат? center(x, na.rm = T) ## [1] -2.5 2.5 Вычисления в R ленивы, то есть они откладываются до тех пор, пока не понадобится результат. Если вы зададите аргумент, который не нужен в теле функции, ошибки не будет. center &lt;- function(x, na.rm = F, what_is_your_name){ if(na.rm) { x &lt;- x[!is.na(x)]} # добавим условие x - mean(x) # на этот раз без return() } center(x, na.rm = T) ## [1] -2.5 2.5 center(x, na.rm = T, what_is_your_name = &quot;Locusclassicus&quot;) ## [1] -2.5 2.5 Часто имеет смысл добавить условие остановки или сообщение, которое будет распечатано в консоль при выполнении. center &lt;- function(x){ if (length(x) == 1) {stop(&quot;И без меня посчитает&quot;)} x - mean(x) # на этот раз без return() } x &lt;- 10 center(x) # вернет ошибку 5.4 Векторизируй это Теперь самое главное: если мы хотим применить функцию к каждому элементу вектора, то в большинстве случаев достаточно просто вызвать функцию. Это называется векторизация. Это относится не только ко многим встроенным функциям R, но и к даже к операторам. x + 4 в действительности представляет собой +(x, 4): x &lt;- c(1.2, 2.51, 3.8) `+`(x, 4) ## [1] 5.20 6.51 7.80 Ключевую роль здесь играет переработка данных, о которой мы уже говорили: короткий вектор повторяется до тех пор, пока его длина не сравняется с длиной более длинного вектора. Как-то так: \\[\\left( \\begin{array}{c} 1.2 \\\\ 2.51 \\\\ 3.8 \\end{array} \\right) + \\left( \\begin{array}{c} 4 \\\\ 4 \\\\ 4 \\end{array} \\right)\\] Понимание того, как действуют векторизованные вычисления, очень важно для написания корректного кода. Посмотрите на пример ниже: почему функция is_article() возвращает два значения, хотя на входе только одно? is_article &lt;- function(x){ x == c(&quot;a&quot;, &quot;the&quot;) } x &lt;- &quot;the&quot; is_article(x) ## [1] FALSE TRUE Поскольку векторы сравниваются поэлементно, то функция ниже вернет разный результат в зависимости от того, в каком порядке заданы элементы: x &lt;- c(&quot;just&quot;, &quot;the&quot;) is_article(x) ## [1] FALSE TRUE x &lt;- c(&quot;the&quot;, &quot;just&quot;) is_article(x) # взрыв мозга ## [1] FALSE FALSE Подумайте, вектор какого типа и какой длины вернет код ниже. is_article &lt;- function(x) { articles &lt;- c(&quot;a&quot;, &quot;the&quot;) x %in% articles } x &lt;- c(rep(&quot;the&quot;, 5), rep(&quot;if&quot;, 5)) # is_article(x) 5.5 Векторизованные конструкции 5.5.1 Циклы Еще один способ повторить действия в R, при этом не копируя один и тот же код много раз, – это циклы. Один из главных принципов программирования на R гласит, что следует обходиться без циклов, а если это невозможно, то циклы должны быть простыми. — Нормат Мэтлофф Существует два основных цикла: цикл for и цикл while. На практике чаще используется цикл for, потому что цикл while легко отправить в бесконечность. 5.5.1.1 Цикл for Цикл ниже считает количество букв для каждого слова в векторе. y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) result &lt;- c() for(i in y) { n &lt;- nchar(i) result &lt;- c(result, n) } result ## [1] 1 5 3 2 7 6 9 4 В данном случае мы указали, что надо совершить какую-то операцию над каждым элементом вектора; но по сути это избыточно, потому что nchar() тоже векторизована. nchar(y) ## [1] 1 5 3 2 7 6 9 4 Поэтому чаще цикл for применяют к другим структурам данных. Например, к спискам и датафреймам. Загрузим и немного изменим датасет о гапаксах у Платона. Изменения нужны, так как цикл работает для данных только одного типа, в то время как в нашей таблице столбец dialogue содержит символьные строки, а group – фактор. Обратите внимание, что оператор pipe и функции из dplyr работают и с обычными датафреймами: rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- hapax_plato %&gt;% select(-group, -dialogue) # str(hapax_plato) ## &#39;data.frame&#39;: 26 obs. of 3 variables: ## $ words: chr &quot;8745&quot; &quot;8311&quot; &quot;17944&quot; &quot;4950&quot; ... ## $ hapax: chr &quot;36&quot; &quot;31&quot; &quot;122&quot; &quot;104&quot; ... ## $ ratio: chr &quot;0.004&quot; &quot;0.004&quot; &quot;0.007&quot; &quot;0.021&quot; ... Сейчас все данные в нашей таблице имеют тип chr, то есть строка, и при помощи цикла мы можем их трансформировать. for (i in seq_along(hapax_plato)) { # seq_along ≈ 1:length(x) hapax_plato[,i] &lt;- as.numeric(hapax_plato[,i]) } str(hapax_plato) # убеждаемся, что все получилось ## &#39;data.frame&#39;: 26 obs. of 3 variables: ## $ words: num 8745 8311 17944 4950 4169 ... ## $ hapax: num 36 31 122 104 19 87 15 125 12 32 ... ## $ ratio: num 0.004 0.004 0.007 0.021 0.005 0.007 0.003 0.005 0.003 0.008 ... При помощи циклов можно не только трансформировать данные, но и создавать новые. Чтобы посчитать среднее для столбца, цикл писать не надо: для этого есть функция colSums() (или, для других задач, rowSums()). А вот посчитать медиану таким образом не получится, тут может пригодиться цикл. library(tictoc) tic() medians &lt;- c() for (i in seq_along(hapax_plato)) { m &lt;- median(hapax_plato[,i]) medians &lt;- c(medians, m) } toc() ## 0.003 sec elapsed medians ## [1] 15589.500 94.500 0.007 Мы сохранили результат, инициировав пустой вектор, к которому затем привязали данные по каждому столбцу. Это не всегда хорошая идея, поскольку для больших данных может сильно замедлить цикл14. Еще один способ – сразу инициировать вектор нужной длины. Сравнить скорость можно при помощи пакета tictoc. tic() medians &lt;- vector(&quot;double&quot;, ncol(hapax_plato)) for (i in seq_along(hapax_plato)) { medians[i] &lt;- median(hapax_plato[,i]) } toc() ## 0.002 sec elapsed Второй способ чуть быстрее, и для больших данных это может быть существенно (знала бы я это раньше). Вы уже заметили, что в циклах часто используется буква i. Но никакой особой магии в ней нет! 5.5.1.2 Цикл while Как уже говорилось, с циклами while стоит быть осторожнее. Посмотрите, например, на этот цикл, который перебирает слова, пока не найдет слово длиной 6 букв. Что могло пойти не так? tic() k &lt;- 0 n &lt;- 0 while (n != 6) { k &lt;- k + 1 n &lt;- nchar(y[k]) } y[k] ## [1] &quot;богиня&quot; toc() ## 0.055 sec elapsed То же самое можно сделать без цикла, причем быстрее! tic() y[nchar(y) == 6][1] ## [1] &quot;богиня&quot; toc() ## 0.001 sec elapsed В целом, ничего незаконного в циклах нет, но множество вложенных друг в друга циклов сложно воспринимать; порой они могут замедлить выполнение кода. И в базовом R, и в диалекте tidyverse для этого есть несколько решений, о которых скажем чуть ниже. Сначала рассмотрим еще одну векторизованную конструкцию – условие. 5.5.2 Условия Иногда необходимо ограничить выполнение функции неким условием. Короткие условия можно писать в одну строку без фигурных скобок. if(any(nchar(y) &gt; 6)) print(&quot;многабукв&quot;) ## [1] &quot;многабукв&quot; Более сложные и множественные условия требуют фигурных скобок. Можно сравнить это с условным периодом: протасис (всегда либо TRUE, либо FALSE) в круглых скобках, аподосис в фигурных. if (sum(nchar(y)) &gt; 10) { print(&quot;много букв&quot;) } else if (sum(nchar(y)) &lt; 5) { print(&quot;мало букв&quot;) } else { print(&quot;норм букв&quot;) } ## [1] &quot;много букв&quot; Также в R можно использовать специальную функцию: ifelse((sum(nchar(y)) &gt; 10), &quot;много букв&quot;, &quot;мало букв&quot;) ## [1] &quot;много букв&quot; Прописывая условие, не забывайте, что применение бинарного оператора к вектору возвращает логический вектор: x &lt;- c(1:10) x &gt;= 5 ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE TRUE Условия с таким логическим вектором используют первый его элемент (а вряд ли это то, что вам нужно): if (x &gt;= 5) print(&quot;все сработало&quot;) ## Warning in if (x &gt;= 5) print(&quot;все сработало&quot;): the condition has length &gt; 1 and ## only the first element will be used Можно скорректировать код так: if (any(x &gt;= 5)) print(&quot;все сработало&quot;) ## [1] &quot;все сработало&quot; По той же причине внутри условия не надо использовать логические операторы | (“или”) или &amp; (“и”), потому что они векторизованы: x &lt; 3 | x &gt; 7 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE Вместо этого можно применять || (“или”) или &amp;&amp; (“и”), которые остановятся, дойдя до первого истинного значения. x &lt; 3 || x &gt; 7 ## [1] TRUE 5.6 Вместо циклов: базовый R и tidyverse Функция tapply() из базового R принимает на входе вектор, фактор (или список факторов) и функцию. Каждый фактор должен быть той же длины, что и вектор. Код ниже считает средний процент гапаксов по группам диалогов: load(&quot;./data/HapaxPlato.Rdata&quot;) # подготавливаем векторы my_fct &lt;- as.factor(hapax_plato$group) my_vct &lt;- as.numeric(hapax_plato$ratio) # применяем к ним функцию mean() tapply(my_vct, my_fct, mean) ## 1 2 3 ## 0.00550000 0.00750000 0.01133333 На диалекте tidyverse эта задача решается так: hapax_plato %&gt;% mutate(ratio = as.numeric(ratio)) %&gt;% group_by(group) %&gt;% summarise(mean = mean(ratio)) ## # A tibble: 3 × 2 ## group mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.0055 ## 2 2 0.0075 ## 3 3 0.0113 Функция apply() вызывает функцию для каждой строки или столбца матрицы или датафрейма. # избавляемся от факторов и строк rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- subset(hapax_plato, select = -c(dialogue, group)) # преобразуем столбцы в числовой формат при помощи apply tic() hapax_plato&lt;- apply(hapax_plato, 2, as.numeric) toc() ## 0 sec elapsed Сравните со скростью цикла, который мы написали выше: load(&quot;./data/HapaxPlato.Rdata&quot;) # избавляемся от факторов и строк rownames(hapax_plato) &lt;- hapax_plato$dialogue hapax_plato &lt;- subset(hapax_plato, select = -c(dialogue, group)) tic() for (i in seq_along(hapax_plato)) { hapax_plato[,i] &lt;- as.numeric(hapax_plato[,i]) } toc() ## 0.002 sec elapsed Функция apply() позволяет применять к данным собственные функции, в том числе анонимные. hapax_centered &lt;- apply(hapax_plato, 2, function(x) x - mean(x)) head(hapax_centered) ## words hapax ratio ## Apology -10619.423 -110.69231 -0.0031538462 ## Charmides -11053.423 -115.69231 -0.0031538462 ## Cratylus -1420.423 -24.69231 -0.0001538462 ## Critias -14414.423 -42.69231 0.0138461538 ## Crito -15195.423 -127.69231 -0.0021538462 ## Euthydemus -6911.423 -59.69231 -0.0001538462 Опять-таки, все это решается (даже проще) в грамматике dplyr: as_tibble(hapax_plato) %&gt;% mutate(words = words - mean(words), hapax = hapax - mean(hapax), ratio = ratio - mean(ratio)) Видно, что по времени мы при этом сильно не выигрываем; к тому же, нам пришлось повторить один код три раза. Значит, надо что-то менять. Например, так15: tic() as_tibble(hapax_plato) %&gt;% mutate_all(function(x) x - mean(x)) toc() Или даже так16: fn &lt;- function(x) x - mean(x) as_tibble(hapax_plato) %&gt;% mutate(across(1:3, fn)) %&gt;% invisible() В любом случае, нам удалось обойтись без цикла, код понятен и хорошо читается. Функции lapply() и sapply() подходят для применения функций к спискам (и к датафреймам, которые по сути представляют собой прямоугольные списки). Чтобы понять, как они работают, сначала создадим список. При анализе текста со списками приходится иметь дело достаточно часто: объекты типа stylo.corpus, которые создает пакет stylo, по сути являются списками. Создадим игрушечный корпус из двух игрушечных текстов. x &lt;- c(&quot;гнев&quot;, &quot;богиня&quot;, &quot;воспой&quot;) y &lt;- c(&quot;в&quot;, &quot;мысли&quot;, &quot;ему&quot;, &quot;то&quot;, &quot;вложила&quot;, &quot;богиня&quot;, &quot;державная&quot;, &quot;гера&quot;) corpus &lt;- list(x = x, y = y) Наш условный корпус – это список из 2 элементов (текстов), а каждый текст хранится как символьный вектор. Допустим, мы хотим взять из каждого диалога выборку размером 5 слов, то есть применить функцию sample() к элементам списка. При помощи lapply() (l = list) это делается так: set.seed(0211) lapply(corpus, sample, 5, replace = T) ## $x ## [1] &quot;воспой&quot; &quot;воспой&quot; &quot;воспой&quot; &quot;богиня&quot; &quot;воспой&quot; ## ## $y ## [1] &quot;державная&quot; &quot;вложила&quot; &quot;то&quot; &quot;мысли&quot; &quot;вложила&quot; Функция sapply() ведет себя так же, но упрощает результат до вектора или матрицы (s = simplify). sapply(corpus, sample, 5, replace = T) ## x y ## [1,] &quot;богиня&quot; &quot;ему&quot; ## [2,] &quot;гнев&quot; &quot;державная&quot; ## [3,] &quot;гнев&quot; &quot;ему&quot; ## [4,] &quot;богиня&quot; &quot;то&quot; ## [5,] &quot;воспой&quot; &quot;мысли&quot; Функция vapply() позволяет задать тип данных на выходе. vapply(corpus, sample, size = 5, replace = T, character(5)) ## x y ## [1,] &quot;воспой&quot; &quot;то&quot; ## [2,] &quot;богиня&quot; &quot;гера&quot; ## [3,] &quot;гнев&quot; &quot;вложила&quot; ## [4,] &quot;гнев&quot; &quot;гера&quot; ## [5,] &quot;гнев&quot; &quot;мысли&quot; Поскольку наш “корпус” – это список, то применить грамматику dplyr не очень удобно, списко легко превращается в таблицу: stack(corpus) # передвинуть и переименовать: `relocate()` и `rename()` ## values ind ## 1 гнев x ## 2 богиня x ## 3 воспой x ## 4 в y ## 5 мысли y ## 6 ему y ## 7 то y ## 8 вложила y ## 9 богиня y ## 10 державная y ## 11 гера y Теперь повторные выборки можно делать так: set.seed(0211) stack(corpus) %&gt;% group_by(ind) %&gt;% sample_n(size = 5, replace = T) ## # A tibble: 10 × 2 ## # Groups: ind [2] ## values ind ## &lt;chr&gt; &lt;fct&gt; ## 1 воспой x ## 2 воспой x ## 3 воспой x ## 4 богиня x ## 5 воспой x ## 6 державная y ## 7 вложила y ## 8 то y ## 9 мысли y ## 10 вложила y 5.7 Purrr По-настоящему мощный инструмент для итераций – это пакет purrr из семейства tidyverse17. Разработчики предупреждают, что потребуется время, чтобы овладеть этим инструментом (Wickham and Grolemund 2017). You should never feel bad about using a loop instead of a map function. The map functions are a step up a tower of abstraction, and it can take a long time to get your head around how they work. — Hadley Wickham &amp; Garrett Grolemund В семействе функций map_ из этого пакета всего 23 вариации18. Вот основные из них: map() map_lgl() map_int() map_dbl() map_chr() Все они принимают на входе данные и функцию, которую следует к ним применить, и возвращают результат в том виде, который указан после подчеркивания. Просто map() вернет список, а map_df() – таблицу: hapax_plato %&gt;% as_tibble() %&gt;% map_df(center) %&gt;% head() ## # A tibble: 6 × 3 ## words hapax ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -10619. -111. -0.00315 ## 2 -11053. -116. -0.00315 ## 3 -1420. -24.7 -0.000154 ## 4 -14414. -42.7 0.0138 ## 5 -15195. -128. -0.00215 ## 6 -6911. -59.7 -0.000154 Если на выходе требуется числовой вектор, то используем суффикс dbl: round(map_dbl(hapax_plato, mean), 3) # это именованный вектор! ## words hapax ratio ## 19364.423 146.692 0.007 Если необходимо несколько раз вызывать одну и ту же функцию с двумя аргументами, используется функция map2() 19. Аргументы, которые меняются при каждом вызове, пишутся до функции; аргументы, которые остаются неизменны, – после. mean = list(5, 10, -3) sd = list(1, 5, 50) map2(mean, sd, rnorm, n = 5) ## [[1]] ## [1] 5.727572 4.812749 4.528140 6.565481 4.356073 ## ## [[2]] ## [1] 18.6183347 0.5775309 23.3592929 3.2133071 15.5702820 ## ## [[3]] ## [1] -10.13466 51.54818 -23.10523 -12.53555 -66.80735 Как работает map2() Это можно обобщить следующим образом (источник){ width=60% }: Можно было бы предположить, что должны быть и map3(), map4() и т.д., но во всех случаеях, когда у функции больше двух аргументов, используется pmap(). 5.8 Пример итерации Функция map2() в анализе текста: функция, которая принимает на входе список таблиц, созданных функцией slide, и назначает каждому окну id.20. Используется для создания скользящего окна при создании эмбеддингов. corpus_tbl &lt;- as_tibble(stack(corpus)) windows &lt;- slider::slide(corpus_tbl, ~.x, .after = 1) out &lt;- map2(.x = windows, .y = 1:length(windows), ~ mutate(.x, window_id = .y)) # out is a list out[2] ## [[1]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 богиня x 2 ## 2 воспой x 2 Поскольку второй аргумент – это, по сути, индекс, можно было бы использовать функцию imap(): out &lt;- imap(.x = windows, ~ mutate(.x, window_id = .y)) out[2:3] ## [[1]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 богиня x 2 ## 2 воспой x 2 ## ## [[2]] ## # A tibble: 2 × 3 ## values ind window_id ## &lt;chr&gt; &lt;fct&gt; &lt;int&gt; ## 1 воспой x 3 ## 2 в y 3 5.9 Furrr Про параллельные вычисления, если останутся силы. Литература "],["импорт-и-экспорт-данных.html", "Тема 6 Импорт и экспорт данных 6.1 Рабочая директория 6.2 Чтение файлов из интернета 6.3 Чтение локальных файлов 6.4 xml и html 6.5 json 6.6 GutenbergR 6.7 RPerseus 6.8 Чтение нескольких файлов 6.9 Экспорт", " Тема 6 Импорт и экспорт данных 6.1 Рабочая директория Любой анализ данных начинается с импорта данных. Прежде чем что-то делать, проверьте свою рабочую директорию при помощи getwd() (подробнее). Для смены можно использовать как абсолютный, так и относительный путь: setwd(&quot;/Users/olga/R_Workflow/Text_Analysis_2023&quot;) # искать в текущей директории setwd(&quot;./Text_Analysis_2023&quot;) # перейти на уровень вверх setwd(&quot;../&quot;) 6.2 Чтение файлов из интернета Основная функция для скачивания файлов из Сети – download.file(), которой необходимо задать в качестве аргументов url, название сохраняемого файла, иногда также метод. Попробуем скачать датасет из Репозитория открытых данных по русской литературе и фольклору под названием “Байрон в русских переводах 1810–1860-х годов”. url &lt;- &quot;https://dataverse.pushdom.ru/api/access/datafile/:persistentId?persistentId=doi:10.5072/openlit-2019.11-R002/VQRXXK&quot; # если url начинается с https, на Mac _может_ потребоваться указать method = &quot;curl&quot; download.file(url, destfile = &quot;files/Byron.tab&quot;) После этого в папке files появится новый файл. Получить список скачанных файлов можно при помощи list.files(). list.files(&quot;./files&quot;) ## [1] &quot;AmazonBooks.xlsx&quot; &quot;antibarbari_archive&quot; &quot;archive.zip&quot; ## [4] &quot;BX-CSV-Dump&quot; &quot;Byron.tab&quot; &quot;CiceroOff.txt&quot; ## [7] &quot;karamzin_liza.pdf&quot; &quot;karamzin_liza.txt&quot; &quot;template.docx&quot; ## [10] &quot;War_and_Peace.xml&quot; 6.3 Чтение локальных файлов Основные функции для чтения локальных файлов в базовом R: read.table() `read.csv()`` 6.3.1 csv и tsv Файл, который мы скачали, имеет расширение .tab. Такие файлы по структуре аналогичны файлам .tsv (tab separated values). Чтобы его прочитать, используем read.table(), указав тип разделителя: Byron &lt;- read.table(&quot;files/Byron.tab&quot;, sep = &quot;\\t&quot;, header = TRUE) head(Byron) Функция read.csv() отличается лишь тем, что автоматически выставляет значения аргументов sep = \",\", header = TRUE. В диалекте tidyverse для импорта подобных файлов используется пакет readr21. Вернемся к нашему датасету про буккроссинг, файлы которого имеют расширение csv. К сожалению, это не всегда гарантия того, что перед вами действительно csv: library(readr) users &lt;- read_csv(&quot;files/BX-CSV-Dump/BX-Users.csv&quot;) ## Rows: 278858 Columns: 1 ## ── Column specification ───────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): User-ID;Location;Age ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(users) ## # A tibble: 6 × 1 ## `User-ID;Location;Age` ## &lt;chr&gt; ## 1 1;nyc, new york, usa;NULL ## 2 2;stockton, california, usa;18 ## 3 3;moscow, yukon territory, russia;NULL ## 4 4;porto, v.n.gaia, portugal;17 ## 5 5;farnborough, hants, united kingdom;NULL ## 6 6;santa monica, california, usa;61 Чтобы исправить дело, воспользуемся другой функцией из того же пакета: users &lt;- read_delim(&quot;files/BX-CSV-Dump/BX-Users.csv&quot;, delim = &quot;;&quot;) ## Rows: 246666 Columns: 3 ## ── Column specification ───────────────────────────────────────────────────────── ## Delimiter: &quot;;&quot; ## chr (2): Location, Age ## dbl (1): User-ID ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. users ## # A tibble: 246,666 × 3 ## `User-ID` Location Age ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 nyc, new york, usa NULL ## 2 2 stockton, california, usa 18 ## 3 3 moscow, yukon territory, russia NULL ## 4 4 porto, v.n.gaia, portugal 17 ## 5 5 farnborough, hants, united kingdom NULL ## 6 6 santa monica, california, usa 61 ## 7 7 washington, dc, usa NULL ## 8 8 timmins, ontario, canada NULL ## 9 9 germantown, tennessee, usa NULL ## 10 10 albacete, wisconsin, spain 26 ## # ℹ 246,656 more rows Очевидно, это не решает всех проблем, но как справиться с оставшимися, мы рассказывали в уроке об опрятных данных. 6.3.2 xls и xlsx Не самый любимый аналитиками, но очень распространенный тип файлов. Чтобы с ним работать, нужно установить пакет readxl из семейства tidyverse22. Это не единственный пакет для работы с Excel, но, пожалуй, самый удобный. Файл с самыми популярными на Amazon книгами можно взять здесь. library(readxl) amazon &lt;- read_excel(&quot;files/AmazonBooks.xlsx&quot;) head(amazon) ## # A tibble: 6 × 7 ## Name Author `User Rating` Reviews Price Year Genre ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10-Day Green Smoothie Cleanse JJ Sm… 4.7 17350 8 2016 Non … ## 2 11/22/63: A Novel Steph… 4.6 2052 22 2011 Fict… ## 3 12 Rules for Life: An Antidote… Jorda… 4.7 18979 15 2018 Non … ## 4 1984 (Signet Classics) Georg… 4.7 21424 6 2017 Fict… ## 5 5,000 Awesome Facts (About Eve… Natio… 4.8 7665 12 2019 Non … ## 6 A Dance with Dragons (A Song o… Georg… 4.4 12643 11 2011 Fict… 6.3.3 txt Для чтения текстовых файлов в базовом R есть функция readlines(). readLines(con = &quot;files/karamzin_liza.txt&quot;, n = 1) ## [1] &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели -- куда глаза глядят -- по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москву, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. &quot; 6.3.4 doc Если есть возможность конвертировать документ Word в простой текстовый формат, то лучше так и сделать. Если нет, то устанавливаем пакет officer. library(officer) files &lt;- list.files(path = &quot;files&quot;, pattern = &quot;docx&quot;) files[1] ## [1] &quot;template.docx&quot; # read file doc &lt;- read_docx(paste0(&quot;files/&quot;, files[1])) content &lt;- docx_summary(doc) head(content, 2) # весь текст доступен в столбце text ## doc_index content_type style_name text level num_id ## 1 1 paragraph NA Chapter Title NA NA ## 2 2 paragraph NA Author’s Name NA NA Таким образом, однако, мы теряем все сноски. Следующий код позволяет их достать: library(xml2) xml_text(xml_find_all(doc$footnotes$get(), &quot;*&quot;)) Тут уже применяются функции для работы с xml. Поэтому лишний раз подумайте, не проще ли конвертировать документ Word в .txt. 6.3.5 pdf С pdf тоже без нужды лучше не иметь дела. Но если все-таки пришлось читать pdf, для этого есть пакет pdftools23. library(pdftools) ## Using poppler version 22.02.0 # длинющий вектор, который придется очищать от \\n (новая строка) liza &lt;- pdf_text(pdf = &quot;files/karamzin_liza.pdf&quot;) # метаданные в виде списка meta &lt;- pdf_info(pdf = &quot;files/karamzin_liza.pdf&quot;) meta$created ## [1] &quot;2023-07-16 19:36:09 MSK&quot; Разработчики утверждают, что пакет справится и с распознаванием текста, но установка нужных для этого зависимостей может быть сопряжена с такими трудостями, что вы, вероятно, захотите решить эту задачу за пределами R. Например, тут. 6.3.6 zip Для работы с архивами есть функция unzip(). Полезно помнить, что большой архив не обязательно распечатывать полностью. Если выставить аргумент list = TRUE, то функция вернет список всех файлов в архиве, из которых можно прочитать в память лишь избранные: archive &lt;- unzip(&quot;files/archive.zip&quot;, files = NULL, list = TRUE) archive ## Name Length Date ## 1 AmazonBooks - Sheet1.csv 4294967295 2021-02-18 19:58:00 ## 2 AmazonBooks.xlsx 4294967295 2021-02-18 19:58:00 Код ниже позволяет извлечь из архива только нужный файл: unzip(&quot;files/archive.zip&quot;, files = &quot;AmazonBooks.xlsx&quot;) После этого файл можно прочитать в R, как указано выше. 6.4 xml и html XML и HTML – это языки разметки. Язык HTML применяется для создания стандартных веб-страниц, поэтому если вы хотите достать некий текст из Интернета, то скорее всего вместе с текстом утащите еще так называемые теги (в треугольных скобках), то есть элементы этой самой разметки: url &lt;- &quot;https://www.thelatinlibrary.com/cicero/off1.shtml&quot; doc_html &lt;- scan(url, what = &quot;character&quot;, sep = &quot;\\n&quot;) head(doc_html) ## [1] &quot;&lt;html&gt;&quot; ## [2] &quot;\\t&lt;head&gt;&quot; ## [3] &quot;\\t\\t&lt;title&gt;&quot; ## [4] &quot;\\t\\t\\tCicero: de Officiis I&quot; ## [5] &quot;\\t\\t&lt;/title&gt;&quot; ## [6] &quot;\\t\\t&lt;meta http-equiv=\\&quot;Content-Type\\&quot; content=\\&quot;text/html; charset=utf-8\\&quot;&gt;&quot; Для анализа текста эти, как правило, не нужны, так как отвечают за оформление. Они “сообщают” браузеру, как отображать тот или иной контент. Пакет stylo дает возможность легко от них избавиться. Текст возвращается без тегов и уже разделенным на слова. library(stylo) # download.file(url, destfile = &quot;files/CiceroOff.txt&quot;) doc_text &lt;- load.corpus.and.parse(files = &quot;CiceroOff.txt&quot;, corpus.dir = &quot;files&quot;, markup.type = &quot;html&quot;, encoding = &quot;UTF-8&quot;) doc_vec &lt;- unlist(doc_text, use.names = FALSE) doc_vec[1:10] ## [1] &quot;m&quot; &quot;tvlli&quot; &quot;ciceronis&quot; &quot;de&quot; &quot;officiis&quot; &quot;liber&quot; ## [7] &quot;primvs&quot; &quot;quamquam&quot; &quot;te&quot; &quot;marce&quot; XML (от англ. eXtensible Markup Language) — расширяемый язык разметки. Слово “расширяемый” значит, что список тегов не зафиксирован раз и навсегда: пользователи могут вводить свои собственные теги и создавать так называемые настраиваемые языки разметки. Один из таких настраиваемых языков – это TEI (Text Encoding Initiative). Большая часть размеченных литературных корпусов хранится именно в таком виде. Это очень удобно, и вот почему: документы в формате XML, как и документы в формате HTML, содержат данные, заключенные в теги, но если в формате HTML теги определяют оформление данных, то в формате XML теги нередко определяют структуру и смысл данных. С их помощью мы можем достать из документа именно то, что нам интересно: определенную главу, речи конкретных персонажей, слова на иностранных языках и т.п. Добавлять и удалять разметку может любой пользователь в редакторе XML кода. По сути, это просто текст, хотя и причудливо (на первый взгляд) оформленный. Подбробнее о структуре XML документов и способах работы с ними вы можете прочитать в книгах: (Nolan and Lang 2014) и (Холзнер 2004). Здесь мы лишь кратко затронем вопрос о том, как парсить XML документ в R, то есть извлекать из него нужную нам информацию. Главное, что надо понимать: любой XML документ представляет собой иерархически организованное дерево, у которого есть один и только один корневой элемент, из которого расходятся ветви. С этим деревом в R можно работать двумя способами: либо как со списком, который содержит другие вложенные в него списки, либо при помощи синтаксиса XPath. В видео по ссылке вы можете ближе познакомиться и с тем, и с другим подходом. В качестве примера загрузим еще один датасет “Пушкинсого дома”, подготовленный Д.А. Скоринкиным: “Персонажи «Войны и мира» Л. Н. Толстого: вхождения в тексте, прямая речь и семантические роли”. Функция xmlTreeParse() разбирает XML-файл и генерирует R-структуру, представляющую дерево XML. Парсинг xml начинается с поиска корневого элемента. library(XML) filename = &quot;files/War_and_Peace.xml&quot; doc &lt;- xmlTreeParse(filename, useInternalNodes = T) rootnode &lt;- xmlRoot(doc) После этого можно внимательнее взглянуть на структуру xml. Корневой элемент расходится на две ветви. Полностью они нам пока не нужны, узнаем только имена: names(xmlChildren(rootnode)) ## [1] &quot;teiHeader&quot; &quot;text&quot; Очевидно, что что-то для нас интересное будет спрятано в ветке text, глядим на нее (индексирование как для списков): names(xmlChildren(rootnode[[&quot;text&quot;]])) ## [1] &quot;div&quot; &quot;div&quot; &quot;div&quot; &quot;div&quot; &quot;div&quot; Итак, текст делится на какие-то пять частей. Функция xmlGetAttr() позволяет узнать значение атрибута type: как выясняется, это том. # это список divs_1 &lt;- rootnode[[&quot;text&quot;]][[&quot;div&quot;]] xmlGetAttr(divs_1, &quot;type&quot;) ## [1] &quot;volume&quot; Добраться до определенного узла можно не только путем индексирования, но и – гораздо удобнее – пр помощи синтаксиса XPath. Для этого просто указываем путь до узла. Попробуем спуститься на уровень ниже: divs_2 &lt;- getNodeSet(doc, &quot;/tei:TEI//tei:text//tei:div//tei:div&quot;, namespaces = c(tei = &quot;http://www.tei-c.org/ns/1.0&quot;)) length(divs_2) ## [1] 375 Теперь мы получили довольно длинный список (375 элементов), к которому снова можем применить xmlGetAttr(). Выясняем, что это, в основном, главы: unique(sapply(divs_2, xmlGetAttr, &quot;type&quot;)) ## [1] &quot;part&quot; &quot;chapter&quot; Чтобы извлечь текст из конкретного узла, нужна функция xmlValue. # забираем 2, т.к. 1 -- это часть, а не глава chapter_1 &lt;- xmlValue(divs_2[[2]]) Распечатывать весь текст первой главы не будем (это очень длинный вектор); разобъем текст на строки и выведем первую и последнюю: library(stringr) chapter_lines &lt;- str_split(chapter_1, pattern = &quot;\\n&quot;) chapter_lines[[1]][[5]] ## [1] &quot; — Eh bien, mon prince. Gênes et Lueques ne sont plus que des apanages, des поместья, de la famille Buonaparte. Non, je vous préviens que si vous ne me dites pas que nous avons la guerre, si vous vous permettez encore de pallier toutes les infamies, toutes les atrocités de cet Antichrist (ma parole, j&#39;y crois) — je ne vous connais plus, vous n&#39;êtes plus mon ami, vous n&#39;êtes plus мой верный раб, comme vous dites. Ну, здравствуйте, здравствуйте. Je vois que je vous fais peur, садитесь и рассказывайте.&quot; chapter_lines[[1]][[838]] ## [1] &quot; Ce sera dans votre famille que je ferai mon apprentissage de vieille fille.&quot; Первая и последняя реплика по-французски: все правильно! Подробнее о работе с XML (и HTML) стоит посмотреть вот это видео. P.S. Для работы с XML также подходит пакет xml224. 6.5 json To be supplied. 6.6 GutenbergR Для R существуют пакеты, позволяющие извлекать тексты из онлайн-библиотек. Пакет GutenbergR25 поможет достать тексты из библиотеки Gutenberg, но будьте осторожны: распознаны они не всегда хорошо и порой содержат много разного шума, например примечания редактора, номера страниц и т.п. В билингвах источник и перевод могут идти вперемешку. И если в XML подобные элементы будут окружены соответствующими тегами, которые позволят их легко отбросить при анализе, то Gutenberg дает вам сырой текст. Часто его надо хорошенько чистить при помощи регулярных выражений или даже вручную. Поиск нужного текста лучше начать с изучения метаданных: library(gutenbergr) ## метаданные содержат множество NA gutenberg_authors ## # A tibble: 21,323 × 7 ## gutenberg_author_id author alias birthdate deathdate wikipedia aliases ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 United States U.S.… NA NA https://… &lt;NA&gt; ## 2 3 Lincoln, Abr… &lt;NA&gt; 1809 1865 https://… United… ## 3 4 Henry, Patri… &lt;NA&gt; 1736 1799 https://… &lt;NA&gt; ## 4 5 Adam, Paul &lt;NA&gt; 1849 1931 https://… &lt;NA&gt; ## 5 7 Carroll, Lew… Dodg… 1832 1898 https://… &lt;NA&gt; ## 6 8 United State… &lt;NA&gt; NA NA https://… Agency… ## 7 9 Melville, He… Melv… 1819 1891 https://… &lt;NA&gt; ## 8 10 Barrie, J. M… &lt;NA&gt; 1860 1937 https://… Barrie… ## 9 12 Smith, Josep… Smit… 1805 1844 https://… &lt;NA&gt; ## 10 14 Madison, Jam… Unit… 1751 1836 https://… &lt;NA&gt; ## # ℹ 21,313 more rows gutenberg_metadata ## # A tibble: 69,199 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 &quot;The De… Jeffe… 1638 en Politics/American … ## 2 2 &quot;The Un… Unite… 1 en Politics/American … ## 3 3 &quot;John F… Kenne… 1666 en &lt;NA&gt; ## 4 4 &quot;Lincol… Linco… 3 en US Civil War ## 5 5 &quot;The Un… Unite… 1 en United States/Poli… ## 6 6 &quot;Give M… Henry… 4 en American Revolutio… ## 7 7 &quot;The Ma… &lt;NA&gt; NA en &lt;NA&gt; ## 8 8 &quot;Abraha… Linco… 3 en US Civil War ## 9 9 &quot;Abraha… Linco… 3 en US Civil War ## 10 10 &quot;The Ki… &lt;NA&gt; NA en Banned Books List … ## # ℹ 69,189 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Функция gutenberg_works() позволяет уточнить, какие столбцы необходимо вернуть: latin_works &lt;- gutenberg_works(languages = &quot;la&quot;) latin_works ## # A tibble: 92 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 C. Iuli… Caesa… 3621 la Classical Antiquity ## 2 226 Cicero&#39;… Cicer… 128 la Classical Antiquity ## 3 227 Aeneidos Virgil 129 la Classical Antiquit… ## 4 229 The Buc… Virgil 129 la Classical Antiquity ## 5 231 Georgic… Virgil 129 la Classical Antiquity ## 6 237 Sexti P… Prope… 133 la &lt;NA&gt; ## 7 825 Latin V… Anony… 216 la Christianity ## 8 826 Latin V… Anony… 216 la Christianity ## 9 828 Latin V… Anony… 216 la Christianity ## 10 4317 Prophet… Anony… 216 la &lt;NA&gt; ## # ℹ 82 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Беглый взгляд на эту таблицу говорит о том, что в галактике Гутенберга царит полный хаос: например, в поле “название” хранится имя автора. Но после того, как нужный автор или нужное сочинение найдены, можно сделать так: caesar &lt;- gutenberg_works(author == &quot;Caesar, Julius&quot;, languages = &quot;la&quot;) caesar ## # A tibble: 3 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 C. Iuli … Caesa… 3621 la Classical Antiquity ## 2 18837 Commenta… Caesa… 3621 la Classical Antiquity ## 3 29645 The Gate… Caesa… 3621 la &lt;NA&gt; ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Чтобы извлечь отдельный текст (тексты): de_bello_gallico &lt;- gutenberg_download(218, meta_fields = &quot;title&quot;, mirror = &quot;ftp://mirrors.xmission.com/gutenberg/&quot;) de_bello_gallico ## # A tibble: 2,552 × 3 ## gutenberg_id text title ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 218 &quot;CAESAR&#39;S COMMENTARIES IN LATIN&quot; C. Iuli Caesaris De Bello Gall… ## 2 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 3 218 &quot;Books I-IV&quot; C. Iuli Caesaris De Bello Gall… ## 4 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 5 218 &quot;By Julius Caesar&quot; C. Iuli Caesaris De Bello Gall… ## 6 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 7 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 8 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 9 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## 10 218 &quot;&quot; C. Iuli Caesaris De Bello Gall… ## # ℹ 2,542 more rows Существует несколько зеркал библиотеки Gutenberg, и, если при выполнении функции gutenberg_download() возникает ошибка “could not download a book at http://aleph.gutenberg.org/”, то следует использовать аргумент mirror. Список зеркал доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL 6.7 RPerseus To be supplied. 6.8 Чтение нескольких файлов To be supplied. 6.9 Экспорт To be supplied. 6.9.1 Rdata To be supplied. Литература "],["воспроизводимые-исследования.html", "Тема 7 Воспроизводимые исследования 7.1 О воспроизводимости 7.2 Markdown 7.3 Синтаксис Markdown 7.4 Публикация html", " Тема 7 Воспроизводимые исследования 7.1 О воспроизводимости Полученный в результате количественных исследований результат должен быть проверяем и воспроизводим. Даже на заре стилометрии, когда все вычисления проводились вручную, ученые стремились максимально подробно задокументировать свои вычисления: какие слова они считали, в каких текстах и т.п. Об одном исключении из этого правила можно прочитать вот здесь. Сегодня к документации исследования предъявляются гораздо более строгие требования: в большинстве случаев недостаточно просто рассказать, что вы проделали. Теоретически читатель должен иметь возможность проделать тот же путь, что и автор: вопроизвести его результаты, но в обратном направлении. Воспроизводимость (reproducibility) – это не то же, что повторяемость (replicability). Ученый, который повторяет исследование, проводит его заново на новых данных. Воспроизведение – гораздо более скромная задача, не требующая таких ресурсов, как повторение (Winter 2020, 47). Иллюстрация ниже заимствована из книги Роджера Пенга, специалиста по биостатистике26. За тем исключением, что вместо квадратика “Nature” в гуманитарном исследовании будет квадратик “Culture”, общие принципы те же: все, что вы делаете – от сбора данных до их оформления в виде графиков – должно быть задокументировано и воспроизводимо. Для этого должны выполняться три основных требования: доступность данных и метаданных; доступность компьютерного кода; доступность программного обеспечения. Именно поэтому всегда, когда возможно, преимущество должно отдаваться свободно распространяемому ПО. В этом смысле R имеет преимущество перед такими программами, как SPSS, SAS, Matlab, STATA и др. Все, что вы делаете на “не имеющем аналогов в мире” закрытом ПО может быть увлекательно лично для вас, но не отвечает научным критериям проверяемости. Поэтому многие разработчики, создающие приложения для анализа текста, тоже выкладывают их на GitHub. (Пример). Правило второ: код имеет преимущество перед GUI (Graphical User Interface): вспомнить, какие кнопки в каком порядке были нажаты, даже самому автору бывает непросто. От скачивания файла до экспорта графиков – все должно быть зафисировано, причем в виде, понятном не только для машины, но и для человека. Некоторые пакеты в R оснащены GUI (например, stylo), но пользоваться им лучше умеренно, пока вы только знакомитесь с инструментом. Правило третье: код и сырые данные для статьи принято публиковать на GitHub. Исследователи, работающие с разными изданиями Аристотеля, могут прийти к разным выводам. Вопроизвести ваше исследование на других данных может быть невозможно. Если вы работаете с материалом, защищенным копирайтом, на GitHub можно настроить доступ к репозиторию: он не будет виден всем, но, например, рецензенты смогут проверить ваши выводы27. Авторитетный International Journal of Digital Humanities прямо пишет в инструкциях для авторов: Please ensure you provide all relevant editable source files at every submission and revision. Failing to submit a complete set of editable source files will result in your article not being considered for review. Уже на этапе планирования исследования очень важно продумать, как вы будете его документировать. Это делается не после того, как вы все выяснили, а в процессе. Правило четвертое: код пишется не только для машин, но и для людей. Важно документировать не только то, что вы делали, но и почему. R дает для этого множество возможностей, главная из которых – это Markdown28. 7.2 Markdown Markdown – это облегчённый язык разметки. Он позволяет создавать документы разного формата – не только HTML (веб-страницы), но и PDF и Word. В R Markdown создается огромное количество документов - сайтов, статей и книг (например, этот курс), презентаций, отчетов, дашбордов и т.п. При этом Markdown позволяет писать код не только на R, но и других языках – например, на Python; это дает возможность создания полностью воспроизводимых документов, сочетающих код и поясняющий текст. Чтобы начать работать с документами .rmd, нужен пакет rmarkdown; в RStudio он уже предустановлен. Создание нового документа .rmd происходит из меню29: По умолчанию документ .rmd снабжен шапкой yaml. Она не обязательна. Здесь содержатся данные об авторе, времени создания, формате, сведения о файле с библиографией и т.п. --- title: &quot;Demo&quot; author: &quot;My name&quot; date: &quot;2023-07-29&quot; output: html_document --- Также в документе .rmd скорее всего будет простой текст и блоки кода. Чтобы “сшить” html (pdf, doc), достаточно нажать кнопку knit. Либо можно запустить в консоли код: rmarkdown::render(\"Demo.Rmd\"). После этого в рабочей директории появится новый файл (html, pdf, или doc), которым можно поделиться с коллегами, грантодателями или друзьями. 7.3 Синтаксис Markdown 7.3.1 Заголовки Заголовки разного уровня задаются при помощи решетки30: # Заголовок первого уровня ## Заголовок второго уровня ### Заголовок третьего уровня #### Заголовок четвёртого уровня Пример заголовка третьего уровня: 7.3.2 Форматирование *курсив* _курсив_ **полужирный** __полужирный__ ***полужирный курсив*** ___полужирный курсив___ ~~зачеркнутый~~ &lt;mark&gt;выделение&lt;/mark&gt; Пример: курсив полужирный уж и не знаю как выделить зачеркнутый выделение 7.3.3 Списки Нумерованный список 1. Пункт первый 2. Пункт второй 3. Пункт третий Пример: Пункт первый Пункт второй Пункт третий Маркированный список - Пункт первый - Пункт второй - Пункт третий Пример: Пункт первый Пункт второй Пункт третий Также Markdown позволяет делать вложенные списки: 1. Пункт первый - Подпункт первый - Подпункт второй 2. Пункт второй Пример: Пункт первый Подпункт первый Подпункт второй Пункт второй Самое удобное, что элементы списка не обязательно нумеровать: (@) Пункт первый. (@) Пункт не знаю какой. Пункт первый. Пункт не знаю какой. 7.3.4 Ссылки [Текст ссылки](http://antibarbari.ru/) Пример: Текст ссылки 7.3.5 Изображения ![Текст описания](http://antibarbari.ru/wp-content/uploads/2023/03/corderius-656x300.png) Пример: Моя картинка Два нюанса: можно давать ссылки на локальные файлы (то есть такие файлы, которые хранятся на компьютере) изображения можно вставлять, пользуясь непосредственно разметкой html. &lt;img src=&quot;images/my_image.jpg&quot; width=40%&gt; 7.3.6 Блоки кода Можно вставлять непосредственно в текст, это будет выглядеть вот так; для этого код выделяют одинарным обратным апострофом (грависом). Но чаще код дают отдельным блоком. Эти блоки можно именовать; тогда в случае ошибки будет сразу понятно, где она случилась31. ```{} some code here ``` В фигурных скобках надо указать язык, например {r}, тогда код будет автоматически подсвечен. Там же в фигурных скобках можно задать следующие параметры: eval = FALSE код будет показан, но не будет выполняться; include = FALSE код будет выполнен, но ни код, ни результат не будут показаны; echo = FALSE код будет выполнен, но не показан, результаты при этом видны; message = FALSE или warning = FALSE прячет сообщения или предупреждения; results = 'hide' не распечатывает результат, а fig.show = 'hide' прячет графики; error = TRUE “сшивание” продолжается, даже если этот блок вернул ошибку. 7.3.7 Цитаты &gt; Omnia praeclara rara. Пример: Omnia praeclara rara. Цитата с подписью может быть оформлена так: &gt; Omnia praeclara rara. &gt; &gt; --- Cicero Пример: Omnia praeclara rara. — Cicero 7.3.8 Разделители Чтобы создать горизонтальную линию, можно использовать ---, *** или ___. Пример: 7.3.9 Таблицы Таблицы можно задать вручную при помощи дефисов - и вертикальных линий |; идеальная точность при этом не нужна. Перед таблицей обязательно оставляйте пустую строку, иначе волшебство не сработает. | Фрукты | Калории | | ----- | ---- | | Яблоко | 52 | | Апельсин | 47 | Пример: Фрукты Калории Яблоко 52 Апельсин 47 По умолчанию Markdown распечатывает таблицы так, как они бы выглядели в консоли. data(&quot;iris&quot;) head(iris) ## Sepal.Length Sepal.Width Petal.Length Petal.Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa Для дополнительного форматирования можно использовать функцию kable::knitr(): knitr::kable(iris[1:6, ], caption = &quot;Таблица knitr&quot;) Table 7.1: Таблица knitr Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa Интерактивную таблицу можно создать так: DT::datatable(iris[1:6,]) 7.3.10 Чек-листы - [x] Таблицы - [ ] Графики Пример: Таблицы Графики 7.3.11 Внутренние ссылки Удобны для навигации по документу. К названию любого раздела можно добавить {#id}. [Вернуться к чек-листам](#id) Пример: Вернуться к чек-листам 7.3.12 Графики Markdown позволяет встраивать любые графики. library(ggplot2) ggplot(aes(x = Sepal.Length, y = Petal.Length, col = Species), data = iris) + geom_point(show.legend = F) Для интерактивных графиков понадобится пакет plotly: library(plotly) plot_ly(data=iris, x = ~Sepal.Length, y = ~Petal.Length, color = ~Species) 7.3.13 Математические формулы Пишутся с использованием синтаксиса LaTeX, о котором можно прочитать подробнее здесь. Формулы заключаются в одинарный $, если пишутся в строку, и в двойной $$, если отдельным блоком. \\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta Вот так это выглядит в тексте: \\(\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\). А вот так – блоком: \\[\\cos (2\\theta) = \\cos^2 \\theta - \\sin^2 \\theta\\] 7.3.14 Библиография Чтобы привязать библиографию, нужно указать имя файла в шапке yaml. --- bibliography: bibliography.bib --- Дальше, чтобы добавить ссылку, достаточно ввести ключ публикации после @ (в квадратных скобках, чтобы публикация отражалась в круглых): [@wickham2016]. Пример: (Wickham and Grolemund 2017). 7.3.15 Смайлы Удобнее вставлять через визуальный редактор (“шестеренка” &gt; Use Visual Editor), но можно и без него: # devtools::install_github(&quot;hadley/emo&quot;) library(emo) emo::ji(&quot;apple&quot;) ## 🍎 Код можно записать в строку, тогда смайл появится в тексте: 💀.32 7.4 Публикация html To be supplied later. Литература "],["регулярные-выражения.html", "Тема 8 Регулярные выражения 8.1 Regex в базовом R 8.2 Литералы и классы 8.3 Якоря 8.4 Метасимволы 8.5 Экранирование 8.6 Квантификация 8.7 Жадная и ленивая квантификация 8.8 Regex в stringr: основы 8.9 str_detect() и str_count() 8.10 str_extract() 8.11 str_subset() и str_match() 8.12 str_replace 8.13 str_split", " Тема 8 Регулярные выражения Есть старая шутка, ее приписывают программисту Джейми Завински: если у вас есть проблема, и вы собираетесь ее решать при помощи регулярных выражений, то у вас две проблемы. Регулярные выражения – это формальный язык, который используется для того, чтобы находить, извлекать и заменять части текста. Регулярные выражения (regex, regexp) объединяют буквальные символы (литералы) и метасимволы (символы-джокеры, англ. wildcard characters). Для поиска используется строка-образец (англ. pattern, по-русски её часто называют “шаблоном”, “маской”), которая задает правило поиска. Строка замены также может содержать в себе специальные символы. Отличный путеводитель по миру регулярных выражений можно найти здесь. 8.1 Regex в базовом R В базовом R за работу со строками отвечают, среди прочего, такие функции, как grep() и grepl(). При этом grepl() возвращает TRUE, если шаблон найден в соответствующей символьной строке, а grep() возвращает вектор индексов символьных строк, содержащих паттерн. Обеим функциям необходим аргумент pattern и аргумент x, где pattern - регулярное выражение, по которому производится поиск, а аргумент x - вектор символов, по которым следует искать совпадения. Функция gsub() позволяет производить замену и требует также аргумента replacement. 8.2 Литералы и классы Буквальные символы – это то, что вы ожидаете увидеть (или не увидеть – для управляющих и пробельных символов); можно сказать, что это символы, которые ничего не “имеют в виду”. Их можно объединять в классы при помощи квадратных скобок, например, так: [abc]. vec &lt;- c(&quot;a&quot;, &quot;d&quot;, &quot;c&quot;) grepl(&quot;[abc]&quot;, vec) ## [1] TRUE FALSE TRUE grep(&quot;[abc]&quot;, vec) ## [1] 1 3 Для некоторых классов есть специальные обозначения. Класс Эквивалент Значение [:upper:] [A-Z] Символы верхнего регистра [:lower:] [a-z] Символы нижнего регистра [:alpha:] [[:upper:][:lower:]] Буквы [:digit:] [0-9], т. е. \\d Цифры [:alnum:] [[:alpha:][:digit:]] Буквы и цифры [:word:] [[:alnum:]_], т. е. Символы, образующие «слово» [:punct:] [-!“#$%&amp;’()*+,./:;&lt;=&gt;?@[\\]_`{|}~] Знаки пунктуации [:blank:] [\\s\\t] Пробел и табуляция [:space:] [[:blank:]\\v\\r\\n\\f], т. е. \\s Пробельные символы [:cntrl:] Управляющие символы (перевод строки, табуляция и т.п.) [:graph:] Печатные символы [:print:] Печатные символы с пробелом Эти классы тоже можно задавать в качестве паттерна. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд1&quot;, &quot;0зебра&quot;) gsub( &quot;[[:digit:]]&quot;, &quot;&quot;, vec) ## [1] &quot;жираф&quot; &quot;верблюд&quot; &quot;зебра&quot; В качестве классов можно рассматривать и следующие обозначения: Представление Эквивалент Значение \\d [0-9] Цифра \\D [^\\\\d] Любой символ, кроме цифры \\w [A-Za-zА-Яа-я0-9_] Символы, образующие «слово» (буквы, цифры и символ подчёркивания) \\W [^\\\\w] Символы, не образующие «слово» \\s [ \\t\\v\\r\\n\\f] Пробельный символ \\S [^\\\\s] Непробельный символ gsub( &quot;\\\\d&quot;, &quot;&quot;, vec) # вторая косая черта &quot;экранирует&quot; первую ## [1] &quot;жираф&quot; &quot;верблюд&quot; &quot;зебра&quot; Внутри квадратных скобор знак ^ означает отрицание: gsub( &quot;[^[:digit:]]&quot;, &quot;&quot;, vec) ## [1] &quot;&quot; &quot;1&quot; &quot;0&quot; 8.3 Якоря Якоря позволяют искать последовательности символов в начале или в конце строки. Знак ^ (вне квадратных скобок!) означает начало строки, а знак $ – конец. Мнемоническое правило: First you get the power (^) and then you get the money ($). vec &lt;- c(&quot;The spring is a lovely time&quot;, &quot;Fall is a time of peace&quot;) grepl(&quot;time$&quot;, vec) ## [1] TRUE FALSE 8.4 Метасимволы Все метасимволы представлены в таблице ниже. Описание Символ открывающая квадратная скобка [ закрывающая квадратная скобка ] обратная косая черта \\ карет ^ знак доллара $ точка . вертикальная черта | знак вопроса ? астериск * плюс + открывающая фигурная скобка { закрывающая фигурная скобка } открывающая круглая скобка ( закрывающая круглая скобка ) Квадратные скобки используются для создания классов, карет и знак доллара – это якоря, но карет внутри квадратных скобор может также быть отрицанием. Точка – это любой знак. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд1&quot;, &quot;0зебра&quot;) grep(&quot;.б&quot;, vec) ## [1] 2 3 8.5 Экранирование Если необходимо найти буквальную точку, буквальный знак вопроса и т.п., то используется экранирование: перед знаком ставится косая черта. Но так как сама косая черта – это метасимвол, но нужно две косые черты, первая из которых экранирует вторую. vec &lt;- c(&quot;жираф?&quot;, &quot;верблюд.&quot;, &quot;зебра&quot;) grep(&quot;\\\\?&quot;, vec) ## [1] 1 grepl(&quot;\\\\.&quot;, vec) ## [1] FALSE TRUE FALSE 8.6 Квантификация Квантификатор после символа, символьного класса или группы определяет, сколько раз предшествующее выражение может встречаться. Квантификатор может относиться более чем к одному символу в регулярном выражении, только если это символьный класс или группа. Представление Число повторений Эквивалент ? Ноль или одно {0,1} * Ноль или более {0,} + Одно или более {1,} Пример: vec &lt;- c(&quot;color&quot;, &quot;colour&quot;, &quot;colouur&quot;) grepl(&quot;ou?r&quot;, vec) # ноль или одно ## [1] TRUE TRUE FALSE grepl(&quot;ou+r&quot;, vec) # одно или больше ## [1] FALSE TRUE TRUE grepl(&quot;ou*r&quot;, vec) # ноль или больше ## [1] TRUE TRUE TRUE Точное число повторений (интервал) можно задать в фигурных скобках: Представление Число повторений {n} Ровно n раз {m,n} От m до n включительно {m,} Не менее m {,n} Не более n vec &lt;- c(&quot;color&quot;, &quot;colour&quot;, &quot;colouur&quot;, &quot;colouuuur&quot;) grepl(&quot;ou{1}r&quot;, vec) ## [1] FALSE TRUE FALSE FALSE grepl(&quot;ou{1,2}r&quot;, vec) ## [1] FALSE TRUE TRUE FALSE grepl(&quot;ou{,2}r&quot;, vec) # это включает и ноль! ## [1] TRUE TRUE TRUE FALSE Часто используется последовательность .* для обозначения любого количества любых символов между двумя частями регулярного выражения. 8.7 Жадная и ленивая квантификация В регулярных выражениях квантификаторам соответствует максимально длинная строка из возможных (квантификаторы являются жадными, англ. greedy). Это может оказаться значительной проблемой. Например, часто ожидают, что выражение &lt;.*&gt; найдёт в тексте теги HTML. Однако если в тексте есть более одного HTML-тега, то этому выражению соответствует целиком строка, содержащая множество тегов. vec &lt;- c(&quot;&lt;p&gt;&lt;b&gt;Википедия&lt;/b&gt; — свободная энциклопедия, в которой &lt;i&gt;каждый&lt;/i&gt; может изменить или дополнить любую статью.&lt;/p&gt;&quot;) gsub(&quot;&lt;.*&gt;&quot;, &quot;&quot;, vec) # все исчезло! ## [1] &quot;&quot; Чтобы этого избежать, надо поставить после квантификатора знак вопроса. Это сделает его ленивым. regex значение ?? 0 или 1, лучше 0 *? 0 или больше, как можно меньше +? 1 или больше, как можно меньше {n,m}? от n до m, как можно меньше Пример: gsub(&quot;&lt;.*?&gt;&quot;, &quot;&quot;, vec) # все получилось! ## [1] &quot;Википедия — свободная энциклопедия, в которой каждый может изменить или дополнить любую статью.&quot; 8.8 Regex в stringr: основы Пакет stringr не является частью tidyverse, хотя и разделяет его принципы33. Его надо загружать отдельно: library(stringr) Это очень удобный инструмент для работы со строками. Вот так можно узнать длину строки или объединить ее с другими строками: vec &lt;- c(&quot;жираф&quot;, &quot;верблюд&quot;) str_length(vec) ## [1] 5 7 str_c(&quot;красивый_&quot;, vec) ## [1] &quot;красивый_жираф&quot; &quot;красивый_верблюд&quot; Элементы вектора можно объединить в одну строку: str_c(vec, collapse = &quot;, &quot;) # теперь у них общие кавычки ## [1] &quot;жираф, верблюд&quot; С помощью str_sub() и str_sub_all() можно выбрать часть строки34. vec &lt;- c(&quot;жираф&quot;, &quot;верблюд&quot;) str_sub(vec, 1, 3) ## [1] &quot;жир&quot; &quot;вер&quot; str_sub(vec, 1, -2) ## [1] &quot;жира&quot; &quot;верблю&quot; Функции ниже меняют начертание с прописного на строчное или наоборот: VEC &lt;- str_to_upper(vec) VEC ## [1] &quot;ЖИРАФ&quot; &quot;ВЕРБЛЮД&quot; str_to_lower(VEC) ## [1] &quot;жираф&quot; &quot;верблюд&quot; str_to_title(vec) ## [1] &quot;Жираф&quot; &quot;Верблюд&quot; Одна из полезнейших функций в этом пакете – str_view(); она помогает увидеть, что поймало регулярное выражение – до того, как вы внесете какие-то изменения в строку. str_view(c(&quot;abc&quot;, &quot;a.c&quot;, &quot;bef&quot;), &quot;a\\\\.c&quot;) ## [2] │ &lt;a.c&gt; Например, с помощью этой функции можно убедиться, что вертикальная черта выступает как логический оператор “или”: str_view(c(&quot;grey&quot;, &quot;gray&quot;), &quot;gr(e|a)y&quot;) ## [1] │ &lt;grey&gt; ## [2] │ &lt;gray&gt; 8.9 str_detect() и str_count() Аналогом grepl() в stringr является функция str_detect() library(rcorpora) data(&quot;fruit&quot;) head(fruit) ## [1] &quot;apple&quot; &quot;apricot&quot; &quot;avocado&quot; &quot;banana&quot; &quot;bell pepper&quot; ## [6] &quot;bilberry&quot; str_detect(head(fruit), &quot;[aeiou]$&quot;) ## [1] TRUE FALSE TRUE TRUE FALSE FALSE # какая доля слов заканчивается на гласный? mean(str_detect(fruit, &quot;[aeiou]$&quot;)) ## [1] 0.35 # сколько всего слов заканчивается на гласный? sum(str_detect(fruit, &quot;[aeiou]$&quot;)) ## [1] 28 Отрицание можно задать двумя способами: data(&quot;words&quot;) no_vowels1 &lt;- !str_detect(words, &quot;[aeiou]&quot;) # слова без гласных no_vowels2 &lt;- str_detect(words, &quot;^[^aeiou]+$&quot;) # слова без гласных sum(no_vowels1 != no_vowels2) ## [1] 0 Логический вектор можно использовать для индексирования: words[!str_detect(words, &quot;[aeiou]&quot;)] ## [1] &quot;by&quot; &quot;dry&quot; &quot;fly&quot; &quot;mrs&quot; &quot;try&quot; &quot;why&quot; Эту функцию можно применять вместе с функцией filter() из пакета dplyr: library(dplyr) gods &lt;- corpora(which = &quot;mythology/greek_gods&quot;) df &lt;- tibble(god = as.character(gods$greek_gods), i = seq_along(god) ) df %&gt;% filter(str_detect(god, &quot;s$&quot;)) ## # A tibble: 18 × 2 ## god i ## &lt;chr&gt; &lt;int&gt; ## 1 Ares 3 ## 2 Artemis 4 ## 3 Dionysus 7 ## 4 Hades 8 ## 5 Hephaestus 9 ## 6 Hermes 11 ## 7 Zeus 14 ## 8 Chaos 17 ## 9 Chronos 18 ## 10 Erebus 19 ## 11 Eros 20 ## 12 Hypnos 21 ## 13 Uranus 22 ## 14 Phanes 24 ## 15 Pontus 25 ## 16 Tartarus 26 ## 17 Thanatos 28 ## 18 Nemesis 31 Вариацией этой функции является str_count(): str_count(as.character(gods$greek_gods), &quot;[Aa]&quot;) ## [1] 1 1 1 1 2 0 0 1 1 1 0 1 0 0 1 2 1 0 0 0 0 1 2 1 0 2 3 2 1 0 0 Эту функцию удобно использовать вместе с mutate() из dplyr: df %&gt;% mutate( vowels = str_count(god, &quot;[AEIOYaeiou]&quot;), consonants = str_count(god, &quot;[^AEIOYaeiou]&quot;) ) ## # A tibble: 31 × 4 ## god i vowels consonants ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Aphrodite 1 4 5 ## 2 Apollo 2 3 3 ## 3 Ares 3 2 2 ## 4 Artemis 4 3 4 ## 5 Athena 5 3 3 ## 6 Demeter 6 3 4 ## 7 Dionysus 7 3 5 ## 8 Hades 8 2 3 ## 9 Hephaestus 9 4 6 ## 10 Hera 10 2 2 ## # ℹ 21 more rows 8.10 str_extract() Функция str_extract() извлекает совпадения35. Рассмотрим ее работу на примере небольшого датасета из пакета stringr. data(&quot;sentences&quot;) length(sentences) ## [1] 720 head(sentences) ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;Rice is often served in round bowls.&quot; ## [6] &quot;The juice of lemons makes fine punch.&quot; Сначала зададим паттерн для поиска. colours &lt;- c(&quot; red&quot;, &quot;orange&quot;, &quot;yellow&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) colour_match &lt;- str_c(colours, collapse = &quot;|&quot;) colour_match ## [1] &quot; red|orange|yellow|green|blue|purple&quot; И применим к предложениями. Используем str_extract_all(), т.к. str_extract() возвращает только первое вхождение. has_colour &lt;- str_subset(sentences, colour_match) matches &lt;- str_extract_all(has_colour, colour_match) head(unlist(matches)) ## [1] &quot;blue&quot; &quot;blue&quot; &quot;blue&quot; &quot;yellow&quot; &quot;green&quot; &quot; red&quot; 8.11 str_subset() и str_match() Круглые скобки используются для группировки. Например, мы можем задать шаблон для поиска существительного или прилагательного с артиклем. noun &lt;- &quot;(a|the) ([^ ]+)&quot; # как минимум один непробельный символ после пробела has_noun &lt;- sentences %&gt;% str_subset(noun) %&gt;% head(10) has_noun ## [1] &quot;The birch canoe slid on the smooth planks.&quot; ## [2] &quot;Glue the sheet to the dark blue background.&quot; ## [3] &quot;It&#39;s easy to tell the depth of a well.&quot; ## [4] &quot;These days a chicken leg is a rare dish.&quot; ## [5] &quot;The box was thrown beside the parked truck.&quot; ## [6] &quot;The boy was there when the sun rose.&quot; ## [7] &quot;The source of the huge river is the clear spring.&quot; ## [8] &quot;Kick the ball straight and follow through.&quot; ## [9] &quot;Help the woman get back to her feet.&quot; ## [10] &quot;A pot of tea helps to pass the evening.&quot; Дальше можно воспользоваться уже известной функцией str_extract() или применить str_match. Результат будет немного отличаться: вторая функция вернет матрицу, в которой хранится не только сочетание слов, но и каждый компонент отдельно. has_noun %&gt;% str_extract(noun) ## [1] &quot;the smooth&quot; &quot;the sheet&quot; &quot;the depth&quot; &quot;a chicken&quot; &quot;the parked&quot; ## [6] &quot;the sun&quot; &quot;the huge&quot; &quot;the ball&quot; &quot;the woman&quot; &quot;a helps&quot; has_noun %&gt;% str_match(noun) ## [,1] [,2] [,3] ## [1,] &quot;the smooth&quot; &quot;the&quot; &quot;smooth&quot; ## [2,] &quot;the sheet&quot; &quot;the&quot; &quot;sheet&quot; ## [3,] &quot;the depth&quot; &quot;the&quot; &quot;depth&quot; ## [4,] &quot;a chicken&quot; &quot;a&quot; &quot;chicken&quot; ## [5,] &quot;the parked&quot; &quot;the&quot; &quot;parked&quot; ## [6,] &quot;the sun&quot; &quot;the&quot; &quot;sun&quot; ## [7,] &quot;the huge&quot; &quot;the&quot; &quot;huge&quot; ## [8,] &quot;the ball&quot; &quot;the&quot; &quot;ball&quot; ## [9,] &quot;the woman&quot; &quot;the&quot; &quot;woman&quot; ## [10,] &quot;a helps&quot; &quot;a&quot; &quot;helps&quot; Функция tidyr::extract работает похожим образом, но требует дать имена для каждого элемента группы. Этим удобно пользоваться, если ваши данные хранятся в виде тиббла. tibble(sentence = sentences) %&gt;% tidyr::extract( sentence, c(&quot;article&quot;, &quot;noun&quot;), &quot;(a|the) ([^ ]+)&quot;, remove = FALSE ) ## # A tibble: 720 × 3 ## sentence article noun ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 The birch canoe slid on the smooth planks. the smooth ## 2 Glue the sheet to the dark blue background. the sheet ## 3 It&#39;s easy to tell the depth of a well. the depth ## 4 These days a chicken leg is a rare dish. a chicken ## 5 Rice is often served in round bowls. &lt;NA&gt; &lt;NA&gt; ## 6 The juice of lemons makes fine punch. &lt;NA&gt; &lt;NA&gt; ## 7 The box was thrown beside the parked truck. the parked ## 8 The hogs were fed chopped corn and garbage. &lt;NA&gt; &lt;NA&gt; ## 9 Four hours of steady work faced us. &lt;NA&gt; &lt;NA&gt; ## 10 A large size in stockings is hard to sell. &lt;NA&gt; &lt;NA&gt; ## # ℹ 710 more rows 8.12 str_replace Функции str_replace() и str_replace_all() позволяют заменять совпадения на новые символы. x &lt;- c(&quot;apple&quot;, &quot;pear&quot;, &quot;banana&quot;) str_replace(x, &quot;[aeiou]&quot;, &quot;-&quot;) ## [1] &quot;-pple&quot; &quot;p-ar&quot; &quot;b-nana&quot; str_replace_all(x, &quot;[aeiou]&quot;, &quot;-&quot;) ## [1] &quot;-ppl-&quot; &quot;p--r&quot; &quot;b-n-n-&quot; Этим можно воспользоваться, если вы хотите, например, удалить из текста все греческие символы. Для стандартного греческого алфавита хватит [Α-Ωα-ω], но для древнегреческого этого, например, не хватит. Попробуем на отрывке из письма Цицерона Аттику, которое содержит греческий текст. cicero &lt;- &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀπεραντολογίας ἀηδοῦς! &quot; str_replace_all(cicero, &quot;[Α-Ωα-ω]&quot;, &quot;&quot;) ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀί ἀῦ! &quot; ὢ ἀί ἀῦ! Не все у нас получилось гладко. Попробуем иначе: str_replace_all(cicero, &quot;[\\u0370-\\u03FF]&quot;, &quot;&quot;) ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ὢ ἀ ἀῦ! &quot; Удалилась (буквально была заменена на пустое место) та диакритика, которая есть в новогреческом (ί). Но остались еще буквы со сложной диакритикой, которой современные греки не пользуются. no_greek &lt;- str_replace_all(cicero, &quot;[[\\u0370-\\u03FF][\\U1F00-\\U1FFF]]&quot;, &quot;&quot;) no_greek ## [1] &quot;nihil hāc sōlitūdine iūcundius, nisi paulum interpellāsset Amyntae fīlius. ! &quot; ! Мы молодцы. Избавились от этого непонятного греческого. На самом деле, конечно, str_replace хорош тем, что он позволяет производить осмысленные замены. Например, мы можем в оставшемся латинском текст заменить гласные с макроном (черточка, означающая долготу) на обычные гласные. str_replace_all(no_greek, c(&quot;ā&quot; = &quot;a&quot;, &quot;ū&quot; = &quot;u&quot;, &quot;ī&quot; = &quot;i&quot;, &quot;ō&quot; = &quot;o&quot;)) ## [1] &quot;nihil hac solitudine iucundius, nisi paulum interpellasset Amyntae filius. ! &quot; Красота. О более сложных заменах с перемещением групп можно посмотреть видео здесь и здесь. Это помогает даже в таком скорбном деле, как переоформление библиографии. 8.13 str_split Функция str_split() помогает разбить текст на предложения, слова или просто на бессмысленные наборы символов. Это важный этап подготовки текста для анализа, и проводится он нередко именно с применением регулярных выражений. sentences %&gt;% head(2) %&gt;% str_split(&quot; &quot;) ## [[1]] ## [1] &quot;The&quot; &quot;birch&quot; &quot;canoe&quot; &quot;slid&quot; &quot;on&quot; &quot;the&quot; &quot;smooth&quot; ## [8] &quot;planks.&quot; ## ## [[2]] ## [1] &quot;Glue&quot; &quot;the&quot; &quot;sheet&quot; &quot;to&quot; &quot;the&quot; ## [6] &quot;dark&quot; &quot;blue&quot; &quot;background.&quot; Но можно обойтись и без регулярных выражений. x &lt;- &quot;This is a sentence. This is another sentence.&quot; str_view_all(x, boundary(&quot;word&quot;)) ## Warning: `str_view()` was deprecated in stringr 1.5.0. ## ℹ Please use `str_view_all()` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. ## [1] │ &lt;This&gt; &lt;is&gt; &lt;a&gt; &lt;sentence&gt;. &lt;This&gt; &lt;is&gt; &lt;another&gt; &lt;sentence&gt;. str_view_all(x, boundary(&quot;sentence&quot;)) ## [1] │ &lt;This is a sentence. &gt;&lt;This is another sentence.&gt; Очень удобно, но убедитесь, что в вашем языке границы слов и предложения выглядят как у людей. С древнегреческим эта штука не справится (как делить на предложения греческие и латинские тексты, я рассказывала здесь): apology &lt;- c(&quot;νῦν δ&#39; ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ&#39; ὅς. “Τίς,” ἦν δ&#39; ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&quot;) str_view_all(apology, boundary(&quot;sentence&quot;)) ## [1] │ &lt;νῦν δ&#39; ἐπειδὴ ἀνθρώπω ἐστόν, τίνα αὐτοῖν ἐν νῷ ἔχεις ἐπιστάτην λαβεῖν; τίς τῆς τοιαύτης ἀρετῆς, τῆς ἀνθρωπίνης τε καὶ πολιτικῆς, ἐπιστήμων ἐστίν; οἶμαι γάρ σε ἐσκέφθαι διὰ τὴν τῶν ὑέων κτῆσιν. ἔστιν τις,” ἔφην ἐγώ, “ἢ οὔ;” “Πάνυ γε,” ἦ δ&#39; ὅς. &gt;&lt;“Τίς,” ἦν δ&#39; ἐγώ, “καὶ ποδαπός, καὶ πόσου διδάσκει;&gt; Полный крах 💩 https://r4ds.had.co.nz/strings.html↩︎ https://stringr.tidyverse.org/reference/str_sub.html↩︎ https://r4ds.had.co.nz/strings.html#extract-matches↩︎ "],["токенизация-и-лемматизация.html", "Тема 9 Токенизация и лемматизация 9.1 Токенизация в tidytext 9.2 Токенизация в tokenizers 9.3 Скипграмы 9.4 Лемматизация и частеречная разметка 9.5 Морфологическая разметка 9.6 Распределение частей речи 9.7 Совместная встречаемость слов 9.8 Синтаксическая разметка", " Тема 9 Токенизация и лемматизация Токенизация — процесс разделения текста на составляющие (их называют «токенами»). Токенами могут быть слова, символьные или словесные энграмы (n-grams), то есть сочетания символов или слов, даже предложения или параграфы. Все зависит от того, какие единицы вам нужны для анализа. Визуально процесс токенизации можно представить так36: Токенизировать можно в базовом R, и Jockers (2014) прекрасно показывает, как это можно делать. Но вы воспользуемся двумя пакетами, которые предназначены специально для работы с текстовыми данными и разделяют идеологию tidyverse. Оба пакета придется загрузить отдельно. library(tidyverse) library(tidytext) library(tokenizers) Для их освоения рекомендую изучить две книги: Silge and Robinson (2017) и @ Обе доступны бесплатно онлайн. Обе содержат множество примеров для английских текстов. Для разнообразия я покажу, как это работает, на русских текстах (потому что латинские и древнегреческие никому не интересны). Для анализа я снова (ср. урок 6) загружу “Бедную Лизу” Карамзина, на этот раз полностью. liza &lt;- readLines(con = &quot;files/karamzin_liza.txt&quot;) class(liza) ## [1] &quot;character&quot; length(liza) ## [1] 46 nchar(liza) ## [1] 1045 505 1524 218 285 999 254 658 1149 629 121 284 170 252 701 ## [16] 1091 632 936 1726 96 698 167 985 316 1323 1844 763 1104 617 959 ## [31] 1191 305 1433 119 830 414 257 1218 977 225 513 1695 132 214 251 ## [46] 267 liza[1] ## [1] &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей города сего, как я, потому что никто чаще моего не бывает в поле, никто более моего не бродит пешком, без плана, без цели -- куда глаза глядят -- по лугам и рощам, по холмам и равнинам. Всякое лето нахожу новые приятные места или в старых новые красоты. Но всего приятнее для меня то место, на котором возвышаются мрачные, готические башни Си...нова монастыря. Стоя на сей горе, видишь на правой стороне почти всю Москву, сию ужасную громаду домов и церквей, которая представляется глазам в образе величественного амфитеатра: великолепная картина, особливо когда светит на нее солнце, когда вечерние лучи его пылают на бесчисленных златых куполах, на бесчисленных крестах, к небу возносящихся! Внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым пескам, течет светлая река, волнуемая легкими веслами рыбачьих лодок или шумящая под рулем грузных стругов, которые плывут от плодоноснейших стран Российской империи и наделяют алчную Москву хлебом. &quot; 9.1 Токенизация в tidytext Прежде чем передать текст пакету tidytext, его следует трансформировать в тиббл – этого требуют функции на входе. По умолчанию столбец будет называться value, и я его сразу переименую. liza_tbl &lt;- as_tibble(liza) %&gt;% rename(text = value) liza_tbl ## # A tibble: 46 × 1 ## text ## &lt;chr&gt; ## 1 &quot; Может быть, никто из живущих в Москве не знает так хорошо окрестностей … ## 2 &quot; На другой стороне реки видна дубовая роща, подле которой пасутся мно… ## 3 &quot; Часто прихожу на сие место и почти всегда встречаю там весну; туда ж… ## 4 &quot; Но всего чаще привлекает меня к стенам Си...нова монастыря воспомина… ## 5 &quot; Саженях в семидесяти от монастырской стены, подле березовой рощицы, … ## 6 &quot; Отец Лизин был довольно зажиточный поселянин, потому что он любил ра… ## 7 &quot; \\&quot;Бог дал мне руки, чтобы работать, -- говорила Лиза, -- ты кормила … ## 8 &quot; Но часто нежная Лиза не могла удержать собственных слез своих -- ах!… ## 9 &quot; Прошло два года после смерти отца Лизина. Луга покрылись цветами, и … ## 10 &quot; Лиза, пришедши домой, рассказала матери, что с нею случилось. \\&quot;Ты х… ## # ℹ 36 more rows Этот текст мы передаем функции unnest_tokens(), которая принимает следующие аргументы: unnest_tokens( tbl, output, input, token = &quot;words&quot;, format = c(&quot;text&quot;, &quot;man&quot;, &quot;latex&quot;, &quot;html&quot;, &quot;xml&quot;), to_lower = TRUE, drop = TRUE, collapse = NULL, ... ) Аргумент token принимает следующие значения: “words” (default), “characters”, “character_shingles”, “ngrams”, “skip_ngrams”, “sentences”, “lines”, “paragraphs”, “regex”, “ptb” (Penn Treebank). Используя уже знакомую функцию map, можно запустить unnest_tokens() с разными аргументами: params &lt;- tribble( ~tbl, ~output, ~input, ~token, liza_tbl[1,], &quot;word&quot;, &quot;text&quot;, &quot;words&quot;, liza_tbl[1,], &quot;sentence&quot;, &quot;text&quot;, &quot;sentences&quot;, liza_tbl[1,], &quot;char&quot;, &quot;text&quot;, &quot;characters&quot;, ) params %&gt;% pmap(unnest_tokens) %&gt;% head() ## [[1]] ## # A tibble: 159 × 1 ## word ## &lt;chr&gt; ## 1 может ## 2 быть ## 3 никто ## 4 из ## 5 живущих ## 6 в ## 7 москве ## 8 не ## 9 знает ## 10 так ## # ℹ 149 more rows ## ## [[2]] ## # A tibble: 5 × 1 ## sentence ## &lt;chr&gt; ## 1 может быть, никто из живущих в москве не знает так хорошо окрестностей города… ## 2 всякое лето нахожу новые приятные места или в старых новые красоты. ## 3 но всего приятнее для меня то место, на котором возвышаются мрачные, готическ… ## 4 стоя на сей горе, видишь на правой стороне почти всю москву, сию ужасную гром… ## 5 внизу расстилаются тучные, густо-зеленые цветущие луга, а за ними, по желтым … ## ## [[3]] ## # A tibble: 846 × 1 ## char ## &lt;chr&gt; ## 1 м ## 2 о ## 3 ж ## 4 е ## 5 т ## 6 б ## 7 ы ## 8 т ## 9 ь ## 10 н ## # ℹ 836 more rows Следующие значения аргумента token требуют также аргумента n: params &lt;- tribble( ~tbl, ~output, ~input, ~token, ~n, liza_tbl[1,], &quot;ngram&quot;, &quot;text&quot;, &quot;ngrams&quot;, 3, liza_tbl[1,], &quot;shingles&quot;, &quot;text&quot;, &quot;character_shingles&quot;, 3 ) params %&gt;% pmap(unnest_tokens) %&gt;% head() ## [[1]] ## # A tibble: 157 × 1 ## ngram ## &lt;chr&gt; ## 1 может быть никто ## 2 быть никто из ## 3 никто из живущих ## 4 из живущих в ## 5 живущих в москве ## 6 в москве не ## 7 москве не знает ## 8 не знает так ## 9 знает так хорошо ## 10 так хорошо окрестностей ## # ℹ 147 more rows ## ## [[2]] ## # A tibble: 844 × 1 ## shingles ## &lt;chr&gt; ## 1 мож ## 2 оже ## 3 жет ## 4 етб ## 5 тбы ## 6 быт ## 7 ыть ## 8 тьн ## 9 ьни ## 10 ник ## # ℹ 834 more rows 9.2 Токенизация в tokenizers При работе с данными в текстовом формате unnest_tokens() опирается на пакет tokenizers, но tokenize_words требует на входе вектор, а не тиббл. Несколько полезных аргументов, о которых стоит помнить: strip_non_alphanum (удаляет пробельные символы и пунктуацию), strip_punct (удаляет пунктуацию), strip_numeric (удаляет числа). words_no_punct &lt;- tokenize_words(liza[1], strip_punct = T) words_no_punct[[1]][25:40] ## [1] &quot;поле&quot; &quot;никто&quot; &quot;более&quot; &quot;моего&quot; &quot;не&quot; &quot;бродит&quot; &quot;пешком&quot; &quot;без&quot; ## [9] &quot;плана&quot; &quot;без&quot; &quot;цели&quot; &quot;куда&quot; &quot;глаза&quot; &quot;глядят&quot; &quot;по&quot; &quot;лугам&quot; words_punct &lt;- tokenize_words(liza[1], strip_punct = F) words_punct[[1]][25:40] ## [1] &quot;не&quot; &quot;бывает&quot; &quot;в&quot; &quot;поле&quot; &quot;,&quot; &quot;никто&quot; &quot;более&quot; &quot;моего&quot; ## [9] &quot;не&quot; &quot;бродит&quot; &quot;пешком&quot; &quot;,&quot; &quot;без&quot; &quot;плана&quot; &quot;,&quot; &quot;без&quot; 9.3 Скипграмы Скипграмы, или скользящие окна, применяются в некоторых языковых моделях. skipgrams &lt;- tokenize_skip_ngrams(liza[1], n=3) skipgrams[[1]][1:10] ## [1] &quot;может&quot; &quot;может быть&quot; &quot;может никто&quot; ## [4] &quot;может быть никто&quot; &quot;может быть из&quot; &quot;может никто из&quot; ## [7] &quot;может никто живущих&quot; &quot;быть&quot; &quot;быть никто&quot; ## [10] &quot;быть из&quot; Функция считает все скипграмы длиной до трех включительно; это можно поправить: skipgrams &lt;- tokenize_skip_ngrams(liza[1], n=3, n_min = 3) skipgrams[[1]][1:10] ## [1] &quot;может быть никто&quot; &quot;может быть из&quot; &quot;может никто из&quot; ## [4] &quot;может никто живущих&quot; &quot;быть никто из&quot; &quot;быть никто живущих&quot; ## [7] &quot;быть из живущих&quot; &quot;быть из в&quot; &quot;никто из живущих&quot; ## [10] &quot;никто из в&quot; Важно выбрать правильное значение n при использовании энграм. Использование униграм быстрее и эффективнее, но мы не получаем информации о порядке слов. Чем выше n, тем больше сохраняется информации, но при этом резко увеличивается векторное пространство, а встречаемость отдельных токенов уменьшается37. Уже на этапе токенизации можно удалить стоп-слова (используя аргумент stopwords), но это имеет смысл, если текст изначально лемматизирован (то есть слова даны в начальной форме): это возможно, если леммы хранились, например, в исходном xml. Наша “Лиза” не лемматизирована, поэтому удалять стоп-слова мы не будем. 9.4 Лемматизация и частеречная разметка Помимо деления на токены, предварительная обработка текста может включать в себя лемматизацию, то есть приведение слов к начальной форме (лемме) и синтаксическую разметку. Для аннотации мы воспользуемся морфологическим и синтаксическим анализатором UDPipe (Universal Dependencies Pipeline), который существует в виде одноименного пакета в R. В отличие от других анализаторов, доступных в R, он позволяет работать со множеством языков (всего 65), для многих из которых представлено несколько моделей, обученных на разных данных. Прежде всего нужно выбрать и загрузить модель для (список). Модель GSD-Russian38, с которой мы начнем работу, обучена на статьях в Википедии, и, вероятно, не очень подойдет для наших задач – но можно попробовать. library(udpipe) # скачиваем модель в рабочую директорию # udpipe_download_model(language = &quot;russian-gsd&quot;) # загружаем модель russian_gsd &lt;- udpipe_load_model(file = &quot;russian-gsd-ud-2.5-191206.udpipe&quot;) Модели передается вектор с текстом. liza_ann &lt;- udpipe_annotate(russian_gsd, liza) Результат возвращается в формате CONLL-U; это широко применяемый формат представления результат морфологического и синтаксического анализа текстов. Формат разбора предложения в Conll-U выглядит так: Cтроки слов содержат следующие поля: 1. ID: индекс слова, целое число, начиная с 1 для каждого нового предложения; может быть диапазоном токенов с несколькими словами. 2. FORM: словоформа или знак препинания. 3. LEMMA: Лемма или основа словоформы. 4. UPOSTAG: универсальный тег части речи. 5. XPOSTAG: тег части речи для конкретного языка. 6. FEATS: список морфологических характеристик. 7. HEAD: заголовок текущего токена, который является либо значением ID, либо нулем (0). 8. DEPREL: Universal Stanford dependency relation к (root iff HEAD = 0) или определенному зависящему от языка подтипу. 9. DEPS: Список вторичных зависимостей. 10. MISC: любая другая аннотация. Для работы данные удобнее трансформировать в прямоугольный формат. liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-sentence, -paragraph_id) str(liza_df) ## tibble [6,447 × 12] (S3: tbl_df/tbl/data.frame) ## $ doc_id : chr [1:6447] &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; &quot;doc1&quot; ... ## $ sentence_id : int [1:6447] 1 1 1 1 1 1 1 1 1 1 ... ## $ token_id : chr [1:6447] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ token : chr [1:6447] &quot;Может&quot; &quot;быть&quot; &quot;,&quot; &quot;никто&quot; ... ## $ lemma : chr [1:6447] &quot;мочь&quot; &quot;быть&quot; &quot;,&quot; &quot;никто&quot; ... ## $ upos : chr [1:6447] &quot;VERB&quot; &quot;AUX&quot; &quot;PUNCT&quot; &quot;PRON&quot; ... ## $ xpos : chr [1:6447] &quot;VBC&quot; &quot;VB&quot; &quot;,&quot; &quot;DT&quot; ... ## $ feats : chr [1:6447] &quot;Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act&quot; &quot;Aspect=Imp|VerbForm=Inf&quot; NA &quot;Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing&quot; ... ## $ head_token_id: chr [1:6447] &quot;0&quot; &quot;4&quot; &quot;2&quot; &quot;10&quot; ... ## $ dep_rel : chr [1:6447] &quot;root&quot; &quot;cop&quot; &quot;punct&quot; &quot;nsubj&quot; ... ## $ deps : chr [1:6447] NA NA NA NA ... ## $ misc : chr [1:6447] &quot;SpacesBefore=\\\\s\\\\s\\\\s\\\\s&quot; &quot;SpaceAfter=No&quot; NA NA ... Выведем часть (!) столбцов для первого предложения: liza_df %&gt;% filter(doc_id == &quot;doc1&quot;) %&gt;% select(-sentence_id, -head_token_id, -deps, -dep_rel, -misc) %&gt;% DT::datatable() Если полистать эту таблицу, можно заметить несколько ошибок, например странное существительное “пешко” (наречие “пешком” понято как форма творительного падежа). Но, как уже говорилось, для некоторых языков, в том числе русского, в uppide представлено несколько моделей, некоторые из которых лучше справляются с текстами определенных жанров. Попробуем использовать другую модель, обученную на корпусе СинТагРус (сокр. от англ. Syntactically Tagged Russian text corpus, «синтаксически аннотированный корпус русских текстов»)39. # скачиваем модель в рабочую директорию # udpipe_download_model(language = &quot;russian-syntagrus&quot;) # загружаем модель russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) liza_ann &lt;- udpipe_annotate(russian_syntagrus, liza) liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-paragraph_id, -sentence, -xpos) liza_df %&gt;% filter(doc_id == &quot;doc1&quot;) %&gt;% select(-sentence_id, -head_token_id, -deps, -dep_rel, -misc) %&gt;% DT::datatable() Здесь “пешком” корректно обозначено как наречие; в целом, кажется, вторая модель лучше справилась с задачей. 9.5 Морфологическая разметка Морфологическая разметка, которую мы получили, дает возможность выбирать и группировать различные части речи. Например, имена и названия: в первом параграфе, который мы проанализировали, их всего 4, причем правильно опознано в качестве собственного имени название Симонова монастыря. propn &lt;- liza_df %&gt;% filter(upos == &quot;PROPN&quot;) propn[,2:6] ## # A tibble: 164 × 5 ## sentence_id token_id token lemma upos ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 8 Москве Москва PROPN ## 2 3 16 Си...нова Си...нов PROPN ## 3 4 12 Москву Москва PROPN ## 4 5 45 Москву Москва PROPN ## 5 2 11 Данилов Данилов PROPN ## 6 2 23 Воробьевы Воробьев PROPN ## 7 3 21 Коломенское Коломенский PROPN ## 8 9 32 Москва Москва PROPN ## 9 1 14 Лизы Лиза PROPN ## 10 2 13 Лиза Лиза PROPN ## # ℹ 154 more rows С помощью функции str_detect() можно выбрать конкретные формы, например, винительный падеж. liza_df %&gt;% filter(str_detect(feats, &quot;Case=Acc&quot;)) ## # A tibble: 558 × 11 ## doc_id sentence_id token_id token lemma upos feats head_token_id dep_rel ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 doc1 1 28 поле поле NOUN Anim… 26 obl ## 2 doc1 2 4 новые новый ADJ Anim… 6 amod ## 3 doc1 2 5 приятные прият… ADJ Anim… 6 amod ## 4 doc1 2 6 места место NOUN Anim… 3 obj ## 5 doc1 2 10 новые новый ADJ Anim… 11 amod ## 6 doc1 2 11 красоты красо… NOUN Anim… 6 conj ## 7 doc1 4 3 сей сей DET Case… 4 det ## 8 doc1 4 4 горе горе NOUN Anim… 1 obl ## 9 doc1 4 11 всю весь DET Case… 12 det ## 10 doc1 4 12 Москву Москва PROPN Anim… 9 nmod ## # ℹ 548 more rows ## # ℹ 2 more variables: deps &lt;chr&gt;, misc &lt;chr&gt; 9.6 Распределение частей речи Литературоведам может быть интересно распределение различных частей речи в повести: так, Бен Блатт задался целью проверить, применительно к англоязычной прозе, знаменитый афоризм Стивена Кинга о том, что «дорога в ад вымощена наречиями». Правда ли, что великие писатели реже используют наречия на -ly? Он получил достаточно любопытные результаты, в частности выяснилось, что Генри Мелвилл и Джейн Остин представляют собой скорее исключение из этого правила, но с двумя важными оговорками: во-первых, в 19 в. наречия в целом используют чаще, чем 20-м; а во-вторых, в признанных шедеврах отдельных авторов наречий, действительно, бывает меньше. Например, в романе Стейнбека «Зима тревоги нашей» их меньше всего. Больше всего наречий у авторов фанфиков, непрофессиональных писателей. Посчитать части речи (расшифровка тегов UPOS по ссылке) можно так: liza_df %&gt;% group_by(upos) %&gt;% count() %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% arrange(-n) ## # A tibble: 14 × 2 ## # Groups: upos [14] ## upos n ## &lt;chr&gt; &lt;int&gt; ## 1 NOUN 1063 ## 2 VERB 988 ## 3 PRON 619 ## 4 ADP 520 ## 5 ADJ 408 ## 6 ADV 321 ## 7 DET 272 ## 8 CCONJ 230 ## 9 PART 187 ## 10 PROPN 164 ## 11 SCONJ 142 ## 12 AUX 71 ## 13 NUM 34 ## 14 INTJ 30 Столбиковая диаграмма позволяет наглядно представить такого рода данные: liza_df %&gt;% group_by(upos) %&gt;% count() %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% ggplot(aes(x = reorder(upos, n), y = n, fill = upos)) + geom_bar(stat = &quot;identity&quot;, show.legend = F) + coord_flip() + theme_bw() Обратите внимание на некоторое заметное число междометий. Какое междометие встречается здесь чаще всего, можно догадаться 😊 Можно отобрать наиболее частотные слова для любой части речи. nouns &lt;- liza_df %&gt;% filter(upos %in% c(&quot;NOUN&quot;, &quot;PROPN&quot;)) %&gt;% count(lemma) %&gt;% arrange(-n) head(nouns, 10) ## # A tibble: 10 × 2 ## lemma n ## &lt;chr&gt; &lt;int&gt; ## 1 Лиза 107 ## 2 Эраст 41 ## 3 сердце 24 ## 4 глаз 23 ## 5 мать 21 ## 6 человек 18 ## 7 день 17 ## 8 рука 17 ## 9 друг 16 ## 10 слеза 15 library(wordcloud) ## Loading required package: RColorBrewer library(RColorBrewer) pal &lt;- RColorBrewer::brewer.pal(7, &quot;Dark2&quot;) nouns %&gt;% with(wordcloud(lemma, n, max.words = 50, colors = pal)) Можно заметить, что в тексте часто встречаются слова “мать”, “матушка”, “старушка” (42 раза): Лизина мать упоминается в тексте так же часто, как Эраст, и чаще, чем слово “сердце” (24). В любовной повести Карамзин чуть ли не чаще говорит о матери героини, чем о её возлюбленном! 9.7 Совместная встречаемость слов Функция cooccurence() из пакета udpipe позволяет выяснить, сколько раз некий термин встречается совместно с другим термином, например: слова встречаются в одном и том же документе/предложении/параграфе; слова следуют за другим словом; слова находятся по соседству с другим словом на расстоянии n слов. Код ниже позволяет выяснить, какие слова встречаются в одном предложении: x &lt;- subset(liza_df, upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;)) cooc &lt;- cooccurrence(x, term = &quot;lemma&quot;, group = c(&quot;doc_id&quot;, &quot;sentence_id&quot;)) head(cooc) ## term1 term2 cooc ## 1 молодой человек 8 ## 2 глаз друг 6 ## 3 последний раз 6 ## 4 глаз слеза 6 ## 5 друг час 6 ## 6 день другой 4 Этот результат легко визуализировать, используя пакет ggraph: library(igraph) library(ggraph) wordnetwork &lt;- head(cooc, 30) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;pink&quot;) + geom_node_text(aes(label = name), col = &quot;darkblue&quot;, size = 4) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Совместная встречаемость слов&quot;, subtitle = &quot;Существительные и прилагательные&quot;) Милый друг, глубокий пруд. Грустная история! Чтобы узнать, какие слова чаще стоят рядом, используем ту же функцию, но с другими аргументами40: cooc &lt;- cooccurrence(x$lemma, relevant = x$upos %in% c(&quot;NOUN&quot;, &quot;ADJ&quot;), skipgram = 1) head(cooc) ## term1 term2 cooc ## 1 молодой человек 8 ## 2 берег река 4 ## 3 друг друг 4 ## 4 последний раз 4 ## 5 Лиза мать 4 ## 6 глаз слеза 3 wordnetwork &lt;- head(cooc, 30) wordnetwork &lt;- graph_from_data_frame(wordnetwork) ggraph(wordnetwork, layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_colour = &quot;salmon&quot;, edge_alpha=0.7), show.legend = F) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 4, angle=15, repel = T) + theme_graph(base_family = &quot;Arial Narrow&quot;) + labs(title = &quot;Слова, стоящие рядом в тексте&quot;, subtitle = &quot;Существительные и прилагательные&quot;) 9.8 Синтаксическая разметка Для анализа выберем одно предложение. liza_synt &lt;- liza_ann %&gt;% as.data.frame() liza_synt_sel &lt;- liza_synt %&gt;% filter(doc_id == &quot;doc17&quot;, sentence_id == 15) %&gt;% filter(token != &quot;-&quot;) liza_synt_sel[,c(&quot;token&quot;, &quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)] ## token token_id head_token_id dep_rel ## 1 Лиза 3 5 nsubj ## 2 не 4 5 advmod ## 3 договорила 5 0 root ## 4 речи 6 5 obl ## 5 своей 7 6 det ## 6 . 8 5 punct Связь между токенами определяется в полях token_id и head_token_id, отношение зависимости определено в dep_rel. Корневой токен имеет значение 0, то есть ни от чего не зависит. Графически изобразить связи поможет пакет textplot. library(textplot) textplot_dependencyparser(liza_synt_sel) Построить граф можно и при помощи библиотек igraph и ggraph: liza_synt_sel &lt;- liza_synt %&gt;% filter(doc_id == &quot;doc17&quot;, sentence_id == 1) e &lt;- subset(liza_synt_sel, head_token_id != 0, select = c(&quot;token_id&quot;, &quot;head_token_id&quot;, &quot;dep_rel&quot;)) e ## token_id head_token_id dep_rel ## 2 2 1 obl ## 3 3 7 punct ## 4 4 7 cc ## 5 5 6 amod ## 6 6 7 nsubj ## 7 7 1 conj ## 8 8 9 advmod ## 9 9 7 xcomp ## 10 10 1 punct e$label &lt;- e$dep_rel gr &lt;- graph_from_data_frame(e, vertices = liza_synt_sel[, c(&quot;token_id&quot;, &quot;token&quot;, &quot;lemma&quot;, &quot;upos&quot;, &quot;xpos&quot;, &quot;feats&quot;)], directed = TRUE) a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.1, &quot;inches&quot;)) ggraph(gr, layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha=0.7, label = dep_rel), arrow = a, end_cap = circle(0.07, &#39;inches&#39;), show.legend = F, label_colour = &quot;grey30&quot;, edge_color = &quot;grey&quot;) + geom_node_point(color = &quot;lightblue&quot;, size = 4) + theme_void(base_family = &quot;&quot;) + geom_node_text(ggplot2::aes(label = token), nudge_y = 0.2) Литература "],["эмоциональная-тональность.html", "Тема 10 Эмоциональная тональность 10.1 Анализ тональности 10.2 Подходы 10.3 Тезаурусы 10.4 Лексиконы для русского языка 10.5 Анализ тональности с Tidy Data 10.6 Подготовка текста 10.7 Модификация лексикона 10.8 Соединение лексикона и текста 10.9 Тональность на оси времени 10.10 Для других языков 10.11 Виды эмоций", " Тема 10 Эмоциональная тональность 10.1 Анализ тональности Анализ тональности текста (англ. Sentiment analysis) — задача компьютерной лингвистики, заключающаяся в определении эмоциональной окраски (тональности) текста и, в частности, в выявлении эмоциональной оценки авторов по отношению к объектам, описываемым в тексте. В целом, задача анализа тональности текста эквивалентна задаче классификации текста, где категориями текстов могут быть тональные оценки (позитивная, негативная или нейтральная). 10.2 Подходы Сделав большое обобщение, можно разделить существующие подходы на следующие категории: подходы, основанные на правилах; подходы, основанные на словарях; машинное обучение с учителем; машинное обучение без учителя. В этом уроке мы будем работать только со словарями. Подробнее о других подходах можно прочитать здесь. 10.3 Тезаурусы Подходы, основанные на словарях, используют так называемые тональные словари (англ. affective lexicons) для анализа текста. В простом виде тональный словарь представляет из себя список слов со значением тональности для каждого слова. Сравнивая текст (или отрывок текста) со словарем, мы можем вычислить тональность для всего текста (или отрывка). Словари эмоциональной тональности размечаются вручную, полуавтоматически или автоматически на основании уже существующих тезаурусов. В основном они содержат лексику из соцсетей, отзывов, Википедии, и поэтому не очень подходят для анализа литературных текстов, особенно написанных 100-200 лет назад. Разные тезаурусы используют разные шкалы: бинарную: negative / positive (-1 / 1) тринарную: бинарная + 0 (neutral) ранжированную: например, от -5 до 5 В некоторых случаях дополнительно вводятся различия между оценочной лексикой (“неряшливый”) и негативным фактом (“кража”) и т.п. 10.4 Лексиконы для русского языка Установка пакета с лексиконами. remotes::install_github(&quot;dmafanasyev/rulexicon&quot;) Начало работы. library(rulexicon) library(tidyverse) library(tidytext) 10.4.1 Chen &amp; Skiena Русский язык входит в языков, для которых Й. Чен и С. Скиена собрали оценочную лексику (Chen and Skiena 2014). Их лексикон построен на основе графа знаний, связывающего слова на разных языках (на основе Wiktionary, Google Translate, транслитерационных ссылок и WordNet). Слова оцениваются по бинарной шкале ( -1 / 1). set.seed(0211) chen_skiena &lt;- hash_sentiment_chen_skiena sample_n(chen_skiena, 10) ## token score ## 1: пустошь -1 ## 2: революционный 1 ## 3: расизм -1 ## 4: медленный -1 ## 5: лекарство 1 ## 6: поддержка 1 ## 7: стабилизировать 1 ## 8: лисица -1 ## 9: мягкость 1 ## 10: примесь -1 10.4.2 RuSentLex 2016 Для русского языка в свободном доступе находится РуСентиЛекс (“Создание лексикона оценочных слов русского языка РуСентилекс” 2016). Он содержит около 15000 уникальных слов или фраз, среди которых оценочные слова, а также слова и выражения, не передающие оценочное отношения автора, но имеющие положительную или отрицательную ассоциацию (коннотацию). Возможные значения переменной sentiment: neutral, positive, negative, positive/negative. set.seed(1102) rusenti2016 &lt;- hash_rusentilex_2016 sample_n(rusenti2016, 10) ## token speech.part lemma sentiment source ## 1 любить Verb любить neutral fact ## 2 самозабвенный Adj самозабвенный positive feeling ## 3 потянуть Verb потянуть neutral fact ## 4 рожа Noun рожа negative opinion ## 5 острозаразный Adj острозаразный negative fact ## 6 тюфяк Noun тюфяк neutral fact ## 7 просчитаться Verb просчитаться negative fact ## 8 перехлестнуть Verb перехлестнуть positive/negative opinion ## 9 беспокойный Adj беспокойный negative feeling ## 10 задеревенеть Verb задеревенеть negative opinion ## ambiguity ## 1 ЛЮБИТЬ (НУЖДАТЬСЯ В УСЛОВИЯХ) ## 2 ## 3 ТЯНУТЬ (ТАЩИТЬ НАПРАВЛЯЯ КУДА-ЛИБО) ## 4 ЛИЦО ЧЕЛОВЕКА ## 5 ## 6 МАТРАЦ ## 7 ## 8 ОБУЯТЬ, ОБУРЕВАТЬ ## 9 ## 10 ДЕРЕВЕНЕТЬ, КОСТЕНЕТЬ (НЕМЕТЬ, ВОЗМОЖНО ОТВЕРДЕВАЯ) При работе с этим лексиконом следует учитывать, что для отдельных слов он содержит несколько вхождений, как положительных, так и отрицательных, например: rusenti2016 %&gt;% filter(token == &quot;нежный&quot;) ## token speech.part lemma sentiment source ambiguity ## 1 нежный Adj нежный negative opinion ХРУПКИЙ (СЛИШКОМ СЛАБЫЙ, НЕЖНЫЙ) ## 2 нежный Adj нежный positive opinion ЛАСКОВЫЙ ## 3 нежный Adj нежный positive opinion МЯГКИЙ, НЕЖНЫЙ НА ОЩУПЬ ## 4 нежный Adj нежный positive opinion НЕЖНЫЙ ПО ЗВУЧАНИЮ 10.4.3 AFINN Словарь AFINN содержит 7268 оценочных слов. Их тональность оценивается по шкале от -5 (крайне негативная) до 5 (в высшей степени положительная). Например, слово “адский” имеет оценку -5, а слово “ангельский” – +5. set.seed(0211) afinn &lt;- hash_sentiment_afinn_ru sample_n(afinn, 10) ## token score ## 1: экстатический 1.7 ## 2: знаковый 1.7 ## 3: счастливчик 5.0 ## 4: суматошный -3.3 ## 5: гад -5.0 ## 6: выразительный 5.0 ## 7: жутковатый -5.0 ## 8: креативность 5.0 ## 9: обнадёживать 2.5 ## 10: привлекательно 5.0 10.4.4 NRC Переведенная версия списка положительных и отрицательных слов Mohammad &amp; Turney (2010)41. Таблица содержит 5179 слов с не нейтральными оценками. Бинарная шкала: -1 / 1. set.seed(1102) nrc &lt;- hash_sentiment_nrc_emolex_ru sample_n(nrc, 10) ## token score ## 1: энциклопедия 1 ## 2: подходящий 1 ## 3: издевательство -1 ## 4: пленительный 1 ## 5: утопический 1 ## 6: недоброжелательство -1 ## 7: вероломный -1 ## 8: вирулентность -1 ## 9: победоносный 1 ## 10: незначительность -1 10.5 Анализ тональности с Tidy Data (Silge and Robinson 2017), говоря об анализе эмоциональной тональности в духе tidy data, предлагают следующую иллюстрацию: Авторы предостерегают, впрочем, что современные лексиконы могут быть не слишком информативны применительно к классической литературе (в книге анализируются романы Джейн Остин). Мы попробуем, тем не менее, подвергнуть “сентиментальному анализу” сентиментальную прозу Карамзина. 10.6 Подготовка текста Прежде всего текст необходимо токенизировать, лемматизировать и привести в опрятный формат, как мы делали в предыдущем уроке. library(udpipe) liza &lt;- readLines(con = &quot;files/karamzin_liza.txt&quot;) russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) liza_ann &lt;- udpipe_annotate(russian_syntagrus, liza) liza_df &lt;- as_tibble(liza_ann) %&gt;% select(-paragraph_id, -sentence, -xpos) Разделим весь текст “Лизы” на отрывки по 100 слов: это позволит проверить, как меняется эмоциональная тональность произведения по мере развития сюжета. liza_tbl &lt;- as_tibble(liza_ann) %&gt;% filter(upos != &quot;PUNCT&quot;) %&gt;% select(lemma) %&gt;% rename(token = lemma) %&gt;% mutate(chunk = round(((row_number() + 50) / 100), 0)) liza_tbl ## # A tibble: 5,049 × 2 ## token chunk ## &lt;chr&gt; &lt;dbl&gt; ## 1 мочь 1 ## 2 быть 1 ## 3 никто 1 ## 4 из 1 ## 5 жить 1 ## 6 в 1 ## 7 Москва 1 ## 8 не 1 ## 9 знать 1 ## 10 так 1 ## # ℹ 5,039 more rows В тексте чуть более 5000 слов, у нас получился 51 отрывок. 10.7 Модификация лексикона Для анализа эмоциональной тональности возьмем лексикон AFINN, доступный в пакете rulexicon. Слово “старый” имеет в этом лексиконе отрицательную оценку, что не соответствует словоупотреблению Карамзина, и мы его удалили; слову “чувствительный” поменяли знак с минуса на плюс, поскольку для автора “Бедной Лизы” это скорее положительное качество. Код ниже показывает, как вносятся подобные изменения: lex &lt;- hash_sentiment_afinn_ru lex &lt;- lex %&gt;% filter(token != &quot;старый&quot;) lex &lt;- lex %&gt;% mutate_at(vars(score), ~ case_when(token == &quot;чувствительный&quot; ~ 1.7, TRUE ~ .)) 10.8 Соединение лексикона и текста Стоп-слова, то есть слова, не несущие никакой смысловой нагрузки, нам не нужны, но удалять их отдельно нет смысла: мы соединим, при помощи функции inner_join() (см. предыдущие уроки), наш текст с одним из лексиконов, и это само по себе отфильтрует ту лексику, которая может быть потенциально интересна. Напомню, что inner_join() работает так: liza_sent &lt;- liza_tbl %&gt;% inner_join(lex) liza_sent ## # A tibble: 461 × 3 ## token chunk score ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 хорошо 1 5 ## 2 новый 1 1.7 ## 3 приятный 1 5 ## 4 новый 1 1.7 ## 5 красота 1 5 ## 6 приятный 1 5 ## 7 мрачный 1 -5 ## 8 горе 1 -5 ## 9 ужасный 1 -5 ## 10 величественный 1 3.3 ## # ℹ 451 more rows Сложив положительно и отрицательно окрашенную лексику для каждого отрывка, получаем значение, позволяющее судить о доминирующей тональности: liza_chunk_sent &lt;- liza_sent %&gt;% group_by(chunk) %&gt;% summarise(sum = sum(score)) %&gt;% arrange(sum) head(liza_chunk_sent, 10) ## # A tibble: 10 × 2 ## chunk sum ## &lt;dbl&gt; &lt;dbl&gt; ## 1 5 -43.3 ## 2 34 -35.8 ## 3 31 -25 ## 4 38 -20 ## 5 42 -20 ## 6 50 -20 ## 7 3 -15.7 ## 8 51 -15 ## 9 4 -14.2 ## 10 46 -13.3 Довольно неожиданно, что самый негативный отрывок находится не в конце повести, ближе к трагической ее развязке, а почти в начале (отрывок 5, ср. отрывки 3 и 4 рядом). Представим эмоционально окрашенную лексику отрывков 3-5 в виде сравнительного облака слов: library(reshape2) library(wordcloud) # добавляем новый столбец для удобства визуализации liza_sent_class &lt;- liza_sent %&gt;% mutate(tone = case_when( score &gt;= 0 ~ &quot;pos&quot;, score &lt; 0 ~ &quot;neg&quot;)) set.seed(0211) liza_sent_class %&gt;% filter(chunk %in% c(3, 4, 5)) %&gt;% count(token, tone, sort = T) %&gt;% acast(token ~ tone, value.var = &quot;n&quot;, fill = 0) %&gt;% comparison.cloud(colors = c(&quot;grey20&quot;, &quot;grey80&quot;), max.words = 99) Здесь видно, что негативная тональность в этой части не связана с судьбой героев: об этом говорят такие слова, как “лютый”, “враг”, “свирепый”. Рассказчик, глядя на заброшенный Симонов монастырь, вспоминает о “печальной истории” Москвы. Таким образом, с количественной точки зрения, самые мрачный фрагмент повести посвящен не судьбе бедной девушки, а “глухому стону времен”: Карамзин-историк уже переигрывает Карамзина-новелиста. Приведем небольшой отрывок из этой части повести: Иногда на вратах храма рассматриваю изображение чудес, в сем монастыре случившихся, там рыбы падают с неба для насыщения жителей монастыря, осажденного многочисленными врагами; тут образ богоматери обращает неприятелей в бегство. Все сие обновляет в моей памяти историю нашего отечества — печальную историю тех времен, когда свирепые татары и литовцы огнем и мечом опустошали окрестности российской столицы и когда несчастная Москва, как беззащитная вдовица, от одного бога ожидала помощи в лютых своих бедствиях. 10.9 Тональность на оси времени Таблица, которую мы подготовили, позволяет наглядно показать, как меняется тональность во времени – разумеется, речь идет о повествовательном времени, которое измеряется не в минутах, а в словах. Каждый отрывок, напомню, – это 100 слов. liza_chunk_sent &lt;- liza_chunk_sent %&gt;% mutate(tone = case_when( sum &gt;= 0 ~ &quot;pos&quot;, sum &lt; 0 ~ &quot;neg&quot;)) library(ggplot2) ggplot(liza_chunk_sent, aes(chunk, sum, fill = tone)) + geom_col(show.legend = F) График получился весьма осмысленным. Мы уже сказали выше про отрывки 3-4. Дальше немного скорби в отрывке 8 посвящено покойному отцу Лизы. В 11-м отрывке отразилась тревога матери за судьбу дочери: “коварно”, “обидеть”, “дурной” вносят вклад в настроение этого фрагмента. Это достаточно характерно для сентиментальной прозы с ее противопоставлением пороков городской жизни и пасторальных добродетелей. У меня всегда сердце бывает не на своем месте, когда ты ходишь в город; я всегда ставлю свечу перед образ и молю господа бога, чтобы он сохранил тебя от всякой беды и напасти. В отрывке 15 несколько негативных слов имеют перед собой отрицания (“не подозревая”, “никакого худого намерения” и т.п.), поэтому к числу отрицательно окрашенных он отнесен ошибочно. К сожалению, это недостаток подхода, основанного на словарях, не принимающего в учет синтаксические связи в предложении. Еще два минимума: отрывки 31 и 34. В первом из них Лиза встревожена вестью о возможном замужестве с сыном крестьянина. Отрывок 34 – это падение Лизы: Грозно шумела буря, дождь лился из черных облаков — казалось, что натура сетовала о потерянной Лизиной невинности. На графике видно, что это место гораздо более эмоционально, чем эпизод самоубийства Лизы: именно после знаменитых карамзинских многоточий и тире события устремляются к трагическому финалу. О самой смерти девушки Карамзин говорит, конечно, с грустью, но без надрыва: “Тут она бросилась в воду”. 38, 39, 42 – Эраст отправляется на войну. Все, как это принято у Карамзина, плачут, что зафиксировал и наш график. Наконец, в отрывках 49-51 доминирует тема смерти: library(RColorBrewer) pal &lt;- RColorBrewer::brewer.pal(5, &quot;Dark2&quot;) liza_sent_class %&gt;% filter(chunk %in% c(49:51)) %&gt;% filter(tone == &quot;neg&quot;) %&gt;% count(token, sort = T) %&gt;% with(wordcloud(token, n, max.words = 100, colors = pal)) Следует отметить, что часть этих слов относится не к самой девушке, а к ее матери. 10.10 Для других языков Для языков, которые используют латиницу, в R есть отличный пакет под названием syuzhet. Его разработал известный цифровой литературовед Мэтью Джокерс42. Название пакета, как говорит его разработчик, подсмотрено у русских формалистов Виктора Шкловского и Владимира Проппа. Возможности и ограничения этого пакета обсуждались в специальной литературе43. Для анализа возьмем стихотворение Роберта Фроста “Медведи” (перевод). library(syuzhet) my_example_text &lt;- &quot;The bear puts both arms around the tree above her And draws it down as if it were a lover And its choke cherries lips to kiss good-bye, Then lets it snap back upright in the sky. Her next step rocks a boulder on the wall (She&#39;s making her cross-country in the fall). Her great weight creaks the barbed-wire in its staples As she flings over and off down through the maples, Leaving on one wire moth a lock of hair. Such is the uncaged progress of the bear. The world has room to make a bear feel free; The universe seems cramped to you and me. Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests. He paces back and forth and never rests The toe-nail click and shuffle of his feet, The telescope at one end of his beat - And at the other end the microscope, Two instruments of nearly equal hope, And in conjunction giving quite a spread. Or if he rests from scientific tread, &#39;Tis only to sit back and sway his head Through ninety odd degrees of arc, it seems, Between two metaphysical extremes. He sits back on his fundamental butt With lifted snout and eyes (if any) shut, (he almost looks religious but he&#39;s not), And back and forth he sways from cheek to cheek, At one extreme agreeing with one Greek - At the other agreeing with another Greek Which may be thought, but only so to speak. A baggy figure, equally pathetic When sedentary and when peripatetic.&quot; Пакет позволяет разделить текст на предложения: sent_vec &lt;- get_sentences(my_example_text) sent_vec[1:2] ## [1] &quot;The bear puts both arms around the tree above her\\nAnd draws it down as if it were a lover\\nAnd its choke cherries lips to kiss good-bye,\\nThen lets it snap back upright in the sky.&quot; ## [2] &quot;Her next step rocks a boulder on the wall\\n(She&#39;s making her cross-country in the fall).&quot; Воспользуемся регулярными выражениями, чтобы удалить переносы строк: library(stringr) sent_vec &lt;- str_replace_all(sent_vec, &quot;\\\\n&quot;, &quot; &quot;) sent_vec[1:2] ## [1] &quot;The bear puts both arms around the tree above her And draws it down as if it were a lover And its choke cherries lips to kiss good-bye, Then lets it snap back upright in the sky.&quot; ## [2] &quot;Her next step rocks a boulder on the wall (She&#39;s making her cross-country in the fall).&quot; Если вы хотите прочитать в R большой файл, для этого есть функция get_text_as_string, который необходимо указать путь к файлу на компьютере или url. Функция get_sentiment принимает в качестве аргументов вектор и метод (о методах можно подробнее узнать, вызвав помощь). bing_vector &lt;- get_sentiment(sent_vec, method=&quot;bing&quot;) bing_vector ## [1] 1 -1 0 1 0 -3 0 -1 0 -2 Как видно, не очень радостное стихотворение. Взглянем на самое мрачное предложение: sent_vec[which.min(bing_vector)] ## [1] &quot;Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests.&quot; Результат применения различных методов может несколько отличаться. afinn_vector &lt;- get_sentiment(sent_vec, method=&quot;afinn&quot;) afinn_vector ## [1] 3 0 3 2 1 -5 2 -2 0 -2 sent_vec[which.min(afinn_vector)] ## [1] &quot;Man acts more like the poor bear in a cage That all day fights a nervous inward rage - His mood rejecting all his mind suggests.&quot; Но в нашем случае методы согласны: сравнение человека с беспокойным медведем в клетке – эмоциональный минимум стихотворения. Эмоциональная валентность произведения в целом вычисляется либо как сумма, либо как среднее всех значений: sum(afinn_vector) ## [1] 2 mean(afinn_vector) ## [1] 0.2 Не так уж плохо: за счет первой части про медведицу, с удовольствием поедающую черемуху, общая тональность скорее положительная (хотя ваш читательский опыт может говорить об обратном). Результат можно передать функции plot(): plot(afinn_vector, type=&quot;l&quot;, main=&quot;Example Plot Trajectory&quot;, xlab = &quot;Narrative Time&quot;, ylab= &quot;Emotional Valence&quot;) О других способах визуализировать результат можно узнать из виньетки к пакету. 10.11 Виды эмоций Лексикон NRC дает возможность позволяет не просто оценить эмоциональную валентность, но и выявить конкретные эмоции: nrc_data &lt;- get_nrc_sentiment(sent_vec) nrc_data ## anger anticipation disgust fear joy sadness surprise trust negative positive ## 1 3 5 1 1 4 1 3 4 1 6 ## 2 1 0 0 1 0 2 0 0 2 0 ## 3 0 1 1 1 1 1 1 1 1 1 ## 4 1 1 0 1 1 0 0 0 0 1 ## 5 1 0 0 1 0 0 0 0 1 0 ## 6 2 1 0 2 0 1 0 0 3 0 ## 7 0 1 0 0 1 0 1 2 0 2 ## 8 0 0 0 0 0 0 0 1 0 1 ## 9 0 1 0 0 0 0 0 2 1 2 ## 10 0 0 1 0 0 1 0 0 1 1 Выбрать конкретные эмоции можно так (но результат в нашем случае не очень осмысленный): joy_items &lt;- which(nrc_data$joy &gt; 0) sent_vec[joy_items] На графике это можно отразить при помощи базовой barplot: barplot( sort(colSums(prop.table(nrc_data[, 1:8]))), horiz = TRUE, cex.names = 0.7, las = 1, main = &quot;Emotions in Sample text&quot;, xlab=&quot;Percentage&quot; ) Сложно осуждать машину за не вполне верно считанную тональность: Фрост – сложный автор. Для отзывов на Tripadvisor это может сработать лучше. Литература "],["распределения-слов-и-анализ-частотностей.html", "Тема 11 Распределения слов и анализ частотностей 11.1 Извлечение источников 11.2 Подготовка источников 11.3 Токенизация 11.4 Cтоп-слова 11.5 Абсолютная частотность 11.6 Стемминг 11.7 Относительная частотность 11.8 Распределения слов (токенов) 11.9 Закон Ципфа 11.10 TTR (type-token ratio) 11.11 TF-IDF 11.12 Сравнение при помощи диаграммы рассеяния", " Тема 11 Распределения слов и анализ частотностей В этом уроке мы научимся считать наиболее частотные и наиболее характерные слова, удалять стоп-слова, познакомимся с алгоритмом стемминга, а также узнаем, как считать type-token ratio (и почему этого делать не стоит). За основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты: “Опыт о человеческом разумении” Джона Локка (1690), первые две книги; “Трактат о принципах человеческого знания” Джорджа Беркли (1710); “Исследование о человеческом разумении” Дэвида Юма (1748)44. 11.1 Извлечение источников Источники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить gutenberg_id. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли. # install.packages(&quot;gutenbergr&quot;) library(gutenbergr) library(tidyverse) library(stringr) gutenberg_works(str_detect(author, &quot;Hume&quot;), languages = &quot;en&quot;) ## # A tibble: 99 × 8 ## gutenberg_id title author gutenberg_author_id language gutenberg_bookshelf ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2868 &quot;The Gr… Hume,… 1057 en &lt;NA&gt; ## 2 4223 &quot;The My… Hume,… 1057 en &lt;NA&gt; ## 3 4320 &quot;An Enq… Hume,… 1440 en Philosophy ## 4 4531 &quot;The Se… Hume,… 1057 en &lt;NA&gt; ## 5 4583 &quot;Dialog… Hume,… 1440 en Philosophy/Paganism ## 6 4705 &quot;A Trea… Hume,… 1440 en Philosophy ## 7 4946 &quot;Madame… Hume,… 1057 en &lt;NA&gt; ## 8 9662 &quot;An Enq… Hume,… 1440 en Harvard Classics/P… ## 9 10574 &quot;The Hi… Hume,… 1440 en United Kingdom ## 10 13117 &quot;The Ne… Hume,… 4843 en Natural History/An… ## # ℹ 89 more rows ## # ℹ 2 more variables: rights &lt;chr&gt;, has_text &lt;lgl&gt; Когда id найдены, gutenbergr позволяет загрузить сочинения; на этом этапе часто возникают ошибки – в таком случае надо воспользоваться одним из зеркал. Список зеркал, как уже говорилось в уроке про импорт данных, доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL. my_corpus &lt;- gutenberg_download(meta_fields = c(&quot;author&quot;, &quot;title&quot;), c(9662, 4723, 10615), mirror = &quot;https://www.gutenberg.org/dirs/&quot;) my_corpus ## # A tibble: 23,844 × 4 ## gutenberg_id text author title ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 4723 &quot;A Treatise Concerning the Principles of Human Kno… Berke… A Tr… ## 2 4723 &quot;&quot; Berke… A Tr… ## 3 4723 &quot;&quot; Berke… A Tr… ## 4 4723 &quot;by&quot; Berke… A Tr… ## 5 4723 &quot;&quot; Berke… A Tr… ## 6 4723 &quot;George Berkeley (1685-1753)&quot; Berke… A Tr… ## 7 4723 &quot;&quot; Berke… A Tr… ## 8 4723 &quot;&quot; Berke… A Tr… ## 9 4723 &quot;WHEREIN THE CHIEF CAUSES OF ERROR AND DIFFICULTY … Berke… A Tr… ## 10 4723 &quot;WITH THE GROUNDS OF SCEPTICISM, ATHEISM, AND IRRE… Berke… A Tr… ## # ℹ 23,834 more rows В этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния и имена можно двумя способами: при помощи функции unique() из базового R или distinct() из tidyverse. unique(my_corpus$title) ## [1] &quot;A Treatise Concerning the Principles of Human Knowledge&quot; ## [2] &quot;An Enquiry Concerning Human Understanding&quot; ## [3] &quot;An Essay Concerning Humane Understanding, Volume 1\\r\\nMDCXC, Based on the 2nd Edition, Books 1 and 2&quot; my_corpus %&gt;% distinct(author) ## # A tibble: 3 × 1 ## author ## &lt;chr&gt; ## 1 Berkeley, George ## 2 Hume, David ## 3 Locke, John 11.2 Подготовка источников Прежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные. my_corpus &lt;- my_corpus %&gt;% select(-gutenberg_id) %&gt;% select(-title) %&gt;% relocate(text, .after = author) %&gt;% mutate(author = str_remove(author, &quot;,.+$&quot;)) %&gt;% filter(text != &quot;&quot;) head(my_corpus, 3) ## # A tibble: 3 × 2 ## author text ## &lt;chr&gt; &lt;chr&gt; ## 1 Berkeley A Treatise Concerning the Principles of Human Knowledge ## 2 Berkeley by ## 3 Berkeley George Berkeley (1685-1753) В случае с Юмом отрезаем предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем. Hume &lt;- my_corpus %&gt;% filter(author == &quot;Hume&quot;) %&gt;% filter(!row_number() %in% c(1:25), !row_number() %in% c(4814:nrow(my_corpus))) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;_&quot;, &quot; &quot;)) %&gt;% filter(!str_detect(text, &quot;SECTION .{1,4}&quot;)) sample_n(Hume, 3) ## # A tibble: 3 × 2 ## author text ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume &quot;not, of themselves, to have any connexion with the secret powers of&quot; ## 2 Hume &quot;but enquire, from what impression is that supposed idea derived ? An… ## 3 Hume &quot; Thirdly, We learn from anatomy, that the immediate object of power … В случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: [Note: Vide Hobbes’ Tripos, ch. v. sect. 6.]45, от них тоже следует избавиться. Berkeley &lt;- my_corpus %&gt;% filter(author == &quot;Berkeley&quot;) %&gt;% filter(!row_number() %in% c(1:38)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]+?\\\\.&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;\\\\[.+?\\\\]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]+&quot;, &quot; &quot;)) Что касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение; удаляем подчеркивания вокруг слов. “Письмо к читателю” уже содержит некоторые философские положения, и его можно оставить. Locke &lt;- my_corpus %&gt;% filter(author == &quot;Locke&quot;) %&gt;% filter(!row_number() %in% c(1:135)) %&gt;% mutate(text = str_replace_all(text, &quot;_&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:digit:]]&quot;, &quot; &quot;)) Соединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем. tidy_corpus &lt;- bind_rows(Hume, Berkeley, Locke) %&gt;% mutate(text = str_replace_all(text, c(&quot;[Mm]an’s&quot; = &quot;man&#39;s&quot;, &quot;[mM]en’s&quot; = &quot;men&#39;s&quot;, &quot;[hH]ath&quot; = &quot;has&quot;))) 11.3 Токенизация После этого делим корпус на слова, как мы уже делали в уроке про токенизацию. library(tidytext) corpus_words &lt;- tidy_corpus %&gt;% unnest_tokens(word, text) corpus_words ## # A tibble: 238,538 × 2 ## author word ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume moral ## 2 Hume philosophy ## 3 Hume or ## 4 Hume the ## 5 Hume science ## 6 Hume of ## 7 Hume human ## 8 Hume nature ## 9 Hume may ## 10 Hume be ## # ℹ 238,528 more rows 11.4 Cтоп-слова Большая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна – это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция anti_join() позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете tidytext; в других случаях их следует загружать отдельно. Для многих языков стоп-слова доступны в пакете stopwords46. Функция anti_join() работает так: other &lt;- c(&quot;section&quot;, &quot;chapter&quot;, 0:40, &quot;edit&quot;, 1710, &quot;v.g&quot;, &quot;v.g.a&quot;) corpus_words_nosp &lt;- corpus_words %&gt;% anti_join(stop_words) %&gt;% filter(!word %in% other) ## Joining with `by = join_by(word)` corpus_words_nosp ## # A tibble: 73,311 × 2 ## author word ## &lt;chr&gt; &lt;chr&gt; ## 1 Hume moral ## 2 Hume philosophy ## 3 Hume science ## 4 Hume human ## 5 Hume nature ## 6 Hume treated ## 7 Hume manners ## 8 Hume peculiar ## 9 Hume merit ## 10 Hume contribute ## # ℹ 73,301 more rows Уборка закончена, мы готовы к подсчетам. 11.5 Абсолютная частотность Для начала посмотрим на самые частотные слова во всем корпусе. library(ggplot2) corpus_words_nosp %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder(word, n), n, fill = word)) + geom_col(show.legend = F) + coord_flip() Этот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие idea. Однако можно заподозрить, что высокие показатели для слов simple, distinct и powers – это заслуга прежде всего Локка, который вводит понятия “простой идеи” и “отчетливой идеи”, а также говорит о “силах” вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка – это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (innate) также занимает в первую очередь его: вся первая книга “Опыта” – это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм – очень кратко. Кроме того, хотя мы взяли только две книги из “Опыта” Локка – это самый длинный в нашем корпусе, что создает значительный перекос: corpus_words_nosp %&gt;% group_by(author) %&gt;% summarise(sum = n()) ## # A tibble: 3 × 2 ## author sum ## &lt;chr&gt; &lt;int&gt; ## 1 Berkeley 11455 ## 2 Hume 18182 ## 3 Locke 43674 Посмотрим статистику по отдельным авторам. corpus_words_nosp %&gt;% group_by(author) %&gt;% count(word, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder_within(word, n, author), n, fill = word)) + geom_col(show.legend = F) + facet_wrap(~author, scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() Наиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов. Согласно Локку, объектом мышления является идея (желательно отчетливая, но тут уж как получится). Все идеи приобретены умом из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких врожденных идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть простыми и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся пространство, в котором находятся тела, а также продолжительность; измеренная продолжительность представляет собой время. Беркли спорит с мнением, согласно котором ум способен образовывать абстрактные идеи. В том числе, утверждает он, невозможна абстрактная идея движения, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому слова являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет умом и духом) существуют не абстрактные идеи, а ощущения, и существование немыслящих вещей безотносительно к их воспринимаемости совершенно невозможно. Нет иной субстанции, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть материя. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами природы. Действительные вещи – это комбинации ощущений, запечатлеваемые в нас могущественным духом. Согласно Юму, все объекты, доступные человеческому разуму, могут быть разделены на два вида, а именно: на отношения между идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все заключения о фактах основаны на отношениях причины и действия. В свою очередь знание о причинности возникает всецело из опыта: только привычка заставляет нас ожидать наступления одного события при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в философии. 11.6 Стемминг Поскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим – стемминг. Стемминг (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году. В R стеммер Портера доступен в пакете snowball. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть47. Не для всех языков, впрочем, и не для всех задач стемминг – это хорошая идея. Но попробуем применить его к нашему корпусу. library(SnowballC) corpus_stems &lt;- corpus_words_nosp %&gt;% mutate(stem = wordStem(word)) corpus_stems %&gt;% count(stem, sort = TRUE) %&gt;% slice_head(n = 15) %&gt;% ggplot(aes(reorder(stem, n), n, fill = stem)) + geom_col(show.legend = F) + coord_flip() Все слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно ниже: # до стемминга corpus_words_nosp %&gt;% distinct(word) %&gt;% nrow() ## [1] 8132 # после стемминга corpus_stems %&gt;% distinct(stem) %&gt;% nrow() ## [1] 5229 Стемминг применяется в некоторых алгоритмах машинного обучения. 11.7 Относительная частотность Абсолютная частотность – плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте. Cначала считаем частотность для всех токенов по авторам. author_word_counts &lt;- corpus_words %&gt;% count(author, word, sort = T) %&gt;% ungroup() author_word_counts ## # A tibble: 14,111 × 3 ## author word n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 Locke the 7431 ## 2 Locke of 7044 ## 3 Locke and 4817 ## 4 Locke to 4603 ## 5 Hume the 3182 ## 6 Locke in 3056 ## 7 Locke that 2954 ## 8 Locke it 2742 ## 9 Locke is 2462 ## 10 Hume of 2461 ## # ℹ 14,101 more rows Затем – число токенов в каждой книге. total_counts &lt;- author_word_counts %&gt;% group_by(author) %&gt;% summarise(total = sum(n)) total_counts ## # A tibble: 3 × 2 ## author total ## &lt;chr&gt; &lt;int&gt; ## 1 Berkeley 36777 ## 2 Hume 53590 ## 3 Locke 148171 Соединяем два тиббла: author_word_counts &lt;- author_word_counts %&gt;% left_join(total_counts) head(author_word_counts) ## # A tibble: 6 × 4 ## author word n total ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; ## 1 Locke the 7431 148171 ## 2 Locke of 7044 148171 ## 3 Locke and 4817 148171 ## 4 Locke to 4603 148171 ## 5 Hume the 3182 53590 ## 6 Locke in 3056 148171 Считаем относительную частотность и умножаем на 100, чтобы получить проценты: author_word_rf &lt;- author_word_counts %&gt;% mutate(rf = round((n / total), 5) * 100) author_word_rf %&gt;% filter(author == &quot;Berkeley&quot;) %&gt;% head() ## # A tibble: 6 × 5 ## author word n total rf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Berkeley the 1915 36777 5.21 ## 2 Berkeley of 1550 36777 4.22 ## 3 Berkeley and 1178 36777 3.20 ## 4 Berkeley to 1057 36777 2.87 ## 5 Berkeley that 816 36777 2.22 ## 6 Berkeley is 772 36777 2.10 11.8 Распределения слов (токенов) Наиболее частотные слова – это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень-очень редко, а слов с высокой частотностью – мало. Тоненький хвост уходит далеко вправо по оси x, для наглядности зададим произвольный предел. author_word_rf %&gt;% ggplot(aes(rf, fill = author)) + geom_histogram(show.legend = FALSE) + facet_wrap(~author, scales = &quot;free&quot;) 11.9 Закон Ципфа Подобная картина характерна для естественных языков. Распределения слов в них подчиняются закону Ципфа. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) из Гарвардского университета и утверждает следующее: Если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота n-го слова в списке окажется обратно пропорциональной его порядковому номеру n. Это можно записать так: \\[tf_{r_i} = \\frac{c}{r^α_i}\\] или \\[ tf_{r_i} \\cdot r^α_i = c \\] Извлекая логарифм из обеих частей, получаем: \\[log(tf_{r_i}) = log(c) - α \\cdot log(r_i) \\] На всякий случай: логарифм дроби равен разности логарифмов числителя и знаменателя. Таким образом, мы получаем (почти) линейную зависимость, где c – точка пересечения оси y, a α - коэффициент наклона прямой. Графически это выглядит вот так: author_word_rf_rank &lt;- author_word_rf %&gt;% group_by(author) %&gt;% mutate(rank = row_number()) author_word_rf_rank %&gt;% ggplot(aes(rank, rf, color = author)) + geom_line(size = 1.1, alpha = 0.7) + scale_x_log10() + scale_y_log10() Чтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом подробнее в следующих уроках): lm_zipf &lt;- lm(data = author_word_rf_rank, formula = log10(rf) ~ log10(rank)) coefficients(lm_zipf) ## (Intercept) log10(rank) ## 1.749726 -1.272644 Мы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график: author_word_rf_rank %&gt;% ggplot(aes(rank, rf, color = author)) + geom_line(size = 1.1, alpha = 0.7) + geom_abline(intercept = 1.744, slope = -1.26, linetype = 2, color = &quot;grey50&quot;) + scale_x_log10() + scale_y_log10() Здесь видно, что отклонения наиболее заметны как в области самых частотных слов, так и в области наиболее редких (с высокими рангами). 11.10 TTR (type-token ratio) На практике это означает, что редкие слова (события) случаются очень часто; это явление известно под названием Large Number of Rare Events (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается (чем дальше, тем сложнее встретить слово, которого еще не было). Это значит, что сравнивать тексты с точки зрения лексического разнообразия – дело достаточно рискованное, хотя интуитивно кажется, что некоторые авторы пишут более разнообразно, а другие - менее. Наиболее известная и наиболее проблемная мера лексического разнообразия – это type-token ratio (TTR). \\[ TTR(T) = \\frac{Voc(T)}{n} \\] где n - общее число токенов, а Voc - число уникальных токенов (типов). В пакете languageR, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов. locke_words &lt;- corpus_words %&gt;% filter(author == &quot;Locke&quot;) %&gt;% pull(word) head(locke_words) ## [1] &quot;i&quot; &quot;have&quot; &quot;put&quot; &quot;into&quot; &quot;thy&quot; &quot;hands&quot; length(locke_words) ## [1] 148171 Это немного задумчивая функция, поэтому я создам не 148 тысяч отрывков, а всего 40. library(languageR) locke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40) ## ........................................ head(locke.growth@data$data) ## Chunk Tokens Types HapaxLegomena DisLegomena TrisLegomena Yule Zipf ## 1 1 1000 409 273 64 18 83.8600 -0.6618709 ## 2 2 2000 638 407 100 34 97.8500 -0.7749886 ## 3 3 3000 854 544 121 59 102.3711 -0.8420029 ## 4 4 4000 1013 608 154 72 103.5000 -0.8514063 ## 5 5 5000 1133 663 171 76 103.8696 -0.8742347 ## 6 6 6000 1256 713 193 85 105.2767 -0.8822200 ## TypeTokenRatio Herdan Guiraud Sichel Lognormal ## 1 0.4090000 0.7683545 12.93372 0.1564792 0.4346750 ## 2 0.3190000 0.7405672 14.26611 0.1567398 0.5013140 ## 3 0.2846667 0.7282750 15.59184 0.1416862 0.5191183 ## 4 0.2532500 0.7124681 16.01694 0.1520237 0.5779033 ## 5 0.2266000 0.6992475 16.02304 0.1509267 0.6194817 ## 6 0.2093333 0.6834975 16.21489 0.1536624 0.6548148 plot(locke.growth) Тут много всего интересного, но обратим внимание лишь на два верхних окошка слева: количество типов растет постоянно, но с разной скоростью. Так же меняется числов гапаксов: нечто похожее мы уже замечали, когда говорили о гапаксах у Платона. Подробнее о различных мерах лексического разнообразия см.: (Baayen 2008, 222–36) и (Savoy 2020). 11.11 TF-IDF Мы уже заметили, говоря об абсолютной частотности, что для трех авторов в нашем условном корпусе многие слова общие. Для многих алгоритмов машинного обучения используется другая мера, tf-idf (term frequency - inverse document frequency). Логарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для некоторого документа. Однако относительная частотность тоже учитывается! Например, Беркли один раз упоминает “сахарные бобы”, а Локк – “миндаль”, но из-за редкой частотности tf-idf для подобных слов будет низкий. Функция bind_tf_idf() принимает на входе тиббл с абсолютной частотностью для каждого слова. author_word_tfidf &lt;- author_word_rf %&gt;% filter(!word %in% other) %&gt;% bind_tf_idf(word, author, n) author_word_tfidf ## # A tibble: 14,103 × 8 ## author word n total rf tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Locke the 7431 148171 5.01 0.0502 0 0 ## 2 Locke of 7044 148171 4.75 0.0476 0 0 ## 3 Locke and 4817 148171 3.25 0.0325 0 0 ## 4 Locke to 4603 148171 3.11 0.0311 0 0 ## 5 Hume the 3182 53590 5.94 0.0594 0 0 ## 6 Locke in 3056 148171 2.06 0.0206 0 0 ## 7 Locke that 2954 148171 1.99 0.0199 0 0 ## 8 Locke it 2742 148171 1.85 0.0185 0 0 ## 9 Locke is 2462 148171 1.66 0.0166 0 0 ## 10 Hume of 2461 53590 4.59 0.0459 0 0 ## # ℹ 14,093 more rows Выбираем слова с высокой tf-idf: author_word_tfidf %&gt;% select(-total) %&gt;% arrange(desc(tf_idf)) ## # A tibble: 14,103 × 7 ## author word n rf tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Hume event 71 0.132 0.00133 1.10 0.00146 ## 2 Locke innate 297 0.2 0.00201 0.405 0.000813 ## 3 Hume sentiment 35 0.065 0.000653 1.10 0.000718 ## 4 Hume reasoning 91 0.17 0.00170 0.405 0.000689 ## 5 Locke duration 249 0.168 0.00168 0.405 0.000682 ## 6 Hume fact 80 0.149 0.00149 0.405 0.000605 ## 7 Hume enquiry 29 0.054 0.000541 1.10 0.000595 ## 8 Hume conjoined 26 0.049 0.000485 1.10 0.000533 ## 9 Hume energy 26 0.049 0.000485 1.10 0.000533 ## 10 Hume witnesses 22 0.041 0.000411 1.10 0.000451 ## # ℹ 14,093 more rows Снова визуализируем. author_word_tfidf %&gt;% arrange(-tf_idf) %&gt;% group_by(author) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) + geom_col(show.legend = F) + labs(x = NULL, y = &quot;tf-idf&quot;) + facet_wrap(~author, scales = &quot;free&quot;) + scale_x_reordered() + coord_flip() ## Selecting by tf_idf На таком графике авторы выглядят гораздо более самобытными, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную. 11.12 Сравнение при помощи диаграммы рассеяния Столбиковая диаграмма – не единственный способ сравнить частотности слов. Еще один наглядный метод – это диаграмма рассеяния с относительными частотностями. Функция spread() позволяет разделить один столбец на несколько новых, а gather(), напротив, – собрать. freq &lt;- author_word_rf %&gt;% anti_join(stop_words) %&gt;% mutate(rf = rf / 100) %&gt;% filter(rf &gt; 0.0001) %&gt;% select(-n, -total) %&gt;% spread(author, rf, fill = 0) %&gt;% gather(author, rf, Hume:Locke) ## Joining with `by = join_by(word)` freq ## # A tibble: 2,458 × 4 ## word Berkeley author rf ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 absence 0 Hume 0 ## 2 absent 0 Hume 0 ## 3 absolute 0.00071 Hume 0.00021 ## 4 absolutely 0.00027 Hume 0.00015 ## 5 abstract 0.0025 Hume 0.00035 ## 6 abstracted 0.00046 Hume 0 ## 7 abstracting 0.00014 Hume 0 ## 8 abstraction 0.00049 Hume 0 ## 9 abstruse 0 Hume 0.00017 ## 10 absurd 0.00049 Hume 0.00017 ## # ℹ 2,448 more rows library(scales) ## ## Attaching package: &#39;scales&#39; ## The following object is masked from &#39;package:syuzhet&#39;: ## ## rescale ## The following object is masked from &#39;package:purrr&#39;: ## ## discard ## The following object is masked from &#39;package:readr&#39;: ## ## col_factor freq %&gt;% ggplot(aes(x = rf, y = Berkeley)) + geom_abline(color = &quot;grey40&quot;, lty = 2) + geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3, color = &quot;darkblue&quot;) + geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, color = &quot;grey30&quot;) + scale_x_log10(labels = percent_format()) + scale_y_log10(labels = percent_format()) + facet_wrap(~author, ncol = 2) + theme(legend.position = &quot;none&quot;) + theme_minimal() + labs(y = &quot;Berkeley&quot;, x = NULL) Эти три трактата нам еще понадобятся, поэтому сохраняем опрятный текст с уже удаленными стоп-словами. save(corpus_words_nosp, file = &quot;data/Idea.Rdata&quot;) Литература "],["тематическое-моделирование.html", "Тема 12 Тематическое моделирование 12.1 Что такое LDA 12.2 Распределение Дирихле 12.3 Подготовка данных 12.4 Матрица встречаемости 12.5 Число тем 12.6 Модель LDA 12.7 Слова и темы 12.8 Темы и документы 12.9 Распределения вероятности для топиков 12.10 Интерактивные визуализации", " Тема 12 Тематическое моделирование 12.1 Что такое LDA Приступая к анализу текстов (текст-майнингу), мы часто хотим разделить большую коллекцию документов на некие естественные группы, например, тематические. Одним из способов такого деления является тематическое моделирование. Латентное размещение Дирихле (LDA) - особенно популярный метод для построения тематической модели. В нем каждый документ рассматривается как смесь тем, а каждая тема - как смесь слов. Это позволяет документам “перекрывать” друг друга по содержанию, а не разделяться на отдельные группы, что отражает типичное использование естественного языка. Например, мы можем представить коллекцию документов по истории искусства, в которой будут тексты о живописи, архитектуре и фотографии. Тема искусства будет представлена во всех документах, где-то может быть будет сочетание 2-3 тем сразу. Источник: Blei, D. M. (2012), Probabilistic topic models Чем-то работа LDA похожа на то, как мы размечаем текст текстовыделителями: например, в этом курсе зеленым можно выделить код, желтым – математические и статистические отступления, а розовым – окологуманитарные сюжеты. Как вы уже поняли, ключевой вопрос в том, сколько у вас текстовыделителей. При тематическом моделировании этот параметр задается вручную, и дальше мы посмотрим, как это делается. 12.2 Распределение Дирихле Математические и статистические основания LDA достаточно хитроумны, но к счастью пользоваться моделью можно и без погружения в интегралы, как можно водить машину, не умея собрать двигатель внутреннего сгорания. Общие принципы на русском языке хорошо изложены в статье “Как понять, о чем текст, не читая его” на сайте “Системный блок”. Альфа и бета на этой схеме - гиперпараметры распределения. Гиперпараметры регулируют распределения тем по документам и слов по темам. Наглядно это можно представить так: При α = 1 получается равномерное распределение: темы распределены равномерно (заметим, что α также называют “параметром концентрации”). При значениях α &gt; 1 выборки начинают концентрироваться в центре треугольника, представляя собой равномерную смесь всех тем. При низких значениях альфа α &lt; 1 большинство наблюдений находится в углах – скорее всего, в в этом случае в документах будет меньше смешения тем48. Распределение документов по топикам θ зависит от значения α, поскольку θ ~ Dir(α). Из θ выбирается конкретная тема Z. Аналогичным образом гиперпараметр 𝛽 управляет распределением слов по темам. При меньших значениях 𝛽 темы, скорее всего, будут больше различаться. Распределение слов φ темы Z ~ Dir(β). Конкретное слово W выбирается уже из этого распределения. Можно представить себе банкетный зал со столами: если их несколько, и они стоят по углам, то вероятность встретить вашего знакомого в углу выше, чем в центре зала. Если он при этом вегетарианец, вы его будете искать у стола с овощами, а не с котлетами. Метафору можно понимать двояко. С одной стороны, ваш знакомый – это слово, а стол – тема, или топик. Он может нечаянно прибиться к столу с котлетами, как и слово “футбол” может оказаться в финансовых новостях. Но это сближение не будет таким устойчивым, как, например, связь слова “банк” с финансовым топиком. С другой стороны, сами “документы” склонны прибиваться к определенным топикам: открыв газету, вы не ожидаете увидеть в одной статье новости вирусологии, педагогики и финансового регулирования такое бывает только в блогах. 12.3 Подготовка данных Чтобы понять возможности алгоритма, мы попробуем передать ему архив телеграм-канала Antibarbari. Он хранится в репозитории этого курса на GitHub, откуда его можно скачать и повторить эксперимент. Но любому моделированию всегда предшествует подготовка данных, согласно древней мудрости garbage in - garbage out. Нужные мне html уже лежат в рабочей директории (2 файла); но при парсинге сайтов, в принципе, действует похожая логика. library(xml2) library(rvest) library(tidyverse) messages &lt;- read_html(&quot;./files/antibarbari_archive/messages.html&quot;) messages2 &lt;- read_html(&quot;./files/antibarbari_archive/messages2.html&quot;) Cледующий код позволяет достать только те узлы div, у которых значение атрибута class = text. text &lt;- html_elements(messages, &quot;div.text&quot;) %&gt;% html_text() text2 &lt;- html_elements(messages2, &quot;div.text&quot;) %&gt;% html_text() Сшиваем и смотрим, что получилось. text &lt;- as_tibble(text) text2 &lt;- as_tibble(text2) text &lt;- text %&gt;% bind_rows(text2) dim(text) ## [1] 780 1 text ## # A tibble: 780 × 1 ## value ## &lt;chr&gt; ## 1 &quot;\\nAntibarbari HSE \\n &quot; ## 2 &quot;\\nLatin never sleeps. Новое видео на канале Antibarbari (фрагмент семинара … ## 3 &quot;\\nПодборка видео семинара по медленному чтению \\&quot;О философии\\&quot; Аристотеля; … ## 4 &quot;\\nНовое видео в плейлисте \\&quot;Латинский язык\\&quot;. Фрагмент семинара 28.02.2022h… ## 5 &quot;\\n🤖 ОТКРЫТА ЗАПИСЬ НА ПРОЕКТВ рамках проекта участники овладеют навыками п… ## 6 &quot;\\nЖелающие присоединиться к группе, читающей \\&quot;Филеба\\&quot; Платона, пишите рук… ## 7 &quot;\\nhttps://youtu.be/I-U_lG0mB3M\\n &quot; ## 8 &quot;\\nВ клубе Antibarbari продолжается семинар по чтению и обсуждению фрагменто… ## 9 &quot;\\nФилеб. Семинар 3 марта 2022. Сократ и Протарх решают следовать за логосом… ## 10 &quot;\\nВышка вернулась в аудитории, поэтому теперь у нас театральная акустика. … ## # ℹ 770 more rows Здесь должна следовать обычная рутина: удаление сносок, переносов строки, чисел, имейлов, хэштегов и т.п. Код мог бы выглядеть, например, вот так: text_clean &lt;- text %&gt;% mutate(value = str_replace_all(value, &quot;(http|https)(\\\\S+)&quot;, &quot; &quot;)) %&gt;% mutate(value = str_replace_all(value, &quot;\\\\d{2}\\\\.\\\\d{2}\\\\.\\\\d{4}&quot;, &quot; &quot;)) %&gt;% mutate(value = str_replace_all(value, &quot;\\n&quot;, &quot; &quot;)) %&gt;% mutate(value = str_replace_all(value, &quot;\\\\W[-A-Za-z0-9_.%]+\\\\@[-A-Za-z0-9_.%]+\\\\.[A-Za-z]+&quot;, &quot; &quot;)) Но некоторые предварительные эксперименты показали, что в исходном html очень много латинского и греческого текста, разного рода смайлов и т.п., и все это плохо сказывается на модели. Поэтому я применю радикальное средство и удалю все, что не написано кириллицей: text_clean &lt;- text %&gt;% mutate(value = str_replace_all(value, &quot;[Сс]ылка|[Чч]асть&quot;, &quot; &quot;)) %&gt;% mutate(value = str_replace_all(value, &quot;[[^\\u0400-\\u04FF]]&quot;, &quot; &quot;)) %&gt;% mutate(value = str_replace_all(value, &quot; \\\\w{1,2} &quot;, &quot; &quot;)) %&gt;% mutate(value = str_remove_all(value, &quot; лат &quot; )) %&gt;% mutate(value = str_remove_all(value, &quot; гр &quot; )) text_clean ## # A tibble: 780 × 1 ## value ## &lt;chr&gt; ## 1 &quot; &quot; ## 2 &quot; Новое видео канале фрагмент семинара … ## 3 &quot; Подборка видео семинара медленному чтению философии Аристотеля руководи… ## 4 &quot; Новое видео плейлисте Латинский язык Фрагмент семинара … ## 5 &quot; ОТКРЫТА ЗАПИСЬ ПРОЕКТВ рамках проекта участники овладеют навыками парсин… ## 6 &quot; Желающие присоединиться группе читающей Филеба Платона пишите руководи… ## 7 &quot; &quot; ## 8 &quot; клубе продолжается семинар чтению обсуждению фрагментов утраче… ## 9 &quot; Филеб Семинар марта Сократ Протарх решают следовать логосом кото… ## 10 &quot; Вышка вернулась аудитории поэтому теперь нас театральная акустика доска… ## # ℹ 770 more rows Каждому документу (посту в Telegram) следует добавить id, иначе при разделении на слова мы потеряем данные об их происхождении. text_clean &lt;- text_clean %&gt;% mutate(id = paste0(&quot;doc_&quot;, row_number()), .before = value) Теперь можно лемматизировать. Как это делать, мы уже знаем. library(udpipe) russian_syntagrus &lt;- udpipe_load_model(file = &quot;russian-syntagrus-ud-2.5-191206.udpipe&quot;) text_ann &lt;- udpipe_annotate(russian_syntagrus, text_clean$value) Разный остаточный мусор. text_tbl &lt;- as_tibble(text_ann) %&gt;% select(doc_id, lemma) %&gt;% filter(!lemma %in% c(&quot;нный&quot;, &quot;ный&quot;, &quot;кнь&quot;, &quot;ся&quot;, &quot;ть&quot;, &quot;ние&quot;, &quot;вие&quot;)) text_tbl ## # A tibble: 62,609 × 2 ## doc_id lemma ## &lt;chr&gt; &lt;chr&gt; ## 1 doc2 новый ## 2 doc2 видео ## 3 doc2 канал ## 4 doc2 фрагмент ## 5 doc2 семинар ## 6 doc3 подборка ## 7 doc3 видео ## 8 doc3 семинар ## 9 doc3 медленный ## 10 doc3 чтение ## # ℹ 62,599 more rows Загружаем список стоп-слов для русского языка: возвращаясь к метафоре с банкетом, это салфетки. Они лежат на всех столах и нам не интересны. library(stopwords) stop &lt;- stopwords(language = &quot;ru&quot;, source = &quot;stopwords-iso&quot;) %&gt;% as_tibble() %&gt;% rename(lemma = value) Очень короткие слова и очень короткие посты удаляем. text_tidy &lt;- text_tbl %&gt;% anti_join(stop) %&gt;% filter(nchar(lemma) &gt; 1) %&gt;% group_by(doc_id) %&gt;% filter(n() &gt;= 15) %&gt;% filter(!lemma %in% c(&quot;ибо&quot;, &quot;либо&quot;)) %&gt;% ungroup() text_tidy ## # A tibble: 39,737 × 2 ## doc_id lemma ## &lt;chr&gt; &lt;chr&gt; ## 1 doc5 открыто ## 2 doc5 запись ## 3 doc5 Проектвый ## 4 doc5 рамка ## 5 doc5 проект ## 6 doc5 участник ## 7 doc5 овладеть ## 8 doc5 навык ## 9 doc5 парсинг ## 10 doc5 подготовить ## # ℹ 39,727 more rows Исправим некоторые ошибки лемматизации. text_tidy &lt;- text_tidy %&gt;% mutate_at(vars(lemma), ~ case_when(lemma == &quot;мят&quot; ~ &quot;мята&quot;, lemma == &quot;Мят&quot; ~ &quot;мята&quot;, TRUE ~ .)) %&gt;% mutate(lemma = tolower(lemma)) 12.4 Матрица встречаемости Поскольку LDA – вероятностная модель, то на входе она принимает целые числа. В самом деле, не имеет смысла говорить о том, что некое распределение породило 0.5 слов или того меньше. Поэтому мы считаем абсолютную, а не относительную встречаемость – и не tf_idf49. text_count &lt;- text_tidy %&gt;% group_by(doc_id, lemma) %&gt;% count(lemma) head(text_count) ## # A tibble: 6 × 3 ## # Groups: doc_id, lemma [6] ## doc_id lemma n ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 doc10 акустика 1 ## 2 doc10 аудитория 1 ## 3 doc10 воспротивиться 1 ## 4 doc10 вышка 1 ## 5 doc10 гибридный 1 ## 6 doc10 грамматика 1 Для работы с LDA в R устанавливаем пакет topicmodels. На входе нужная нам функция этого пакета принимает такую структуру данных, как document-term matrix (dtm), которая используется для хранения сильно разреженных данных и происходит из популярного пакета для текст-майнинга tm. Поэтому “тайдифицированный” текст придется для моделирования преобразовать в этот формат, а полученный результат вернуть в опрятный формат для визуализаций50. Для преобразования подготовленного корпуса в формат dtm воспользуемся возможностями пакета tidytext: library(tidytext) text_dtm &lt;- text_count %&gt;% cast_dtm(doc_id, term = lemma, value = n) text_dtm ## &lt;&lt;DocumentTermMatrix (documents: 538, terms: 11296)&gt;&gt; ## Non-/sparse entries: 32720/6044528 ## Sparsity : 99% ## Maximal term length: 27 ## Weighting : term frequency (tf) Убеждаемся, что почти все ячейки в нашей матрице – нули (99% разреженность). 12.5 Число тем Количество тем для модели всегда задается вручную. Мы не всегда заранее знаем, сколько тем в нашем корпусе, и здесь на помощь приходит функция perplexity() из topicmodels. Она показывает, насколько подогнанная модель не соответствует данным – поэтому чем значение меньше, тем лучше. Подгоним сразу несколько моделей с разным количеством тем и посмотрим, какая из них покажет себя лучше. Выполнение кода ниже займет какое-то время. library(topicmodels) n_topics &lt;- c(2, 4, 8, 16, 32, 64) text_lda_compare &lt;- n_topics %&gt;% map(LDA, x = text_dtm, control = list(seed = 0211)) data_frame(k = n_topics, perplex = map_dbl(text_lda_compare, perplexity)) %&gt;% ggplot(aes(k, perplex)) + geom_point() + geom_line() + labs(title = &quot;Оценка LDA модели&quot;, subtitle = &quot;Оптимальное количество топиков&quot;, x = &quot;Число топиков&quot;, y = &quot;Perplexity&quot;) Если верить графику, предпочтительны 64 темы (на самом деле, если подогнать еще больше моделей, то и все 200). Но спешить не стоит. Если эксперт задаст в параметрах своей программы слишком мало тем, то разные самостоятельные топики сольются в один и станут неразличимы для взгляда исследователя. Если будет задано слишком большое число топиков, то помимо реальных тем, присутствующих в корпусе, появятся «паразитные», которые с точки зрения математического аппарата показывают совместно встречающиеся слова, однако на практике эти слова не будут образовывать тематически самостоятельных контекстов. Поэтому процесс тематического моделирования включает этап подбора нужного количества топиков и соизмерение получившихся результатов с разноплановыми соображениями. Источник Мои разноплановые соображения говорят, что больше 10 топиков выделять непродуктивно. 12.6 Модель LDA text_lda &lt;- LDA(text_dtm, k = 10, control = list(seed = 1111)) Итак, наша тематическая модель готова. Осталось понять, что с ней делать. 12.7 Слова и темы Пакет tidytext дает возможность “тайдифицировать” объект lda с использованием разных методов. Метод β (“бета”) извлекает вероятность того, что слово происходит из данного топика. text_topics &lt;- tidy(text_lda, matrix = &quot;beta&quot;) text_topics %&gt;% filter(term == &quot;огурец&quot;) %&gt;% arrange(-beta) ## # A tibble: 10 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2 огурец 2.36e- 3 ## 2 5 огурец 1.20e- 3 ## 3 9 огурец 2.10e-253 ## 4 8 огурец 1.00e-271 ## 5 10 огурец 3.88e-273 ## 6 4 огурец 1.93e-274 ## 7 1 огурец 1.70e-274 ## 8 6 огурец 4.58e-275 ## 9 7 огурец 5.83e-276 ## 10 3 огурец 3.39e-277 Например, слово “огурец” с большей вероятностью порождено темой 2 или 5, чем остальными темами. Кажется, мы нашли огуречные темы 🥒🥒🥒 Посмотрим на главные термины в топиках. text_top_terms &lt;- text_topics %&gt;% group_by(topic) %&gt;% arrange(-beta) %&gt;% slice_head(n = 12) %&gt;% ungroup() head(text_top_terms) ## # A tibble: 6 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 месяц 0.0121 ## 2 1 язык 0.00691 ## 3 1 цицерон 0.00591 ## 4 1 образ 0.00443 ## 5 1 являться 0.00395 ## 6 1 отношение 0.00354 text_top_terms %&gt;% filter(topic &lt; 10) %&gt;% mutate(term = reorder(term, beta)) %&gt;% ggplot(aes(term, beta, fill = factor(topic))) + geom_col(show.legend = FALSE) + facet_wrap(~ topic, scales = &quot;free&quot;, ncol=3) + coord_flip() В “огуречной” теме 5 обнаружились также “тыква” и “каштан”. Видимо, наш алгоритм вполне “узнал” плодово-овощную рубрику Ирины Макаровой, в которой она рассказывает, что и зачем выращивали древние греки и римляне. Однако “мята” прибилась к более философскому топику 4. Сравним топики 4 и 5 по формуле: \\(log_2\\left(\\frac{β_2}{β_1}\\right)\\). Если \\(β_2\\) в 2 раза больше \\(β_1\\), то логарифм будет равен 1; если наоборот, то -1. На всякий случай: \\(\\frac{1}{2} = 2^{-1}\\). Для подсчетов снова придется трансформировать данные. beta_spread &lt;- text_topics %&gt;% filter(topic %in% c(4, 5)) %&gt;% mutate(topic = paste0(&quot;topic_&quot;, topic)) %&gt;% spread(topic, beta) %&gt;% filter(topic_4 &gt; .001 | topic_5 &gt; .001) %&gt;% mutate(log_ratio = log2(topic_5 / topic_4)) head(beta_spread) ## # A tibble: 6 × 4 ## term topic_4 topic_5 log_ratio ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 абрикос 8.46e-285 2.40e- 3 935. ## 2 автор 2.48e- 3 3.89e- 3 0.650 ## 3 аида 1.38e- 3 2.68e-269 -883. ## 4 анализ 1.14e- 3 4.43e- 12 -27.9 ## 5 английский 1.91e- 3 1.18e- 3 -0.692 ## 6 аполлон 1.11e- 3 1.59e-252 -827. На графике выглядит понятнее: beta_log_ratio &lt;- beta_spread %&gt;% mutate(sign = case_when(log_ratio &gt; 0 ~ &quot;pos&quot;, log_ratio &lt; 0 ~ &quot;neg&quot;)) %&gt;% select(-topic_5, -topic_4) %&gt;% group_by(sign) %&gt;% arrange(desc(abs(log_ratio))) %&gt;% slice_head(n = 10) beta_log_ratio %&gt;% ggplot(aes(reorder(term, log_ratio), log_ratio, fill = sign)) + geom_col(show.legend = FALSE) + xlab(&quot;термин&quot;) + ylab(&quot;log2 (beta_5 / beta_4)&quot;) + coord_flip() В теме 5 видим и других обитателей садов и огородов: “слива”, “саранка”. В теме 4 больше лексики, связанной с мифологией. 12.8 Темы и документы Распределение тем по документам хранит матрица gamma. text_documents &lt;- tidy(text_lda, matrix = &quot;gamma&quot;) text_documents %&gt;% filter(topic == 5) %&gt;% arrange(-gamma) ## # A tibble: 538 × 3 ## document topic gamma ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 doc288 5 0.999 ## 2 doc519 5 0.999 ## 3 doc186 5 0.999 ## 4 doc620 5 0.999 ## 5 doc682 5 0.999 ## 6 doc774 5 0.999 ## 7 doc398 5 0.999 ## 8 doc165 5 0.999 ## 9 doc160 5 0.999 ## 10 doc634 5 0.998 ## # ℹ 528 more rows Значение gamma можно понимать как долю слов в документе, происходящую из данного топика. Например, тема 4 представлена в документе 288 Посмотрим на него: doc_288 &lt;- text_clean %&gt;% filter(id == &quot;doc_288&quot;) %&gt;% pull(value) paste0(substr(doc_288, 1, 279), &quot;...&quot;) ## [1] &quot; Тыква Закончилось лето подходит завершению наша сезонная плодово ягодная рубрика Сегодня вспомним тыкве ведь греки охотно выращивали привычная нам тыква те что были греков римлян одно то хотя все они относятся семейству тыквенных Тыквы прилавка супер...&quot; Каждый документ в рамках LDA рассматривается как собрание тем. Значит, сумма всех гамм для текста должна быть равна единице. Проверим. text_documents %&gt;% filter(document == &quot;doc288&quot;) %&gt;% summarise(sum = sum(gamma)) ## # A tibble: 1 × 1 ## sum ## &lt;dbl&gt; ## 1 1 Все верно! Теперь отберем несколько длинных постов и посмотрим, какие топики в них представлены. long_posts &lt;- text_clean %&gt;% mutate(char = nchar(value)) %&gt;% arrange(-char) %&gt;% slice_head(n = 6) %&gt;% pull(id) long_posts &lt;- str_remove_all(long_posts, &quot;_&quot;) long_posts ## [1] &quot;doc731&quot; &quot;doc216&quot; &quot;doc740&quot; &quot;doc332&quot; &quot;doc624&quot; &quot;doc563&quot; text_documents %&gt;% filter(document %in% long_posts) %&gt;% arrange(-gamma) %&gt;% ggplot(aes(as.factor(topic), gamma, color = document)) + geom_boxplot(show.legend = F) + facet_wrap(~document) Документы 624 и 563 относятся к топику 10. Проверим, насколько эти документы тематически близки. doc_624 &lt;- text_clean %&gt;% filter(id == &quot;doc_624&quot;) %&gt;% pull(value) paste0(substr(doc_624, 1, 279), &quot;...&quot;) ## [1] &quot; Цицерон географ продолжаем наши цицеронианские четверги сегодня расскажем том как Цицерон чуть было стал географом цицерон Как видно письма Аттик предложил своему другу задуматься адаптации монументальной Географии Эратосфена трех книгах этого труда Эра сфе...&quot; doc_563 &lt;- text_clean %&gt;% filter(id == &quot;doc_563&quot;) %&gt;% pull(value) paste0(substr(doc_563, 1, 279), &quot;...&quot;) ## [1] &quot; Цицерона любовью нескольких письмах Цицерон обсуждает Аттиком автобиографические сочинения посвященные событиям есть заговору Катилины говорит латинских греческих записках также упоминает планах написать поэму чтобы никакой жанр восхваления был самим м...&quot; 12.9 Распределения вероятности для топиков text_documents %&gt;% ggplot(aes(gamma, fill = as.factor(topic))) + geom_histogram(show.legend = F) + facet_wrap(~ topic, ncol = 5) + scale_y_log10() + labs(title = &quot;Распределение вероятностей для каждого топика&quot;, y = &quot;Число документов&quot;, x = expression(gamma)) Почти ни одна тема не распределена равномерно: гамма чаще всего принимает значения либо около нуля, либо в районе единицы. Тема 3, однако, отклоняется от этого правила. text_top_terms %&gt;% filter(topic == 3) ## # A tibble: 12 × 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 3 философия 0.0181 ## 2 3 проект 0.00944 ## 3 3 платон 0.00863 ## 4 3 латинский 0.00754 ## 5 3 философский 0.00742 ## 6 3 античность 0.00726 ## 7 3 семинар 0.00720 ## 8 3 научный 0.00707 ## 9 3 текст 0.00700 ## 10 3 язык 0.00688 ## 11 3 студент 0.00681 ## 12 3 курс 0.00674 Действительно, здесь встречаются слова, связанные с изучением античности в целом и потому возможные в разных контекстах. Можно сказать, что это своего рода метатопик, характеризующий тематику канала в целом. 12.10 Интерактивные визуализации Более подробно изучить полученную модель можно при помощи интерактивной визуализации. Функция ниже заимстовована отсюда. topicmodels2LDAvis &lt;- function(x, ...){ post &lt;- topicmodels::posterior(x) if (ncol(post[[&quot;topics&quot;]]) &lt; 3) stop(&quot;The model must contain &gt; 2 topics&quot;) mat &lt;- x@wordassignments LDAvis::createJSON( phi = post[[&quot;terms&quot;]], theta = post[[&quot;topics&quot;]], vocab = colnames(post[[&quot;terms&quot;]]), doc.length = slam::row_sums(mat, na.rm = TRUE), term.frequency = slam::col_sums(mat, na.rm = TRUE) ) } library(LDAvis) LDAvis::serVis(topicmodels2LDAvis(text_lda), out.dir = &quot;ldavis&quot;) Ссылка на интерактивную визуализацию топиков. https://www.mithilaguha.com/post/topic-modeling-and-latent-dirichlet-allocation↩︎ https://datascience.stackexchange.com/questions/21950/why-we-should-not-feed-lda-with-tfidf/49704#49704?newreg=c17592380de141cf9064c9c5ef09cdc6↩︎ https://www.tidytextmining.com/topicmodeling.html↩︎ "],["литература.html", "Литература", " Литература "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
