# Латентно-семантический анализ (Часть 2)

Снова смоделируем тематику постов из телеграм-канала Antibarbari. "Опрятный" текст был подготовлен ранее, сейчас просто загрузим его. 

```{r message=FALSE}
library(tidyverse)
library(tidytext)
library(lsa)
library(widyr)

load("./data/antibarbari_count.Rdata")
```

```{r}
text_count_long 
```

Итак, у нас есть коллекция документов, к которой мы планируем применить LSA. Посмотрим на состав этой коллекции.

```{r}
doc_count <- text_count_long %>% 
  group_by(doc_id) %>% 
  summarise(sum = sum(n))

doc_median <- median(doc_count$sum)
doc_median
```

Очень короткие посты мы удалили в прошлом уроке; медиана -- около 46 слов.

```{r}
doc_count %>% 
  ggplot(aes(sum)) + 
  geom_histogram(binwidth = 10, fill = "hotpink4", col = "white") +
  geom_vline(xintercept = doc_median, linetype = "dotted", col = "grey40")
```

## Модель LSA

Теперь построим модель. Топиков пусть будет 7, как и в модели LDA. 

```{r}
tidy_word_emb <- text_count_long %>%
  cast_sparse(doc_id, lemma, n) %>% 
  lsa(dims = 7)
```

Эбеддинги слов:

```{r}
word_emb_lsa <- lsaSpace$dk %*% diag(lsaSpace$sk)

word_emb_lsa
```

Матрица расстояния для слов. 

```{r eval=FALSE}
dist_mx_words <- cosine(t(word_emb_lsa))
```

Ближайшие соседи.

```{r eval=F, echo=F}
nearest <- function(mx, x, y) {
  idx<- which(rownames(mx) == x)
  mx_subset <- mx[idx, -idx] 
  neighbours <- sort(mx_subset, decreasing = T)
  names(head(neighbours, y))
}

nearest(dist_mx_words, "платон", 3)
```

Посмотрим на главные компоненты (так иногда называют новые [измерения](https://courses.engr.illinois.edu/cs440/fa2018/lectures/lect36.html)).




## Поиск соседей

Посмотрим, какие слова оказались "ближе" друг к другу. Для этого воспользуемся функцией `nearest_neighbors`, которая считает косинусное расстояние между словами [отсюда](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings).

```{r eval=F, echo=F}
tidy_word_emb %>% 
  nearest_neighbors("платон")
```


```{r eval=F, echo=F}
hc <- hclust(dist_cos)
```

```{r eval=F, echo=F}
text_cmd <- cmdscale(dist_cos)
```

```{r eval=F, echo=F}
clusters <- cutree(hc, 8)
```

```{r eval=F, echo=F}
library(ggrepel)

text_cmd %>% 
  as_tibble(rownames = "item") %>% 
  bind_cols(as_tibble(clusters)) %>%
  sample_frac(0.25) %>% 
  ggplot(aes(V1, V2, label = item, col = as.factor(value))) +
  geom_point(show.legend = F) + 
  geom_text_repel(show.legend = F, max.overlaps = 10)
```


LSA, в отличие от LDA, -- это не вероятностная, а алгебраическая модель. Это значит, что термдокументная матрица может хранить не только данные об абсолютной встречаемости слова в документе (как это требует LDA), выраженные целыми числами, но и любые реальные числа. Как показывает практика, использование tf_idf может улучшить модель.
