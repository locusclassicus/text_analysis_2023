# Веб-скрапинг

Выше мы говорили о таком импорте html, когда все теги разом удаляются. Это не всегда удобно, поскольку файл хранит данные в структурированном виде, например, под разными тегами дату, автора и текст. И может быть желательно эту структуру сохранить. 

В R это позволяет делать пакет `rvest`. С его помощью мы подготовим для дальнейшего построения тематической модели архив телеграм-канала Antibarbari HSE. Канал публичный, и Telegram дает возможность скачать архив в формате html при помощи кнопки export (однако эта функция может быть недоступна на MacOS). 

Эта глава опирается в основом на [второе издание](https://r4ds.hadley.nz/webscraping.html) книги R for Data Science Хадли Викхема.

## Структура html 

Документы html (HyperText Markup Language) имеют ирархическую структуру, состоящую из **элементов**.  В каждом элементе есть **открывающий тег** (<tag>), опциональные **атрибуты** (id='first') и **закрывающий тег** (</tag>). Все, что находится между открывающим и закрывающим тегом, называется **содержанием** элемента. 

Важнейшие теги, о которых стоит знать:

- html (есть всегда), с двумя детьми (дочерними элементами): head и body
- элементы, отвечающие за структуру: h1 (заголовок), section, p (параграф), ol (упорядоченный список)
- элементы, отвечающие за оформление: b (bold), i (italics), a (ссылка)

Чтобы увидеть структуру страницы, надо нажать правой кнопкой мыши и выбрать View Source (это работает и для тех html, которые хранятся у вас на компьютере).


## Каскадные таблицы стилей

У тегов могут быть именованные атрибуты; важнейшие из них -- это id и class, которые в сочетании с CSS контролируют внешний вид страницы.

:::infobox
CSS (англ. Cascading Style Sheets «каскадные таблицы стилей») — формальный язык декорирования и описания внешнего вида документа (веб-страницы), написанного с использованием языка разметки (чаще всего HTML или XHTML).
:::

У этого курса тоже есть свой файл .css, в котором блок infobox (вы его видите как серый квадратик с определением) описан так:

```{}
.infobox {
  padding: 1em 1em 1em 4em;
  background: aliceblue 5px center/3em no-repeat;
  color: black;
}
```

Проще говоря, это инструкция, что делать с тем или иным элементом. Каждое правило CSS имеет две основные части — **селектор** и **блок объявлений**. Селектор, расположенный в левой части правила до знака «{», определяет, на какие части документа (возможно, специально обозначенные) распространяется правило. Блок объявлений располагается в правой части правила. Он помещается в фигурные скобки, и, в свою очередь, состоит из одного или более объявлений, разделённых знаком «;».

Селекторы CSS полезны для скрапинга, потому что они помогают вычленить необходимые элементы. Это работает так:

- p выберет все элементы <p>
- .title выберет элементы с классом "title"
- #title выберет все элементы с атрибутом id='title'

Важно: если изменится структура страницы, откуда вы скрапили информацию, то и код, возможно, придется переписывать.

## Извлечение данных

Чтобы прочесть файл html, используем одноименную функцию.

```{r}
library(rvest)
messages <- read_html("./files/antibarbari_archive/messages.html")
messages2 <- read_html("./files/antibarbari_archive/messages2.html")

messages
```

На следующем этапе важно понять, какие именно элементы нужны. Рассмотрим на примере одного сообщения. Для примера я сохраню этот элемент как небольшой отдельных html; `rvest` позволяет это сделать (но внутри двойных кавычек должны быть только одинарные):

```{r}
html <-  minimal_html("
<div class='message default clearfix' id='message83'>
      <div class='pull_left userpic_wrap'>
       <div class='userpic userpic2' style='width: 42px; height: 42px'>
        <div class='initials' style='line-height: 42px'>
A
        </div>
       </div>
      </div>
      <div class='body'>
       <div class='pull_right date details' title='19.05.2022 11:18:07 UTC+03:00'>
11:18
       </div>
       <div class='from_name'>
Antibarbari HSE 
       </div>
       <div class='text'>
Этот пост открывает серию переложений из «Дайджеста платоновских идиом» Джеймса Ридделла (1823–1866), английского филолога-классика, чей научный путь был связан с Оксфордским университетом. По приглашению Бенджамина Джоветта он должен был подготовить к изданию «Апологию», «Критон», «Федон» и «Пир». Однако из этих четырех текстов вышла лишь «Апология» с предисловием и приложением в виде «Дайджеста» (ссылка) — уже после смерти автора. <br><br>«Дайджест» содержит 326 параграфов, посвященных грамматическим, синтаксическим и риторическим особенностям языка Платона. Знакомство с этим теоретическим материалом позволяет лучше почувствовать уникальный стиль философа и добиться большей точности при переводе. Ссылки на «Дайджест» могут быть уместны и в учебных комментариях к диалогам Платона. Предлагаемая здесь первая часть «Дайджеста» содержит «идиомы имен» и «идиомы артикля» (§§ 1–39).<br><a href='http://antibarbari.ru/2022/05/19/digest_1/'>http://antibarbari.ru/2022/05/19/digest_1/</a>
       </div>
       <div class='signature details'>
Olga Alieva
       </div>
      </div>
     </div>
")
```

Из всего этого мне может быть интересно id сообщения (<div class='message default clearfix' id='message83'>), текст сообщения (<div class='text'>),  а также, если указан, автор сообщения (<div class='signature details'>). Извлекаем текст:

```{r}
html %>%
  html_element(".text") %>% 
  html_text2()
```


В классе signature details есть пробел, достаточно на его месте поставить точку:

```{r}
html %>%
  html_element(".signature.details") %>% 
  html_text2()
```

Важно помнить, что `html_element` всегда возвращает один элемент. Если их больше, надо использовать `html_elements`.

Осталось добыть message id:

```{r}
html %>%
  html_element("div") %>% 
  html_attr("id")
```

## Извлечение в тиббл

```{r message=FALSE}
library(tidyverse)

tibble(id = html %>% 
         html_element("div") %>% 
         html_attr("id"),
       signature = html %>%
         html_element(".signature.details") %>% 
         html_text2(),
       text = html %>% 
         html_element(".text") %>%
         html_text2()
)
```

## Скрапим телеграм-канал

До сих пор наша задача упрощалась тем, что мы имели дело с игрушечным html для единственного сообщения. В настоящем html тег div повторяется на разных уровнях, нам надо извлечь только такие div, которым соответствует определенный класс:

```{r}
messages %>%
  html_elements("div.message.default") %>% 
  head()
```
Уже из этого списка можем доставать все остальное. 

```{r}
messages_tbl1 <- tibble(id = messages %>% 
         html_elements("div.message.default") %>% 
         html_attr("id"),
       signature = messages %>%
         html_elements("div.message.default") %>% 
         html_element(".signature.details") %>% 
         html_text2(),
       text = messages %>% 
         html_elements("div.message.default") %>% 
         html_element(".text") %>%
         html_text2()
)

messages_tbl2 <- tibble(id = messages2 %>% 
         html_elements("div.message.default") %>% 
         html_attr("id"),
       signature = messages2 %>%
         html_elements("div.message.default") %>% 
         html_element(".signature.details") %>% 
         html_text2(),
       text = messages2 %>% 
         html_elements("div.message.default") %>% 
         html_element(".text") %>%
         html_text2()
)

```

Обратите внимание, что мы сначала извлекаем нужные элементы при помощи `html_elements()`, а потом применяем к каждому из них `html_element()`. Это гарантирует, что в каждом столбце нашей таблицы равное число наблюдений, т.к. функция `html_element()`, если она не может найти, например, подпись, возвращает NA.

Сшиваем воедино два тиббла.

```{r}
messages_tbl <- messages_tbl1 %>% 
  bind_rows(messages_tbl2)

messages_tbl
```

Создатели канала не сразу разрешили подписывать посты, поэтому для первых нескольких десятков подписи не будет. В некоторых постах только фото, для них в столбце text -- NA, их можно сразу отсеять.

```{r}
messages_tbl <- messages_tbl %>%
  filter(!is.na(text))

set.seed(1234)
```

##  Эмотиконы

В постах довольно много эмотиконов. Я их удалю, но сначала скажу о полезном пакете, который позволяет их все извлечь и, например, посчитать.


```{r message=FALSE}
library(emoji)

messages_tbl %>% 
  mutate(emoji = emoji_extract_all(text)) %>% 
  pull(emoji) %>% 
  unlist() %>% 
  as_tibble() %>%
  count(value) %>% 
  arrange(-n) 
```

Заменяем их все на пробелы.

```{r}
messages_tbl <- messages_tbl %>% 
  mutate(text = emoji_replace_all(text, " "))
```


## Рутинная уборка

Подготовка текста для анализа включает в себя удаление сносок, иногда хэштегов, чисел, имейлов, возможно имен и т.п. В нашем случае ситуация осложняется тем, что тексты включают цитаты на латыни и древнегреческом, некоторые технические сокращения, номера страниц и др. Вот так, например, выглядит типичный пост:

```{r}
example <- messages_tbl$text[340]

example
```
Вот так вылавливается гиперссылка.

```{r}
str_extract_all(example, "(http|https)(\\S+)")
```
Вот так вылавливается пагинация и номер семинара (и некоторые другие числа).

```{r}
str_extract_all(example, "#?\\d{2,3}\\w?\\d?-?")
```
Похожим образом можно выловить даты, имейлы и т.п. Все это удаляем из текста. 

```{r}
messages_clean <- messages_tbl %>% 
  mutate(text = str_replace_all(text, "(http|https)(\\S+)", " ")) %>% 
  mutate(text = str_replace_all(text, "\\d{2}\\.\\d{2}\\.\\d{4}", " ")) %>% 
  mutate(text = str_replace_all(text, "\\W[-A-Za-z0-9_.%]+\\@[-A-Za-z0-9_.%]+\\.[A-Za-z]", " ")) %>% 
  mutate(text = str_replace_all(text, "#?\\d{2,3}\\w?\\d?-?", " ")) %>% 
  mutate(text = str_replace_all(text, "\\n+", " "))
```

Остались еще сокращения вроде "г.", но токены из одной буквы можно будет удалить после разделения на слова. Знаки пунктуации можно оставить или убрать -- иногда они бывают интересным стилистическим маркером. В любом случае лучше это делать после лемматизации, т.к. на тексте без знаков препинания анализатор покажет себя хуже.


```{r}
messages_clean %>% 
  filter(row_number() == 340)
```

Число id и число текстов не совпадает, поскольку для некоторых постов текста нет (NA), а у других он совпадает ("Пост выходного дня"). Это надо сразу исправить, чтобы результат лемматизации можно было потом соединить с данными о подписи. Я просто уберу очень короткие посты, поскольку для анализа они неинтересны.

```{r}
messages_clean <- messages_clean %>%
  filter(nchar(text) > 19)
```

```{r}
dim(messages_clean)
```

Переименуем первый столбец и переназначим id, чтобы можно было потом соединить с результатами лемматизации.

```{r}
messages_clean <- messages_clean %>% 
  rename(doc_id = id) %>% 
  mutate(doc_id = paste0("doc", row_number()))
```

## Лемматизация

На лемматизацию мы отдаем вектор с сообщениями.
  
```{r message=FALSE, eval=FALSE}
library(udpipe)
russian_syntagrus <- udpipe_load_model(file = "russian-syntagrus-ud-2.5-191206.udpipe")

messages_ann <- udpipe_annotate(russian_syntagrus, messages_clean$text)

messages_ann <- as_tibble(messages_ann)

messages_ann
```

```{r eval=FALSE, echo=FALSE}
save(messages_ann, file = "./data/messages_ann.Rdata")
```

```{r echo=FALSE}
load("./data/messages_ann.Rdata")
```

Убедимся, что после лемматизации число id не изменилось, и соединим этот тиббл с данными о подписи.

```{r}
length(unique(messages_ann$doc_id))
```
```{r}
messages_signed <- messages_ann %>% 
  left_join(messages_clean, by = "doc_id") %>% 
  select(-text, -sentence_id, -paragraph_id, -xpos, -feats,
         -head_token_id, -dep_rel, -deps, -misc)

length(unique(messages_signed$doc_id))
```

В постах упоминаются многие коллеги и студенты, чьи имена я бы хотела удалить, чтобы они не появлялись на графиках и т.п., но есть и много древних и новых имен, которые хотелось бы оставить. 

```{r eval = F }
# valid_names <- messages_ann %>% 
#   filter(upos == "PROPN") %>% 
#   count(lemma) %>% 
#   arrange(-n) %>% 
#   filter(!str_detect(lemma, "[\\.«]")) %>% 
#   filter(str_detect(lemma, "[[\u0400-\u04FF]]")) %>% 
#   filter(n > 1)
# 
# valid_names_vec <- as.character(valid_names$lemma)

# список имен отредактирован вручную
# write.table(valid_names_vec, file = "files/names.txt", 
#            row.names = F, col.names = F)
```

```{r message=FALSE}
valid_names <- read_table(file = "files/names.txt", col_names = F)

valid_names <- valid_names %>% 
  rename(names = X1) %>% 
  mutate(names = str_remove_all(names, "\\W")) %>% 
  pull(names)
```

```{r}
messages_signed <- messages_signed %>% 
  filter(upos != "PROPN" | upos == "PROPN" & lemma %in% valid_names) %>% 
  filter(upos != "PUNCT") %>% 
  filter(!upos %in% c("X", "NUM")) %>% 
  mutate(lemma = str_replace_all(lemma, "-", "")) %>% 
  mutate(lemma = str_remove_all(lemma, "[[:punct:]]"))
```

После некоторых сомнений, я удалю все, кроме кириллицы -- на канале очень много латинского и греческого текста, на этапе создания термдокументной матрицы из-за этого будет почти 100% разреженность.

```{r}
messages_signed <- messages_signed %>% 
  mutate(lemma = str_replace_all(lemma, "[[^\u0400-\u04FF]]", "")) %>% 
  filter(nchar(lemma) > 0) 
```

Дальше были исправлены некоторые ошибки лемматизации.

```{r echo=FALSE}
messages_signed <- messages_signed  %>% 
  filter(!lemma %in% c("Витя", "Рафаэлл"))
```


```{r}
text_tidy <- messages_signed %>%
  mutate(lemma = tolower(lemma)) %>% 
  mutate_at(vars(lemma), ~
       case_when(lemma == "мят" ~  "мята",
                 str_detect(lemma, "сенек") ~ "сенека",
                 str_detect(lemma, "аттик") ~ "аттик",
                 str_detect(lemma, "кальвиз") ~ "кальвизий",
                 str_detect(lemma, "горац") ~ "гораций",
                 str_detect(lemma, "гален") ~ "гален",
                 str_detect(lemma, "плотин") ~ "плотин",
                 str_detect(lemma, "августин") ~ "августин",
                 str_detect(lemma, "абари") ~ "абарид",
                 str_detect(lemma, "катулл") ~ "катулл",
                 str_detect(lemma, "лукул") ~ "лукулл",
                 str_detect(lemma, "посидо") ~ "посидоний",
                 str_detect(lemma, "деркилл") ~ "деркиллид",
                 str_detect(lemma, "порфир") ~ "порфирий",
                 str_detect(lemma, "афин") ~ "афины",
                 str_detect(lemma, "локк") ~ "локк",
                 str_detect(lemma, "макроб") ~ "макробий",
                 str_detect(lemma, "лаэр") ~ "лаэрций",
                 str_detect(lemma, "макрин") ~ "макрина",
                 str_detect(lemma, "маркиш") ~ "маркиш",
                 str_detect(lemma, "маячк") ~ "маячок",
                 str_detect(lemma, "очерк") ~ "очерк",
                 str_detect(lemma, "птолем") ~ "птолемей",
                 str_detect(lemma, "росса") ~ "росс",
                 str_detect(lemma, "самосат") ~ "самосата",
                 str_detect(lemma, "стрепсиад") ~ "стрепсиад",
                 str_detect(lemma, "эпихар") ~ "эпихарм",
                 str_detect(lemma, "ямвл") ~ "ямвлих",
                 str_detect(lemma, "брумал") ~ "брумалии",
                 str_detect(lemma, "иоанн") ~ "иоанн",
                 str_detect(lemma, "кинопоиск") ~ "кинопоиск",
                 str_detect(lemma, "корнар") ~ "корнарий",
                 str_detect(lemma, "луция") ~ "луций",
                 str_detect(lemma, "кассия") ~ "кассий",
                 str_detect(lemma, "минф") ~ "минфа",
                 str_detect(lemma, "персефон") ~ "персефона",
                 str_detect(lemma, "пестум") ~ "пестум",
                 str_detect(lemma, "платон") ~ "платон",
                 str_detect(lemma, "платно") ~ "платон",
                 str_detect(lemma, "филеб") ~ "филеб",
                 str_detect(lemma, "фульг") ~ "фульгенций",
                 str_detect(lemma, "аврел") ~ "аврелий",
                 str_detect(lemma, "антигон") ~ "антигона",
                 str_detect(lemma, "анция") ~ "анций",
                 str_detect(lemma, "борея") ~ "борей",
                 str_detect(lemma, "вольтер") ~ "вольтер",
                 str_detect(lemma, "гераклид") ~ "гераклид",
                 str_detect(lemma, "теэтет") ~ "теэтет",
                 str_detect(lemma, "евангел") ~ "евангелие",
                 str_detect(lemma, "федон") ~ "федон",
                 TRUE ~ .))
```

Получившийся тиббл сохраняю -- он понадобится в главе 13. 

```{r eval=FALSE}
save(text_tidy, file = "data/AntibarbariTidy.Rdata")
```

## Html таблицы

Если вам повезет, то ваши данные уже будут храниться в HTML-таблице, и их можно будет просто считать из этой таблицы^[https://r4ds.hadley.nz/webscraping#tables]. Распознать таблицу в браузере обычно несложно: она имеет прямоугольную структуру из строк и столбцов, и ее можно скопировать и вставить в такой инструмент, как Excel.

Таблицы HTML строятся из четырех основных элементов: `<table>`, `<tr>` (строка таблицы), `<th>` (заголовок таблицы) и `<td>` (данные таблицы).  Мы соберем информацию о [проектных группах](https://hum.hse.ru/proj/project2022_2024) ФГН в 2022-2024 г.

```{r}
html <- read_html("https://hum.hse.ru/proj/project2022_2024")
my_table <- html %>%  
  html_element(".bordered") %>% 
  html_table()

my_table
```

## Wikisource

Многие тексты доступны на сайте Wikisource.org. Попробуем извлечь все сказки Салтыкова-Щедрина. 

```{r}
url <- "https://ru.wikisource.org/wiki/%D0%9C%D0%B8%D1%85%D0%B0%D0%B8%D0%BB_%D0%95%D0%B2%D0%B3%D1%80%D0%B0%D1%84%D0%BE%D0%B2%D0%B8%D1%87_%D0%A1%D0%B0%D0%BB%D1%82%D1%8B%D0%BA%D0%BE%D0%B2-%D0%A9%D0%B5%D0%B4%D1%80%D0%B8%D0%BD"
html = read_html(url)
```

Для того, чтобы справиться с такой страницей, пригодится Selector Gadget (расширение для Chrome). Вот [тут](https://youtu.be/oqNTfWrGdbk) можно посмотреть короткое видео, как его установить. При помощи селектора выбираем нужные уровни.

```{r warning=FALSE}
toc <- html %>% 
  html_elements("ul:nth-child(22) a")

head(toc)
```

Теперь у нас есть список ссылок.

```{r}
tales <- tibble(
  title = toc %>%
    html_attr("title"),
  href = toc %>% 
    html_attr("href")
)
```

Данных о годе публикации под тегом <a> нет; надо подняться на уровень выше:

```{r}
toc2 <- html %>% 
  html_elements("ul:nth-child(22) li")

head(toc2)
```

```{r}
toc2 %>%
  html_text2()
```

Соединяем:

```{r}
tales <- tibble(
  title_year = toc2 %>%
    html_text2(),
  href = toc %>% 
    html_attr("href")
)

tales
```

Дальше можно достать текст для каждой сказки. Потренируемся на одной. Снова привлекаем Selector Gadget для составления правила.

```{r}
url_test <- tales %>% 
  filter(row_number() == 1) %>% 
  pull(href) %>% 
  paste0("https://ru.wikisource.org", .)

text <- read_html(url_test) %>% 
  html_elements(".indent p") %>% 
  html_text2() 

text[1]
text[length(text)]
```

Первый и последний параграф достали верно; можно обобщать.

```{r}
tales <- tales %>% 
    mutate(href = paste0("https://ru.wikisource.org", href))
```


```{r}
urls <- tales %>% 
  pull(href)
```

Функция для извлечения текстов.

```{r}
get_text <- function(url) {
  read_html(url) %>% 
  html_elements(".indent p") %>% 
  html_text2() %>% 
  paste(collapse= " ")
}
```

```{r}
tales_text <- map(urls, get_text)
```

Несколько сказок не выловились: там другая структура html, но в целом все получилось.

```{r}
tales_text <- tales_text %>%
  flatten_chr() %>% 
  as_tibble()

tales <- tales %>% 
  bind_cols(tales_text)
```

```{r}
tales
```

Дальше можно разделить столбец с названием и годом и, например, удалить ссылку, она больше не нужна. Разделить по запятой не получится, т.к. она есть в некоторых названиях.

```{r}
tales <- tales %>% 
  select(-href) %>% 
  separate(title_year, into = c("title", "year"), sep = -5) %>% 
  mutate(title = str_remove(title, ",$"))
```

```{r}
tales
```
Недостающие две сказки я не буду пытаться извлечь, но логику вы поняли. 