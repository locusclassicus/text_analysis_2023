# Распределения слов и анализ частотностей

В этом уроке мы научимся считать наиболее частотные и наиболее характерные слова, удалять стоп-слова, познакомимся с алгоритмом стемминга, а также узнаем, как считать type-token ratio (и почему этого делать не стоит). 

За основу для всех эти вычислений мы возьмем три философских трактата, написанных на английском языке. Это хронологически и тематически близкие тексты:

- "Опыт о человеческом разумении" Джона Локка (1690), первые две книги;
- "Трактат о принципах человеческого знания" Джорджа Беркли  (1710);
- "Исследование о человеческом разумении" Дэвида Юма (1748)^[Идеей подобного количественного сравнения я обязана моей коллеге Дарье Дроздовой].

## Извлечение источников

Источники для этого урока доступны в библиотеке Gutengerg; чтобы их извлечь, следует выяснить `gutenberg_id`. Пример ниже; таким же образом можно найти id для трактатов Локка и Беркли.

```{r message=FALSE}
# install.packages("gutenbergr")
library(gutenbergr)
library(tidyverse)
library(stringr)

gutenberg_works(str_detect(author, "Hume"), languages = "en")
```

Когда id найдены, `gutenbergr` позволяет загрузить сочинения; на этом этапе часто возникают ошибки -- в таком случае надо воспользоваться одним из зеркал. Список зеркал, как уже говорилось в уроке про импорт данных, доступен по ссылке: https://www.gutenberg.org/MIRRORS.ALL.

```{r eval=F}
my_corpus <- gutenberg_download(meta_fields = c("author", "title"), c(9662, 4723, 10615), mirror = "https://www.gutenberg.org/dirs/")

my_corpus
```

```{r eval=F, echo=FALSE}
save(my_corpus, file = "data/Gutenberg.Rdata")
```

```{r echo=FALSE}
load("data/Gutenberg.Rdata")
my_corpus
```

В этом тиббле хранятся все три текста, которые нам нужны. Уточнить уникальные называния и имена можно двумя способами: при помощи функции `unique()` из базового R или `distinct()` из tidyverse.

```{r}
unique(my_corpus$title)
```

```{r}
my_corpus %>% distinct(author)
```

## Подготовка источников

Прежде чем приступать к анализу, придется немного прибраться. Для этого используем инструменты tidyverse, о которых шла речь в главе про опрятные данные.

```{r}
my_corpus <- my_corpus %>% 
  select(-gutenberg_id) %>% 
  select(-title) %>% 
  relocate(text, .after = author) %>%
  mutate(author = str_remove(author, ",.+$")) %>% 
  filter(text != "")

head(my_corpus, 3)
```

В случае с Юмом отрезаем  предисловия, оглавление и индексы, а также номера разделов (везде прописными). Многие слова, которые в оригинале были выделены курсивом, окружены знаками подчеркивания (_), их тоже удаляем.

```{r}
Hume <- my_corpus %>% 
  filter(author == "Hume") %>% 
  filter(!row_number() %in% c(1:25),
         !row_number() %in% c(4814:nrow(my_corpus))) %>% 
  mutate(text = str_replace_all(text, "[[:digit:]]", " ")) %>% 
  mutate(text = str_replace_all(text, "_", " ")) %>% 
  filter(!str_detect(text, "SECTION .{1,4}"))
  

sample_n(Hume, 3)
```

В случае с Беркли отрезаем метаданные и посвящение в самом начале, а также удаляем нумерацию параграфов. Кроме того, текст содержит примечания следующего вида: [Note: Vide Hobbes' Tripos, ch. v. sect. 6.]^[https://www.gutenberg.org/cache/epub/4723/pg4723.txt], от них тоже следует избавиться. 

```{r}
Berkeley <- my_corpus %>% 
  filter(author == "Berkeley") %>% 
  filter(!row_number() %in% c(1:38)) %>% 
  mutate(text = str_replace_all(text, "[[:digit:]]+?\\.", " ")) %>%
  mutate(text = str_replace_all(text, "\\[.+?\\]", " ")) %>% 
  mutate(text = str_replace_all(text, "[[:digit:]]+", " "))
```

Что касается Локка, то здесь удаляем метаданные и оглавление в самом начале, а также посвящение; удаляем подчеркивания вокруг слов. "Письмо к читателю" уже содержит некоторые философские положения, и его можно оставить.

```{r}
Locke <- my_corpus %>% 
  filter(author == "Locke") %>% 
  filter(!row_number() %in% c(1:135)) %>% 
  mutate(text = str_replace_all(text, "_", " ")) %>% 
  mutate(text = str_replace_all(text, "[[:digit:]]", " "))
```

Соединив обратно все три текста, замечаем некоторые орфографические нерегулярности; исправляем.

```{r} 
tidy_corpus <- bind_rows(Hume, Berkeley, Locke) %>% 
  mutate(text = str_replace_all(text, c("[Mm]an’s" = "man's", "[mM]en’s" = "men's", "[hH]ath" = "has")))
```

## Токенизация

После этого делим корпус на слова, как мы уже делали в уроке про токенизацию.

```{r}
library(tidytext)

corpus_words <- tidy_corpus %>% 
  unnest_tokens(word, text)

corpus_words
```

## Cтоп-слова

Большая часть слов, которые мы сейчас видим в корпусе, нам пока не интересна -- это шумовые слова, или стоп-слова, не несущие смысловой нагрузки. Функция `anti_join()` позволяет от них избавиться; в случае с английским языком список стоп-слов уже доступен в пакете `tidytext`; в других случаях их следует загружать отдельно. Для многих языков стоп-слова доступны в пакете `stopwords`^[https://cran.r-project.org/web/packages/stopwords/readme/README.html]. Функция `anti_join()` работает так:

![](https://d33wubrfki0l68.cloudfront.net/f29a85efd53a079cc84c14ba4ba6894e238c3759/c1408/diagrams/join-anti.png){ width=70% }

```{r}
other <- c("section", "chapter", 0:40, "edit", 1710, "v.g", "v.g.a")

corpus_words_nosp <- corpus_words %>% 
  anti_join(stop_words) %>% 
  filter(!word %in% other)

corpus_words_nosp
```
Уборка закончена, мы готовы к подсчетам.

## Абсолютная частотность

Для начала посмотрим на самые частотные слова _во всем корпусе_.

```{r}
library(ggplot2)

corpus_words_nosp %>% 
  count(word, sort = TRUE) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(reorder(word, n), n, fill = word)) +
  geom_col(show.legend = F) + 
  coord_flip() 
```

Этот график уже дает общее представление о тематике нашего корпуса: это теория познания, в центре которой для всех трех авторов стоит понятие idea. 

Однако можно заподозрить, что высокие показатели для слов _simple_, _distinct_ и _powers_ -- это заслуга прежде всего Локка, который вводит понятия "простой идеи" и "отчетливой идеи", а также говорит о "силах" вещей, благодаря которым они воздействуют как друг на друга, так и на разум. Силы для Локка -- это причины идей, и как таковые они часто упоминаются в его тексте. Понятие врожденности (innate) также занимает в первую очередь его: вся первая книга "Опыта" -- это опровержение теории врожденных идей. Беркли о врожденности не говорит вообще, а Юм -- очень кратко.

Кроме того, хотя мы взяли только две книги из "Опыта" Локка -- это самый длинный в нашем корпусе, что создает значительный перекос:

```{r}
corpus_words_nosp %>% 
  group_by(author) %>% 
  summarise(sum = n())
```

Посмотрим статистику по отдельным авторам.

```{r}
corpus_words_nosp %>%
  group_by(author) %>% 
  count(word, sort = TRUE) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(reorder_within(word, n, author), n, fill = word)) +
  geom_col(show.legend = F) + 
  facet_wrap(~author, scales = "free") +
  scale_x_reordered() +
  coord_flip()
```

Наиболее частотные слова (при условии удаления стоп-слов) дают вполне адекватное представление о тематике каждого из трех трактатов. 

Согласно Локку, объектом мышления является идея (желательно _отчетливая_, но тут уж как получится).  Все идеи приобретены _умом_ из опыта, направленного на либо на внешние предметы (ощущения, или чувства), либо на внутренние действия разума (рефлексия, или внутреннее чувство). Никаких _врожденных_ идей у человека нет, изначально его душа похожа на чистый лист (антинативизм). Идеи могут быть _простыми_ и сложными; они делятся на модусы, субстанции и отношения. К числу простых модусов относятся _пространство_, в котором находятся _тела_, а также _продолжительность_; измеренная продолжительность представляет собой _время_.

Беркли спорит с мнением, согласно котором ум способен образовывать _абстрактные_ идеи. В том числе, утверждает он, невозможна абстрактная идея _движения_, отличная от движущегося тела. Он пытается устранить заблуждение Локка, согласно которому  _слова_ являются знаками абстрактных общих идей. В мыслящей душе (которую он также называет _умом_ и _духом_) _существуют_ не абстрактные идеи, а _ощущения_, и _существование_ немыслящих вещей безотносительно к их _воспринимаемости_ совершенно невозможно. Нет иной _субстанции_, кроме духа; немыслящие вещи ее совершенно лишены. По этой причине нельзя допустить, что существует невоспринимающая протяженная субстанция, то есть _материя_. Идеи ощущений возникают в нас согласно с некоторыми правилами, которые мы называем законами _природы_. Действительные вещи -- это комбинации ощущений, запечатлеваемые в нас могущественным духом.

Согласно Юму, все  _объекты_,  доступные  _человеческому разуму_,  могут быть разделены на два вида, а именно: на отношения между
идеями и факты. К суждениям об отношениях можно прийти благодаря одной только мыслительной деятельности, в то время как все _заключения_ о фактах основаны на отношениях причины и _действия_. В свою очередь знание о причинности возникает всецело из _опыта_: только привычка заставляет нас ожидать наступления одного _события_ при наступлении другого. Прояснение этого позволяет добиться большей ясности и точности в _философии_.

## Стемминг

Поскольку мы не лемматизировали текст, то единственное и множественное число слова idea рассматриваются как разные токены. Один из способов справиться с этим -- стемминг.

Стемминг (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. Стемминг применяется в поисковых системах для расширения поискового запроса пользователя, является частью процесса нормализации текста. Один из наиболее популярных алгоритмов стемминга был написан Мартином Портером и опубликован в 1980 году. 

В R стеммер Портера доступен в пакете `snowball`. К сожалению, он поддерживает не все языки, но русский, французский, немецкий и др. там есть^[https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf]. Не для всех языков, впрочем, и не для всех задач стемминг -- это хорошая идея. Но попробуем применить его к нашему корпусу.

```{r}
library(SnowballC)

corpus_stems <- corpus_words_nosp %>%
  mutate(stem = wordStem(word)) 

corpus_stems %>% 
  count(stem, sort = TRUE) %>%
  slice_head(n = 15) %>% 
  ggplot(aes(reorder(stem, n), n, fill = stem)) +
  geom_col(show.legend = F) + 
  coord_flip() 
```

Все слова немного покромсаны, но вполне узнаваемы. При этом общее количество уникальных токенов стало значительно ниже:

```{r}
# до стемминга
corpus_words_nosp %>%
  distinct(word) %>% 
  nrow()

# после стемминга
corpus_stems %>%
  distinct(stem) %>% 
  nrow()
```

Стемминг применяется в некоторых алгоритмах машинного обучения.

## Относительная частотность

Абсолютная частотность -- плохой показатель для текстов разной длины. Чтобы тексты было проще сравнивать, разделим показатели частотности на общее число токенов в тексте. 

Cначала считаем частотность для всех токенов по авторам.

```{r}
author_word_counts <- corpus_words %>%
  count(author, word, sort = T) %>% 
  ungroup()

author_word_counts
```

Затем -- число токенов в каждой книге.

```{r}
total_counts <- author_word_counts %>% 
  group_by(author) %>% 
  summarise(total = sum(n))

total_counts
```

Соединяем два тиббла:

```{r message=FALSE}
author_word_counts <- author_word_counts %>% 
  left_join(total_counts)

head(author_word_counts)
```
Считаем относительную частотность и умножаем на 100, чтобы получить проценты:

```{r}
author_word_rf <- author_word_counts %>% 
  mutate(rf = round((n / total), 5) * 100)

author_word_rf %>% 
  filter(author == "Berkeley") %>% 
  head()
```

## Распределения слов (токенов)

Наиболее частотные слова -- это служебные части речи. На графике видно, что подавляющее большинство слов встречается очень-очень редко, а слов с высокой частотностью -- мало. Тоненький хвост уходит далеко вправо по оси x, для наглядности зададим произвольный предел.

```{r message=FALSE, warning=FALSE}
author_word_rf %>% 
  ggplot(aes(rf, fill = author)) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~author, scales = "free") 
```

## Закон Ципфа

Подобная картина характерна для естественных языков. Распределения слов в них подчиняются **закону Ципфа**. Этот закон носит имя американского лингвиста Джорджа Ципфа (George Zipf) из Гарвардского университета и утверждает следующее:

:::infobox
Если все слова языка или длинного текста упорядочить по убыванию частоты использования, частота n-го слова в списке окажется обратно пропорциональной его порядковому номеру n.
:::


Это можно записать так: 

$$tf_{r_i} = \frac{c}{r^α_i}$$ или

$$ tf_{r_i} \cdot r^α_i = c $$
Извлекая логарифм из обеих частей, получаем:

$$log(tf_{r_i}) = log(c) - α \cdot log(r_i) $$

На всякий случай: логарифм дроби равен разности логарифмов числителя и знаменателя.

Таким образом, мы получаем (почти) линейную зависимость, где c -- точка пересечения оси y, a α - коэффициент наклона прямой. Графически это выглядит вот так:

```{r warning=FALSE}
author_word_rf_rank <- author_word_rf %>% 
  group_by(author) %>% 
  mutate(rank = row_number()) 

author_word_rf_rank %>% 
  ggplot(aes(rank, rf, color = author)) +
  geom_line(size = 1.1, alpha = 0.7) +
  scale_x_log10() +
  scale_y_log10()
```

Чтобы узнать точные коэффициенты, придется подогнать линейную модель (об этом подробнее в следующих уроках):

```{r}
lm_zipf <- lm(data = author_word_rf_rank, 
              formula = log10(rf) ~ log10(rank))

coefficients(lm_zipf)
```
Мы получили коэффициент наклона α чуть больше -1 (на практике точно -1 встречается редко). Добавим линию регрессии на график:

```{r}
author_word_rf_rank %>% 
  ggplot(aes(rank, rf, color = author)) +
  geom_line(size = 1.1, alpha = 0.7) +
  geom_abline(intercept = 1.744,
              slope = -1.26, 
              linetype = 2, 
              color = "grey50") +
  scale_x_log10() +
  scale_y_log10()
```

Здесь видно, что отклонения наиболее заметны как в области самых частотных слов, так и в области наиболее редких (с высокими рангами).

## TTR (type-token ratio)

На практике это означает, что редкие слова (события) случаются очень часто; это явление известно под названием *Large Number of Rare Events* (LNRE). И чем длиннее текст, тем больше в нем будет редких слов, но скорость их прибавления постепенно уменьшается (чем дальше, тем сложнее встретить слово, которого еще не было).

Это значит, что сравнивать тексты с точки зрения **лексического разнообразия** -- дело достаточно рискованное, хотя интуитивно кажется, что некоторые авторы пишут более разнообразно, а другие - менее. 

Наиболее известная и наиболее проблемная мера лексического разнообразия -- это *type-token ratio* (TTR). 

$$ TTR(T) = \frac{Voc(T)}{n} $$
где n - общее число токенов, а Voc - число уникальных токенов (типов).

В пакете `languageR`, написанном лингвистом Гаральдом Баайеном, есть функция, позволяющая быстро производить такие вычисления. Она требует на входе вектор, а не тиббл, поэтому для эксперимента извлечем один из текстов.

```{r}
locke_words <- corpus_words %>% 
  filter(author == "Locke") %>% 
  pull(word)

head(locke_words)
length(locke_words)
```
Это немного задумчивая функция, поэтому я создам не 148 тысяч отрывков, а всего 40.

```{r message=FALSE}
library(languageR)
locke.growth = growth.fnc(text = locke_words, size = 1000, nchunks = 40)

head(locke.growth@data$data)
```

```{r}
plot(locke.growth)
```

Тут много всего интересного, но обратим внимание лишь на два верхних окошка слева: количество типов растет постоянно, но с разной скоростью. Так же меняется числов гапаксов: нечто похожее мы уже замечали, когда говорили о гапаксах у Платона. Подробнее о различных мерах лексического разнообразия см.: [@Baayen2008, 222-236] и [@savoy2020].

## TF-IDF

Мы уже заметили, говоря об абсолютной частотности, что для трех авторов в нашем условном корпусе многие слова общие. Для многих алгоритмов машинного обучения используется другая мера, **tf-idf** (term frequency - inverse document frequency). 

![](https://avatars.dzeninfra.ru/get-zen_doc/3986249/pub_5f589066197cd55cd9ab2254_5f5890f7197cd55cd9ac1822/scale_1200)


Логарифм единицы равен нулю, поэтому если слово встречается во всех документах, его tf-idf равно нулю. Чем выше tf-idf, тем более характерно некое слово для некоторого документа. Однако относительная частотность тоже учитывается! Например, Беркли один раз упоминает "сахарные бобы", а Локк -- "миндаль", но из-за редкой частотности tf-idf для подобных слов будет низкий.

Функция `bind_tf_idf()` принимает на входе тиббл с абсолютной частотностью для каждого слова. 

```{r}
author_word_tfidf <- author_word_rf %>% 
  filter(!word %in% other) %>% 
  bind_tf_idf(word, author, n)

author_word_tfidf
```

Выбираем слова с высокой tf-idf:

```{r}
author_word_tfidf %>% 
  select(-total) %>% 
  arrange(desc(tf_idf))
```

Снова визуализируем.

```{r}
author_word_tfidf %>% 
  arrange(-tf_idf) %>% 
  group_by(author) %>% 
  top_n(15) %>% 
  ungroup() %>% 
  ggplot(aes(reorder_within(word, tf_idf, author), tf_idf, fill = author)) +
  geom_col(show.legend = F) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~author, scales = "free") +
  scale_x_reordered() +
  coord_flip()
```

На таком графике авторы выглядят гораздо более самобытными, но будьте осторожны: все то, что их сближает (а это не только служебные части речи!), сюда просто не попало. 

Можно также заметить, что ряд характерных слов связаны не столько с тематикой, сколько со стилем: чтобы этого избежать, можно использовать лемматизацию или задать правило для замены вручную.

## Сравнение при помощи диаграммы рассеяния 

Столбиковая диаграмма -- не единственный способ сравнить частотности слов. Еще один наглядный метод -- это диаграмма рассеяния с относительными частотностями. Функция `spread()` позволяет разделить один столбец на несколько новых, а `gather()`, напротив, -- собрать.

```{r}
freq <- author_word_rf %>%
  anti_join(stop_words) %>% 
  mutate(rf = rf / 100) %>% 
  filter(rf > 0.0001) %>% 
  select(-n, -total) %>% 
  spread(author, rf, fill = 0) %>% 
  gather(author, rf, Hume:Locke)

freq
```

```{r warning=FALSE}
library(scales)

freq %>% 
  ggplot(aes(x = rf, y = Berkeley)) +
  geom_abline(color = "grey40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, 
              height = 0.3, color = "darkblue") +
  geom_text(aes(label = word), check_overlap = TRUE, 
            vjust = 1.5, color = "grey30") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  facet_wrap(~author, ncol = 2) +
  theme(legend.position = "none") +
  theme_minimal() +
  labs(y = "Berkeley", x = NULL)
  
```
Эти три трактата нам еще понадобятся, поэтому сохраняем опрятный текст с уже удаленными стоп-словами.

```{r}
save(corpus_words_nosp, file = "data/Idea.Rdata")
```