# Латентно-семантический анализ (Часть 1)

## Что это такое

В этом уроке речь пойдет о таком методе **тематического моделирования**, как **латентно-семантический анализ**, или LSA (в области информационного поиска называемый также LSI, Latent Semantic Indexing). 

Как и LDA, это метод метод обработки информации на естественном языке, который позволяет находить взаимосвязь между коллекцией документов и встречающимися в них терминами за счет сопоставления этих документов и терминов с некоторыми **темами**. 

Слово "скрытый" в названии указывает на то, что **семантические взаимосвязи** между документами и терминами, как правило, заранее не известны. В исходной термдокументной матрице, которую мы передаем алгоритму, не видно закономерностей. Как раз задача аналитика -- их обнаружить, сгруппировав по темам, с одной стороны, термины, а с другой -- документы.

![](https://sysblok.ru/wp-content/uploads/2019/07/image16.gif)

## Векторное представление слов

Как вообще машина может обнаружить "близкие" слова и документы? В 1957 британский лингвист Джон Руперт Фёрс сформулировал это так: you shall know a word by the company it keeps. 

Мы можем представить термин в виде вектора, который хранит информацию о его встречаемости в документах. Каждый элемент вектора будет координатой в многомерном пространстве. В англоязычной литературе векторные представления называют **эмбеддингами** (embedding, т.е. «вложение»). Представляя объект в виде вектора, мы как бы «вкладываем» его в векторное пространство, где действуют геометрические законы.

Дальше дело за вычислением **расстояния** (или сходства) между векторами -- это уже задача из области линейной алгебры. Как правило, в алгоритме LSA используется **косинусное сходство** (о том, что это такое, см. видео: [часть 1](https://vk.com/video-211800158_456239261) и [часть 2](https://vk.com/video-211800158_456239263)).

![](images/dist.png)


Трудность в том, что даже для небольших коллекций документов термдокументная матрица является очень **разреженной** (даже в небольшом примере из предыдущего урока разреженность составляла почти 100%). Чем больше ваш корпус, тем более разреженной будет матрица: это естественно, поскольку в каждом документе встречается лишь небольшая часть всех слов. 

Любые действия над такими матрицами требуют больших вычислительных затрат, при этом результат не обязательно будет точным из-за проблемы синонимии: слово apple можно встретить как в плодово-овощной рубрике, так и в заметке об IT. Справиться с этим помогают методы **снижения размерности**.

## Сингулярное разложение матрицы

Мы можем "спроецировать" исходную матрицу $C_r$ в пространство меньшей размерности. Если r -- ранг исходной матрицы, а k -- ранг новой матрицы, при этом k значительно ниже r, то матрица $C_k$ называется **малоранговой аппроксимацией**. 

:::infobox
Рангом системы строк (столбцов) матрицы A с m строками и n столбцами называется максимальное число линейно независимых строк (столбцов). Несколько строк (столбцов) называются линейно независимыми, если ни одна из них не выражается линейно через другие. Ранг системы строк всегда равен рангу системы столбцов, и это число называется рангом матрицы. 
:::

Для ее получения новой матрицы применяется трехэтапная процедура:

1.  для $C_r$ строится ее **сингулярное разложение** (SVD (Singular Value Decomposition) по формуле: $C = UΣV^t$ (подробнее см. [видео](https://vk.com/video-211800158_456239325));

2.  по матрице Σ строится $Σ_k$: $r - k$ наименьших сингулярных значений на диагонали матрицы заменяются нулями;

3. вычисляется новая матрица $C_k = UΣ_kV^t$.

Теперь подробнее. 

1. Матричное разложение, или **факторизация** -- представление матрицы в виде произведения нескольких матриц. Сингулярное разложение (SVD) матрицы A равно $A=U⋅Σ⋅V^t$, где
- U — матрица левых сингулярных векторов матрицы A,
- Σ — диагональная матрица сингулярных чисел матрицы A,
- V — матрица правых сингулярных векторов матрицы A.

![](https://www.askpython.com/wp-content/uploads/2020/11/SVD-1.jpg.webp)

Строки матрицы U соответствуют словам; а в V^t столбцы соответствуют отдельным документам. Следовательно, первая строка матрицы U показывает, в каких документах встречается слово, а первый столбец V^T показывает, какие темы встречаются в документе.

2. **Сингулярные значения** в диагональной матрице всегда упорядочены по убыванию, и можно без больших потерь отсечь малоинформативные ряды или столбцы. Такое SVD называется **усеченным**.

**Сингулярные векторы** (они выделены цветом) в матрицах U и V соответствуют темам в тексте, которых в общей сложности k штук. Чему равно k — человек задает вручную при вычислении разложения. Математически это выражается в том, что в диагональной матрице остается k самых больших сингулярных чисел, а остальные становятся нулями. При перемножении матриц это приведет к отсечению $r - k$ столбцов в U и $r - k$ рядов в $V^t$. ([Подробнее](https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/)).

3. Умножение U на Σ дает векторное представление слов; умножение Σ на V -- векторное представление документов. Объединяя пространство слов с пространством документов, можно находить ближайшие документы по поисковому запросу. 

Все это будет понятнее на простом примере.

## Простой пример

Рассморим это на простом примере ([отсюда](https://www.engr.uvic.ca/~seng474/svd.pdf)).

Допустим, у нас есть пять документов. 

_d1 : Romeo and Juliet._

_d2 : Juliet: O happy dagger!_

_d3 : Romeo died by dagger"._

_d4 : "Live free or die", that's the New-Hampshire's motto._

_d5 : Did you know, New Hampshire is in New-England._

Поисковый запрос: _dies_, _dagger_. Очевидно, ближе всего к запросу d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 -- ни одного. 

Составим термдокументную матрицу.

```{r}
df = data.frame(d1 = c(c(1, 1), rep(0, 6)),
                d2 = c(c(0, 1, 1, 1), rep(0, 4)),
                d3 = c(1, 0, 0, 1, 0, 1, 0, 0),
                d4 = c(rep(0, 4), rep(1, 4)),
                d5 = c(rep(0, 7), c(1)))
rownames(df) <- c("romeo", "juliet", "happy", "dagger", "live",
                  "die", "free", "new-hampshire")             
df
```

И применим SVD. В R для этого есть специальная функция.

```{r}
result = svd(df)
S = round((diag(result$d, nrow = length(result$d))), 3)

S
```
Сингулярные значения меньше двух убираем, остается два сингулярных значения.

```{r}
S_truncated <- S[1:2,1:2]

S_truncated
```
Матрица левых сингулярных векторов выглядит так:

```{r}
U <- round((result$u), 3)

U
```
От нее отсекаем все столбцы, кроме первых двух:

```{r}
U_truncated <- U[,1:2]

U_truncated
```

Матрица правых сингулярных векторов тоже усекается; не забудем ее транспонировать.

```{r}
Vt_truncated <- round((t(result$v[,1:2])), 3)

Vt_truncated
```
Каждый столбец в этой матрице соответствует одному документу.

Умножим U на S_truncated (усеченную сигму), и S_truncated -- на V_t. 


```{r}
# эмбеддинги слов
word_emb <- round((U_truncated %*% S_truncated), 3)

rownames(word_emb) <- c("romeo", "juliet", "happy", "dagger", "live", "die", "free", "new-hampshire") 

word_emb
```

```{r}
# эмбеддинги документов
doc_emb <- round((S_truncated %*% Vt_truncated), 3)
colnames(doc_emb) <- c("d1", "d2", "d3", "d4", "d5")

doc_emb 
```

Координаты поискового запроса (который рассматриваем как новый документ) считаем как среднее арифметическое координат:

```{r}
q = c("die", "dagger")
q_doc <-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2
q_doc
```

Объединив все в единый датафрейм, можем визуализировать.

```{r warning=FALSE, message=FALSE}
library(tidyverse)

all_df <- as.data.frame(rbind(word_emb, t(doc_emb), q_doc))

all_tbl <- as_tibble(all_df, rownames = "item")  %>% 
  mutate(type = c(rep("word", 8), rep("doc", 6))) %>% 
  rename(dim1 = V1, dim2 = V2)

all_tbl
```

```{r echo=FALSE}
zero = tibble(dim1 = 0, dim2 = 0)
line1 = all_tbl %>% filter(item == "d1") %>% 
  select(-item, -type) %>% bind_rows(zero)
line2 = all_tbl %>% filter(item == "d5") %>% 
  select(-item, -type) %>% bind_rows(zero)
lm1 = summary(lm(data=line1, dim2 ~ dim1))
lm2 = summary(lm(data=line2, dim2 ~ dim1))
```

Теперь строим график.

```{r message=FALSE, echo=FALSE}
library(ggrepel)

all_tbl %>% 
  ggplot(aes(dim1, dim2, 
             color = as.factor(type), label = item)) +
  geom_point(show.legend = F) + 
  geom_text_repel(aes(fontface = "bold"), show.legend = F) +
  theme_bw() + 
  xlab(NULL) + 
  ylab(NULL) +
  geom_abline(slope = 0, intercept = 0, linetype = "dotted") +
  geom_vline(xintercept = -1.10, linetype = "dotted") +
  coord_cartesian(xlim = c(-1.7, 0.2), ylim = c(-1.5, 1.5)) +
  geom_abline(slope = lm1$coefficients[2], 
              intercept = lm1$coefficients[1], linetype = "dotted") + 
  geom_abline(slope = lm2$coefficients[2], 
              intercept = lm2$coefficients[1], linetype = "dotted")
```

Как видно, поисковый запрос оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.

Мы исследовали наш небольшой корпус графически, теперь посчитаем косинусное расстояние.

```{r}
dist_mx <- all_df %>% 
  filter(type == "doc") %>% 
  philentropy::distance(method = "cosine", use.row.names = T) 

dist_mx
```
## Векторы слов: опрятно 

Текст хранится в опрятном формате, если одному наблюдению (термину) соответствует один ряд:

```{r}
library(tidyr)

tidy_corpus <- df %>% 
  as_tibble(rownames = "word") %>% 
  pivot_longer(d1:d5, names_to = "doc")

tidy_corpus
```

Чтобы вычислить svd, такой корпус надо преобразовать в широкий формат, произвести все вычисления, а затем "тайдифицировать". Для подобных операций существует пакет `widyr`. Функцию `pivot_wider()` я добавляю лишь для удобства сравнения с тем, что мы получили выше.

```{r}
library(widyr)

tidy_u <- tidy_corpus %>% 
  widely_svd(word, doc, value,
             nv = 2, maxit = 100) %>% 
  mutate(value = round(value, 3)) %>% 
  pivot_wider(names_from = dimension, values_from = value) 

tidy_u
```

Сравним с тем, что у нас получилось выше.

```{r}
U_truncated == tidy_u[,2:3]
```

Итак, мы получили только матрицу левых сингулярных векторов. Чтобы получить word_emb, используем аргумент `weight_d`:

```{r}
tidy_word_emb <- tidy_corpus %>% 
  widely_svd(word, doc, value, 
             nv = 2, maxit = 100, 
             weight_d = T) %>%
  mutate(value = round(value, 3)) 


tidy_word_emb
```
Сравнив с `word_emb` выше, видим, что округление не всегда с точностью до тысячных совпадает, но в остальном все ок.

## Векторы документов: опрятно

Осталось понять, как _опрятно_ получить doc_emb. Здесь есть достаточно элегантное решение, которое заключается в том, чтобы рассматривать каждый документ как набор слов^[https://smltar.com/embeddings.html#understand-word-embeddings-by-finding-them-yourself]. Перемножив матрицу документ-термин, и матрицу с эмбеддингами слов, получаем искомые эмбеддинги документов.

Частотность слов хранится в объекте `tidy_corpus`, преобразуем его.

```{r}
library(tidytext)

word_mx <- tidy_corpus %>% 
  cast_sparse(doc, word, value)

word_mx
```

Эмбеддинги слов преобразуем в тот же формат. 

```{r}
embedding_mx <- tidy_word_emb %>%
  cast_sparse(word, dimension, value)

embedding_mx
```
Перемножаем частотности слов в документе и эмбеддингов слов. 

```{r}
doc_mx <- word_mx %*% embedding_mx

tidy_doc_emb <- as.data.frame(as.matrix(doc_mx)) %>% 
  as_tibble(rownames = "item") %>% 
  rename(dim1 = `1`, dim2 = `2`)

tidy_doc_emb
```

Убедимся, что слова и документы расположились осмысленно:


```{r echo=FALSE, message=FALSE}
all_df <- tidy_word_emb %>% 
  pivot_wider(names_from = dimension, values_from = value) %>%
  rename(dim1 = `1`, dim2 = `2`, item = word) %>% 
  bind_rows(tidy_doc_emb) %>% 
  mutate(type = c(rep("word", 8), rep("doc", 5)))

zero = tibble(dim1 = 0, dim2 = 0)
line1 = all_df %>% 
  filter(item == "d1") %>% 
  select(-item, -type) %>% 
  bind_rows(zero)
line2 = all_df %>% 
  filter(item == "d5") %>% 
  select(-item, -type) %>% 
  bind_rows(zero)
lm1 = summary(lm(data=line1, dim2 ~ dim1))
lm2 = summary(lm(data=line2, dim2 ~ dim1))
```

```{r echo=FALSE, message=FALSE}
library(ggrepel)

all_df %>% 
  ggplot(aes(dim1, dim2, 
             color = as.factor(type), label = item)) +
  geom_point(show.legend = F) + 
  geom_text_repel(aes(fontface = "bold"), show.legend = F) +
  theme_bw() + 
  xlab(NULL) + 
  ylab(NULL) +
  geom_abline(slope = 0, intercept = 0, linetype = "dotted") +
  geom_abline(slope = lm1$coefficients[2], 
              intercept = lm1$coefficients[1], linetype = "dotted") + 
  geom_abline(slope = lm2$coefficients[2], 
              intercept = lm2$coefficients[1], linetype = "dotted") +
  coord_cartesian(xlim = c(-3.5, 0.2), ylim = c(-4, 3)) +
  geom_point(aes(x = -1.099, y = 0.124), show.legend = F) +
  geom_text(aes(x = -1.099, y = 0.124), label = "q", hjust = 2, show.legend = F)
```

Видно, что документы "уехали" влево, так как мы суммировали эмбединги для документов (можно взять среднее); однако при использовании косинусного сходства на результат это не влияет. 

## Пакет LSA

Имея матрицу документ-термин, мы можем быстро построить модель, воспользовавшись функцией `lsa()` из одноименного пакета. 

```{r}
library(lsa)

lsaSpace <- lsa(word_mx, dims = 2)
```

Функция вернет список, в котором хранятся элементы под названием `tk`, `dk`, `sk`. Они соответствуют матрице правых сингулярных векторов V, матрице правых сингулярных векторов U, а также сингулярным значениям. Убедимся в этом.

```{r}
round(lsaSpace$tk, 3); t(Vt_truncated)
```
Для матрицы $V^t$ результаты совпадают за исключением знака в первом столбце; это явление известно как [sign ambiguity](https://www.educative.io/blog/sign-ambiguity-in-singular-value-decomposition). Первый столбец матрицы U тоже меняет знак, так что при перемножении мы вновь получаем исходную матрицу. Убедимся в этом. 

```{r}
round(lsaSpace$dk, 3); U_truncated
```


Чтобы построить эмбеддинги документов, перемножаем диагональную матрицу и $V^t$:

```{r}
diag(lsaSpace$sk) %*% t(lsaSpace$tk)
```
Это совпадает с тем результатом, который мы получали выше (за исключением знака, о чем было сказано выше):

```{r}
doc_emb
```

Дальше это можно транспонировать и посчитать расстояние между документами, как мы уже делали, и проделать то же самое со словами. 

## Кластеризация

Функция `cosine()` из пакета `lda` считает косинусное сходство между векторами или _столбцами_ матрицы (будьте внимательны: многие функции для подсчета расстояния или сходства оперируют рядами!). Чтобы получить расстояние, вычитаем результат из единицы.

```{r}
dist_cos <- as.dist(1 - cosine(doc_emb))
```

Имея матрицу расстояния, можно найти кластеры документов:

```{r}
hc <- hclust(dist_cos)
plot(hc)
```

"Бьютифицировать" это грустное деревце можно тысячью разных [способов](http://www.sthda.com/english/wiki/beautiful-dendrogram-visualizations-in-r-5-must-known-methods-unsupervised-machine-learning), в том числе с использованием специального пакета [`ggdendro`](https://cran.r-project.org/web/packages/ggdendro/vignettes/ggdendro.html).

Если документов (или слов) много, то удобнее бывает обозначить кластеры цветом:

```{r warning=FALSE}
t(doc_emb) %>% 
  as_tibble(rownames = "item") %>% 
  ggplot(aes(V1, V2, label = item, col = as.factor(cutree(hc, 2)))) +
  geom_point(show.legend = F) + 
  geom_text_repel(show.legend = F)
```

## Визуализации топиков

Для лучшей интерпретируемости модели бывает полезно визуализировать сами топики. У нас игрушечный пример, поэтому выглядит это не очень интересно, подробнее можно почитать [здесь](https://smltar.com/embeddings.html#exploring-cfpb-word-embeddings).

```{r}
tidy_word_emb %>% 
  group_by(dimension) %>%
  arrange(abs(value)) %>% 
  slice_head(n = 3) %>% 
  ungroup() %>% 
  mutate(word = reorder_within(word, value, dimension)) %>% 
  ggplot(aes(word, value, fill = dimension)) +
  geom_col(show.legend = F) +
  facet_wrap(~dimension, scales = "free_y", ncol = 4) +
  scale_x_reordered() +
  coord_flip()
  
```

LSA, в отличие от LDA, -- это не вероятностная, а алгебраическая модель. Это значит, что термдокументная матрица может хранить не только данные об абсолютной встречаемости слова в документе (как это требует LDA), выраженные целыми числами, но и любые реальные числа. Как показывает практика, использование tf_idf в большинстве случаев улучшает модель. 








