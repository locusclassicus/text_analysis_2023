# Латентно-семантический анализ

## Что это такое

В этом уроке речь пойдет о таком методе, как **латентно-семантический анализ**, или LSA (в области информационного поиска называемый также LSI, Latent Semantic Indexing). 

Как и LDA, это метод метод обработки информации на естественном языке, который позволяет находить взаимосвязь между коллекцией документов и встречающимися в них терминами за счет сопоставления этих документов и терминов с некоторыми **темами**. 

То есть LSA -- это тоже метод **тематического моделирования**. Скрытым такое моделирование называется потому, что **семантические взаимосвязи** между документами и терминами, как правило, заранее не известны. В исходной термдокументной матрице, которую мы скармливаем алгоритму, не видно никаких закономерностей. Как раз задача аналитика -- их обнаружить, как это показывает анимация ниже. 

![](https://sysblok.ru/wp-content/uploads/2019/07/image16.gif)

## Векторное представление слов

Как вообще машина может обнаружить "близкие" слова и документы? При помощи векторизации: каждый документ или слово можно представить в виде вектора частотностей, где каждое значение будет некой координатой в многомерном пространстве.
Мы исходим из того, что в похожих документах употребляются одни и те же слова, а похожие слова встречаются в похожих документах. В 1957 британский лингвист Джон Руперт Фёрс сформулировал это так: you shall know a word by the company it keeps. 

Можно сказать, что это основополагающий принцип векторной семантики. Мы не пытаемся заглянуть в голову носителю языка (в случае с древнегреческим это было бы затруднительно), а смотрим на цифры.

Как только документ или термин представлены в виде вектора, в дело вступает простая (или не очень простая) геометрия: под _расстоянием_ между документами или терминами имеется в виду именно геометрическое расстояние (или сходство). Как правило, в алгоритме LSA используется **косинусное сходство** (о том, что это такое, я рассказывала в видео: [часть 1](https://vk.com/video-211800158_456239261) и [часть 2](https://vk.com/video-211800158_456239263)).

В англоязычной литературе векторные представления называют **эмбеддингами**. По-английски embedding означает «вложение». Представляя объект в виде вектора, мы как бы «вкладываем» его в векторное пространство, где действуют геометрические законы.

## Проклятие размерности

Идея хорошая, но она сразу же наталкивается на трудность: даже для небольших коллекций документов термдокументная матрица является очень **разреженной** (даже в небольшом примере из предыдущего урока разреженность составляла почти 100%). Чем больше ваш корпус, тем более разреженной будет матрица: это естественно, поскольку в каждом документе встречается лишь небольшая часть всех слов. 

Любые действия над такими матрицами требуют больших вычислительных затрат, а результат не обязательно будет точным из-за проблемы синонимии: слово apple можно встретить как в плодово-овощной рубрике, так и в заметке об IT. Латентно-семантическое индексирование частично решает обе проблемы. 

## Сингулярное разложение матрицы

В основе LSA лежит метод **малоранговой аппроксимации** термдокументной матрицы, позволяющий "спроецировать" исходную матрицу C в пространство меньшей размерности при помощи алгоритма SVD (Singular Value Decomposition, или **сингулярное разложение матрицы**). Подробнее об этом алгоритме см. [видео](https://vk.com/video-211800158_456239325).

:::infobox
Рангом системы строк (столбцов) матрицы A с m строками и n столбцами называется максимальное число линейно независимых строк (столбцов). Несколько строк (столбцов) называются линейно независимыми, если ни одна из них не выражается линейно через другие. Ранг системы строк всегда равен рангу системы столбцов, и это число называется рангом матрицы. 
:::

Если r -- ранг исходной матрицы, а k -- ранг новой матрицы, при этом k значительно ниже r, то матрица C_k называется малоранговой аппроксимацией. Для ее получения применяется трехэтапная процедура:

1.  для заданной матрицы строится ее **сингулярное разложение** по формуле: $C = UΣV^t$;

2.  по матрице Σ строится $Σ_k$: r - k наименьших сингулярных значений на диагонали матрицы заменяются нулями;

3. вычисляется новая матрица $C_k = UΣ_kV^t$.

Теперь подробнее. 

1. Матричное разложение, или **факторизация** -- представление матрицы в виде произведения нескольких матриц. Сингулярное разложение (SVD) матрицы A равно $A=U⋅Σ⋅V^t$, где
- U — матрица левых сингулярных векторов матрицы A,
- Σ — диагональная матрица сингулярных чисел матрицы A,
- V — матрица правых сингулярных векторов матрицы A.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/1024px-Singular_value_decomposition_visualisation.svg.png)

2. Сингулярные значения в диагональной матрице всегда упорядочены по убыванию, и можно без больших потерь отсечь малоинформативные ряды или столбцы. Как правило,  так и делают, и такое SVD называется **усеченным**.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Reduced_Singular_Value_Decompositions.svg/220px-Reduced_Singular_Value_Decompositions.svg.png)


Сингулярные векторы (они выделены цветом) в матрицах U и V соответствуют темам в тексте, которых в общей сложности k штук. Чему равно k — человек задает вручную при вычислении разложения, а значит, всегда есть возможность найти ровно k тем. Математически это представлено так, что алгоритм разложения выбирает из исходной матрицы k самых больших сингулярных чисел и формирует из них среднюю матрицу Σ, элементы которой идут по диагонали. ([Отсюда](https://sysblok.ru/knowhow/kak-ponjat-o-chem-tekst-ne-chitaja-ego/)).

Строки матрицы U соответствуют словам; а в V^t столбцы соответствуют отдельным документам. Следовательно, первая строка матрицы U показывает, в каких документах встречается слово, а первый столбец V^T показывает, какие темы встречаются в документе.

3. Умножение U на Σ дает векторное представление слов; умножение Σ на V -- векторное представление документов. Часто пространство слов объединяют с пространством документов, что позволяет находить ближайшие документы по поисковому запросу. 

Все это будет понятнее на простом примере.

## Простой пример

Рассморим это на простом примере ([отсюда](https://www.engr.uvic.ca/~seng474/svd.pdf)).

Допустим, у нас есть пять документов. 

_d1 : Romeo and Juliet._

_d2 : Juliet: O happy dagger!_

_d3 : Romeo died by dagger"._

_d4 : "Live free or die", that's the New-Hampshire's motto._

_d5 : Did you know, New Hampshire is in New-England._

Поисковый запрос: _dies_, _dagger_. Очевидно, ближе всего к запросу d3, т.к. он содержит оба слова. Но какой документ должен быть следующим? И d2, d4 содержат по одному слову из запроса, а явно релевантный d1 -- ни одного. 

Составим термдокументную матрицу.

```{r}
df = data.frame(d1 = c(c(1, 1), rep(0, 6)),
                d2 = c(c(0, 1, 1, 1), rep(0, 4)),
                d3 = c(1, 0, 0, 1, 0, 1, 0, 0),
                d4 = c(rep(0, 4), rep(1, 4)),
                d5 = c(rep(0, 7), c(1)))
rownames(df) <- c("romeo", "juliet", "happy", "dagger", "live",
                  "die", "free", "new-hampshire")             
df
```
И применим SVD.

```{r}
result = svd(df)
S = round((diag(result$d, nrow = length(result$d))), 3)

S
```
Сингулярные значения меньше двух усекаем, остается два сингулярных значения.

```{r}
S_truncated <- S[1:2,1:2]

S_truncated
```
Матрица левых сингулярных векторов выглядит так:

```{r}
U <- round((result$u), 3)

U
```
От нее отсекаются все столбцы, кроме первых двух; каждый ряд в этой матрице отвечает за определенный термин:

```{r}
U_truncated <- U[,1:2]

U_truncated
```

Матрица правых сингулярных векторов тоже усекается:

```{r}
Vt_truncated <- round((t(result$v[,1:2])), 3)

Vt_truncated
```
Каждый столбец в этой матрице соответствует одному документу.

Умножим U на S_truncated (усеченную сигму), и S_truncated -- на V_t, мы объединяем документы и термины в единое векторное пространство. 

```{r}
# эмбеддинги документов
doc_emb <- round((S_truncated %*% Vt_truncated), 3)
colnames(doc_emb) <- c("d1", "d2", "d3", "d4", "d5")

doc_emb 
```

```{r}
# эмбеддинги слов
word_emb <- round((U_truncated %*% S_truncated), 3)

rownames(word_emb) <- c("romeo", "juliet", "happy", "dagger", "live", "die", "free", "new-hampshire") 

word_emb
```

Координаты поискового запроса (который рассматриваем как новый документ) считаем как центроид двух векторов его слов:

```{r}
q = c("die", "dagger")
q_doc <-  colSums(word_emb[rownames(word_emb) %in% q, ]) / 2
q_doc
```
Объединяем все в единый датафрейм.
```{r warning=FALSE}
all_df <- rbind(word_emb, t(doc_emb), q_doc)
all_df <- as_tibble(all_df, rownames = "names") 

all_df
```
Добавим еще одну переменную. 

```{r message=FALSE}
type <- c(rep("word", 8), rep("doc", 6))
all_df <- all_df %>% bind_cols(type)
colnames(all_df) <- c("x", "dim1", "dim2", "type")

all_df
```

```{r echo=FALSE}
zero = tibble(dim1 = 0, dim2 = 0)
line1 = all_df %>% filter(x == "d1") %>% 
  select(-x, -type) %>% bind_rows(zero)
line2 = all_df %>% filter(x == "d5") %>% 
  select(-x, -type) %>% bind_rows(zero)
lm1 = summary(lm(data=line1, dim2 ~ dim1))
lm2 = summary(lm(data=line2, dim2 ~ dim1))
```

Теперь строим график.

```{r message=FALSE}
library(tidyverse)
library(ggrepel)

all_df %>% 
  ggplot(aes(dim1, dim2, 
             color = as.factor(type), label = x)) +
  geom_point(show.legend = F) + 
  geom_text_repel(aes(fontface = "bold"), show.legend = F) +
  theme_bw() + 
  xlab(NULL) + 
  ylab(NULL) +
  geom_abline(slope = 0, intercept = 0, linetype = "dotted") +
  geom_vline(xintercept = -1.10, linetype = "dotted") +
  coord_cartesian(xlim = c(-1.7, 0.2), ylim = c(-1.5, 1.5)) +
  geom_abline(slope = lm1$coefficients[2], 
              intercept = lm1$coefficients[1], linetype = "dotted") + 
  geom_abline(slope = lm2$coefficients[2], 
              intercept = lm2$coefficients[1], linetype = "dotted")
```

Как видно, поисковый запрос оказался ближе к d2, чем к d4, хотя в каждом из документов было одно слово из запроса. Более того: он оказался ближе к d1, в котором не было ни одного слова из запроса! Наш алгоритм оказался достаточно умен, чтобы понять, что d1 более релевантен, хотя и не содержит точных совпадений с поисковыми словами. Возможно, человек дал бы такую же рекомендацию.

Мы исследовали наш небольшой корпус графически, теперь посчитаем косинусное расстояние.

```{r}
dist_mx <- all_df %>% 
  filter(type == "doc") %>% 
  select(-x, -type) %>% 
  philentropy::distance(method = "cosine") 

rownames(dist_mx) <- c("d1", "d2", "d3", "d4", "d5", "q")
colnames(dist_mx) <- c("d1", "d2", "d3", "d4", "d5", "q")

dist_mx
```
## Пример посложнее

LSA, в отличие от LDA, -- это не вероятностная, а алгебраическая модель. Это значит, что термдокументная матрица может хранить не только данные об абсолютной встречаемости слова в документе (как это требует LDA), выраженные целыми числами, но и любые реальные числа. Как правило, используется tf_idf, о которой шла речь в одном из предыдущих уроков.









